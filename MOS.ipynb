{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pickle\n",
    "import os, sys\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from embed_regularize import embedded_dropout\n",
    "from locked_dropout import LockedDropout\n",
    "from weight_drop import WeightDrop\n",
    "\n",
    "import gc\n",
    "\n",
    "import data\n",
    "\n",
    "from utils import batchify, get_batch, repackage_hidden, create_exp_dir, save_checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors' implementation uses exactly the same regularization and optimizing techniques as were presented in [Merity 2017]. The only differences are: smaller lr(20.0 vs 30.0), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, rnn_type, ntoken, ninp, nhid, nhidlast, nlayers, \n",
    "                 dropout=0.5, dropouth=0.5, dropouti=0.5, dropoute=0.1, wdrop=0, \n",
    "                 tie_weights=False, ldropout=0.5, n_experts=10, moc=False, mos=False):\n",
    "        super(RNNModel, self).__init__()\n",
    "        #lock dropout uses same dropout mask for all repeated connections in one forward\n",
    "        self.lockdrop = LockedDropout()\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "        \n",
    "        self.rnns = [torch.nn.LSTM(ninp if l == 0 else nhid, nhid if l != nlayers - 1 else nhidlast, 1, dropout=0) for l in range(nlayers)]\n",
    "        if wdrop:\n",
    "            self.rnns = [WeightDrop(rnn, ['weight_hh_l0'], dropout=wdrop) for rnn in self.rnns]\n",
    "        self.rnns = torch.nn.ModuleList(self.rnns)\n",
    "        \n",
    "        #MOS\n",
    "        if moc or mos:\n",
    "            self.prior = nn.Linear(nhidlast, n_experts, bias=False)\n",
    "            self.latent = nn.Sequential(nn.Linear(nhidlast, n_experts*ninp), nn.Tanh())\n",
    "        \n",
    "        if tie_weights:\n",
    "            self.decoder.weight = self.encoder.weight\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.ninp = ninp\n",
    "        self.nhid = nhid\n",
    "        self.nhidlast = nhidlast\n",
    "        self.nlayers = nlayers\n",
    "        \n",
    "        #output dropout\n",
    "        self.dropout = dropout\n",
    "        #input dropout\n",
    "        self.dropouti = dropouti\n",
    "        #hidden dropout\n",
    "        self.dropouth = dropouth\n",
    "        #embedded dropout\n",
    "        self.dropoute = dropoute \n",
    "        #latent dropout\n",
    "        self.ldropout = ldropout \n",
    "        self.dropoutl = ldropout \n",
    "        \n",
    "        self.n_experts = n_experts \n",
    "        self.ntoken = ntoken\n",
    "\n",
    "        self.wdrop = wdrop\n",
    "        self.moc = moc\n",
    "        self.mos = mos\n",
    "\n",
    "        size = 0\n",
    "        for p in self.parameters():\n",
    "            size += p.nelement()\n",
    "        print('Model param size: {}'.format(size))\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, input, hidden, return_h=False, return_prob=False):\n",
    "        batch_size = input.size(1)\n",
    "        emb = embedded_dropout(self.encoder, input, dropout=self.dropoute if self.training else 0)\n",
    "        \n",
    "        \n",
    "        emb = self.lockdrop(emb, self.dropouti)\n",
    "\n",
    "        raw_output = emb\n",
    "        new_hidden = []\n",
    "        raw_outputs = []\n",
    "        outputs = []\n",
    "        for l, rnn in enumerate(self.rnns):\n",
    "            raw_output, new_h = rnn(raw_output, hidden[l])\n",
    "            new_hidden.append(new_h)\n",
    "            raw_outputs.append(raw_output)\n",
    "            if l != self.nlayers - 1:\n",
    "                raw_output = self.lockdrop(raw_output, self.dropouth)\n",
    "                outputs.append(raw_output)\n",
    "        hidden = new_hidden\n",
    "\n",
    "        output = self.lockdrop(raw_output, self.dropout)# size: seq_len x batch_size x nhidlast\n",
    "        outputs.append(output)\n",
    "        \"\"\" MoS / MoC / SoftMax\"\"\"\n",
    "        if self.mos:\n",
    "            latent = self.latent(output) #size: seq_len x batch_size x n_experts*ninp\n",
    "            latent = self.lockdrop(latent, self.dropoutl) \n",
    "            logit = self.decoder(latent.view(-1, self.ninp)) # h^t_c dot w_c^T\n",
    "\n",
    "            prior_logit = self.prior(output).contiguous().view(-1, self.n_experts)\n",
    "            prior = nn.functional.softmax(prior_logit)\n",
    "            prob = nn.functional.softmax(logit.view(-1, self.ntoken)).view(-1, self.n_experts, self.ntoken)\n",
    "            prob = (prob * prior.unsqueeze(2).expand_as(prob)).sum(1)\n",
    "        elif self.moc:\n",
    "            latent = self.latent(output) #size: seq_len x batch_size x n_experts*ninp\n",
    "            latent = self.lockdrop(latent, self.dropoutl) \n",
    "            logit = self.decoder(latent.view(-1, self.ninp)) # h^t_c dot w_c^T, size: seq_len*batch_size*n_experts x ntokens\n",
    "\n",
    "            prior_logit = self.prior(output).contiguous().view(-1, self.n_experts)\n",
    "            prior = nn.functional.softmax(prior_logit)\n",
    "            logit = logit.view(-1, self.n_experts, self.ntoken)\n",
    "            logit = (logit * prior.unsqueeze(2).expand_as(logit)).sum(1)\n",
    "            prob = nn.functional.softmax(logit)\n",
    "            \n",
    "        else:\n",
    "            logit = self.decoder(output.view(-1, self.ninp))\n",
    "            prob = nn.functional.softmax(logit.view(-1, self.ntoken))\n",
    "\n",
    "            \n",
    "        if return_prob:\n",
    "            model_output = prob\n",
    "        else:\n",
    "            log_prob = torch.log(prob + 1e-8)\n",
    "            model_output = log_prob\n",
    "\n",
    "        model_output = model_output.view(-1, batch_size, self.ntoken)\n",
    "\n",
    "        if return_h:\n",
    "            return model_output, hidden, raw_outputs, outputs\n",
    "        return model_output, hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "        return [(Variable(weight.new(1, bsz, self.nhid if l != self.nlayers - 1 else self.nhidlast).zero_()),\n",
    "                 Variable(weight.new(1, bsz, self.nhid if l != self.nlayers - 1 else self.nhidlast).zero_()))\n",
    "                for l in range(self.nlayers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Arguments\n",
    "###############################################################################\n",
    "import arguments\n",
    "# Get arguments - defaultPennParameters(), defaultWT2Parameters(), or custom\n",
    "#args = arguments.defaultPennParameters()\n",
    "\n",
    "\n",
    "\n",
    "def new_params(args_str):\n",
    "    a = arguments.parser.parse_args(args_str.split())\n",
    "    \n",
    "    if a.nhidlast < 0 or not (a.mos or a.moc):\n",
    "        a.nhidlast = a.emsize\n",
    "    if a.dropoutl < 0:\n",
    "        a.dropoutl = a.dropouth\n",
    "    if a.small_batch_size < 0:\n",
    "        a.small_batch_size = a.batch_size\n",
    "        \n",
    "    # Set the random seed manually for reproducibility.\n",
    "    np.random.seed(a.seed)\n",
    "    torch.manual_seed(a.seed)\n",
    "    if torch.cuda.is_available():\n",
    "        if not a.cuda:\n",
    "            print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
    "        else:\n",
    "            torch.cuda.manual_seed_all(a.seed)\n",
    "            \n",
    "    if not a.continue_train:\n",
    "        a.save = '{}-{}'.format(a.save, time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "        create_exp_dir(a.save, scripts_to_save=['main.py', 'model.py'])\n",
    "    \n",
    "    return a\n",
    "    \n",
    "def logging(s, print_=True, log_=True):\n",
    "    if print_:\n",
    "        print(s)\n",
    "    if log_:\n",
    "        with open(os.path.join(args.save, 'log.txt'), 'a+') as f_log:\n",
    "            f_log.write(s + '\\n')\n",
    "            \n",
    "def serialize(folder, var, var_name):\n",
    "    with open(os.path.join(folder, var_name + '.pkl'), 'wb') as f:\n",
    "        pickle.dump(var, f, protocol=2)\n",
    "        \n",
    "def unserialize(folder, var_name):\n",
    "    with open(os.path.join(folder, var_name + '.pkl'), 'rb') as f:\n",
    "        v = pickle.load(f)\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment dir : EXP-20180205-002751\n",
      "torch.Size([21764, 10])\n",
      "torch.Size([245569, 1])\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# Load data\n",
    "###############################################################################\n",
    "#args = new_params(\"--data data/penn\")\n",
    "args = new_params(\"--data data/wikitext-2\")\n",
    "corpus = data.Corpus(args.data)\n",
    "\n",
    "eval_batch_size = 10\n",
    "test_batch_size = 1\n",
    "val_data = batchify(corpus.valid, eval_batch_size, args)\n",
    "test_data = batchify(corpus.test, test_batch_size, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Build the model\n",
    "###############################################################################\n",
    "def new_model(args, corpus):\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    if args.continue_train:\n",
    "        model = torch.load(os.path.join(args.save, 'model.pt'))\n",
    "    else:\n",
    "        model = RNNModel(args.model, ntokens, args.emsize, args.nhid, args.nhidlast, args.nlayers, \n",
    "                           args.dropout, args.dropouth, args.dropouti, args.dropoute, args.wdrop, \n",
    "                           args.tied, args.dropoutl, args.n_experts, args.moc, args.mos)\n",
    "\n",
    "    if args.cuda:\n",
    "        if args.single_gpu:\n",
    "            parallel_model = model.cuda()\n",
    "        else:\n",
    "            parallel_model = nn.DataParallel(model, dim=1).cuda()\n",
    "    else:\n",
    "        parallel_model = model\n",
    "\n",
    "    total_params = sum(x.data.nelement() for x in model.parameters())\n",
    "    logging('Args: {}'.format(args))\n",
    "    logging('Model total parameters: {}'.format(total_params))\n",
    "    \n",
    "    return model, parallel_model\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Training code\n",
    "###############################################################################\n",
    "\n",
    "def evaluate(model, parallel_model, corpus, args, data_source, batch_size=10):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    for i in range(0, data_source.size(0) - 1, args.bptt):\n",
    "        data, targets = get_batch(data_source, i, args, evaluation=True)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        log_prob, hidden = parallel_model(data, hidden)\n",
    "\n",
    "        loss = nn.functional.nll_loss(log_prob.view(-1, log_prob.size(2)), targets).data\n",
    "\n",
    "        total_loss += loss * len(data)\n",
    "\n",
    "        hidden = repackage_hidden(hidden)\n",
    "    return total_loss[0] / len(data_source)\n",
    "\n",
    "\n",
    "def train(model, parallel_model, optimizer, args, corpus, history, train_state):\n",
    "    assert args.batch_size % args.small_batch_size == 0, 'batch_size must be divisible by small_batch_size'\n",
    "    # Turn on training mode which enables dropout.\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = [model.init_hidden(args.small_batch_size) for _ in range(args.batch_size // args.small_batch_size)]\n",
    "    batch, i = 0, 0\n",
    "    training_loss, log_iter = 0, 1\n",
    "    training_epoch_loss, losses_iter = 0, 0\n",
    "    \n",
    "    while i < train_data.size(0) - 1 - 1:\n",
    "        bptt = args.bptt if np.random.random() < 0.95 else args.bptt / 2.\n",
    "        # Prevent excessively small or negative sequence lengths\n",
    "        seq_len = max(5, int(np.random.normal(bptt, 5)))\n",
    "        # There's a very small chance that it could select a very long sequence length resulting in OOM\n",
    "        seq_len = min(seq_len, args.bptt + args.max_seq_len_delta)\n",
    "\n",
    "        lr2 = optimizer.param_groups[0]['lr']\n",
    "        optimizer.param_groups[0]['lr'] = lr2 * seq_len / args.bptt\n",
    "        model.train()\n",
    "        data, targets = get_batch(train_data, i, args, seq_len=seq_len)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        start, end, s_id = 0, args.small_batch_size, 0\n",
    "        while start < args.batch_size:\n",
    "            cur_data, cur_targets = data[:, start: end], targets[:, start: end].contiguous().view(-1)\n",
    "\n",
    "            # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "            # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "            hidden[s_id] = repackage_hidden(hidden[s_id])\n",
    "\n",
    "            log_prob, hidden[s_id], rnn_hs, dropped_rnn_hs = parallel_model(cur_data, hidden[s_id], return_h=True)\n",
    "            raw_loss = nn.functional.nll_loss(log_prob.view(-1, log_prob.size(2)), cur_targets)\n",
    "            training_epoch_loss += raw_loss.data[0]\n",
    "            losses_iter += 1\n",
    "            \n",
    "            loss = raw_loss\n",
    "            # Activiation Regularization\n",
    "            loss +=  sum(args.alpha * dropped_rnn_h.pow(2).mean() for dropped_rnn_h in dropped_rnn_hs[-1:])\n",
    "            # Temporal Activation Regularization (slowness)\n",
    "            loss += sum(args.beta * (rnn_h[1:] - rnn_h[:-1]).pow(2).mean() for rnn_h in rnn_hs[-1:])\n",
    "            loss *= args.small_batch_size / args.batch_size\n",
    "            total_loss += raw_loss.data * args.small_batch_size / args.batch_size\n",
    "            loss.backward()\n",
    "\n",
    "            s_id += 1\n",
    "            start = end\n",
    "            end = start + args.small_batch_size\n",
    "\n",
    "            gc.collect()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # total_loss += raw_loss.data\n",
    "        optimizer.param_groups[0]['lr'] = lr2\n",
    "            \n",
    "        if batch % args.log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss[0] / args.log_interval\n",
    "            #history[\"train_errs\"].append((train_state.iteration, cur_loss))\n",
    "            training_loss += cur_loss\n",
    "            elapsed = time.time() - start_time\n",
    "            logging('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                train_state.epoch, batch, len(train_data) // args.bptt, optimizer.param_groups[0]['lr'],\n",
    "                elapsed * 1000 / args.log_interval, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "        ###\n",
    "        batch += 1\n",
    "        i += seq_len\n",
    "        train_state.iteration += 1\n",
    "    history[\"train_errs\"].append(float(training_epoch_loss)/losses_iter)\n",
    "\n",
    "class TrainState:\n",
    "    def __init__(self):\n",
    "        self.iteration = 1\n",
    "        self.epoch = 1\n",
    "        \n",
    "def SGD(model, parallel_model, args, corpus):\n",
    "    # Loop over epochs.\n",
    "    train_state = TrainState()\n",
    "    saved_iteration = 0\n",
    "    lr = args.lr\n",
    "    best_val_loss = []\n",
    "    stored_loss = 100000000\n",
    "    history = {\"train_errs\":[], \"val_errs\":[], \"val_errs2\":[]}\n",
    "\n",
    "    # At any point you can hit Ctrl + C to break out of training early.\n",
    "    try:\n",
    "        if args.continue_train:\n",
    "            optimizer_state = torch.load(os.path.join(args.save, 'optimizer.pt'))\n",
    "            train_state = unserialize(args.save, 'train_state')\n",
    "            history = unserialize(args.save, 'history')\n",
    "            if 't0' in optimizer_state['param_groups'][0]:\n",
    "                optimizer = torch.optim.ASGD(model.parameters(), lr=args.lr, t0=0, lambd=0., weight_decay=args.wdecay)\n",
    "            else:\n",
    "                optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.wdecay)\n",
    "            optimizer.load_state_dict(optimizer_state)\n",
    "        else:\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.wdecay)\n",
    "\n",
    "        for epoch in range(train_state.epoch, args.epochs+1):\n",
    "            epoch_start_time = time.time()\n",
    "            train_state.epoch = epoch\n",
    "            train(model, parallel_model, optimizer, args, corpus, history, train_state)\n",
    "            if 't0' in optimizer.param_groups[0]:\n",
    "                tmp = {}\n",
    "                for prm in model.parameters():\n",
    "                    tmp[prm] = prm.data.clone()\n",
    "                    prm.data = optimizer.state[prm]['ax'].clone()\n",
    "\n",
    "                val_loss2 = evaluate(model, parallel_model, corpus, args, val_data, eval_batch_size)\n",
    "                history[\"val_errs2\"].append((train_state.iteration, val_loss2))\n",
    "                logging('-' * 89)\n",
    "                logging('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "                        'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                                   val_loss2, math.exp(val_loss2)))\n",
    "                logging('-' * 89)\n",
    "\n",
    "                if val_loss2 < stored_loss:\n",
    "                    save_checkpoint(model, optimizer, args.save)\n",
    "                    logging('Saving Averaged!')\n",
    "                    stored_loss = val_loss2\n",
    "\n",
    "                for prm in model.parameters():\n",
    "                    prm.data = tmp[prm].clone()\n",
    "\n",
    "            else:\n",
    "                val_loss = evaluate(model, parallel_model, corpus, args, val_data, eval_batch_size)\n",
    "                history[\"val_errs\"].append((train_state.iteration, val_loss))\n",
    "                logging('-' * 89)\n",
    "                logging('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "                        'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                                   val_loss, math.exp(val_loss)))\n",
    "                logging('-' * 89)\n",
    "\n",
    "                if val_loss < stored_loss:\n",
    "                    save_checkpoint(model, optimizer, args.save)\n",
    "                    logging('Saving Normal!')\n",
    "                    stored_loss = val_loss\n",
    "\n",
    "                if 't0' not in optimizer.param_groups[0] and (len(best_val_loss)>args.nonmono and val_loss > min(best_val_loss[:-args.nonmono])):\n",
    "                    logging('Switching!')\n",
    "                    optimizer = torch.optim.ASGD(model.parameters(), lr=args.lr, t0=0, lambd=0., weight_decay=args.wdecay)\n",
    "                    #optimizer.param_groups[0]['lr'] /= 2.\n",
    "                best_val_loss.append(val_loss)\n",
    "            \n",
    "        saved_iteration = train_state.iteration\n",
    "        serialize(args.save, history, 'history')\n",
    "        serialize(args.save, train_state, 'train_state')\n",
    "        eval_test(corpus, args)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        logging('-' * 89)\n",
    "        logging('Exiting from training early')\n",
    "        train_state.iteration = saved_iteration\n",
    "        serialize(args.save, history, 'history')\n",
    "        serialize(args.save, train_state, 'train_state')\n",
    "        eval_test(corpus, args)\n",
    "\n",
    "def eval_test(corpus, args):\n",
    "    # Load the best saved model.\n",
    "    model = torch.load(os.path.join(args.save, 'model.pt'))\n",
    "    parallel_model = nn.DataParallel(model, dim=1).cuda()\n",
    "\n",
    "    # Run on test data.\n",
    "    test_loss = evaluate(model, parallel_model, corpus, args, test_data, test_batch_size)\n",
    "    logging('=' * 89)\n",
    "    logging('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "        test_loss, math.exp(test_loss)))\n",
    "    logging('=' * 89)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'PTB-20180119-065855/history.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-0b635f8f424c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PTB-20180119-065855\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"history\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train_errs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mhist2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val_errs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mhist3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val_errs2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-61328817df15>\u001b[0m in \u001b[0;36munserialize\u001b[0;34m(folder, var_name)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0munserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'PTB-20180119-065855/history.pkl'"
     ]
    }
   ],
   "source": [
    "history = unserialize(\"PTB-20180119-065855\", \"history\")\n",
    "hist = np.array(history[\"train_errs\"])\n",
    "hist2 = np.array(history[\"val_errs\"])\n",
    "hist3 = np.array(history[\"val_errs2\"])\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.plot(hist[:,0], hist[:,1], label='batch train loss')\n",
    "plt.plot(hist2[:,0], hist2[:,1], label='val train loss')\n",
    "plt.plot(hist3[:,0], hist3[:,1], label='val2 train loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#history = unserialize(\"Experiments/PTBRepro\", 'history')\n",
    "#state = unserialize(\"Experiments/PTBRepro\", 'train_state')\n",
    "\n",
    "#print(history['train_errs'][-1])\n",
    "#print(state.epoch)\n",
    "\n",
    "#--data data/penn --dropouti 0.4 --dropoutl 0.29 --dropouth 0.225 --seed 28 --batch_size 12 --lr 20.0 --epoch 1000 --nhid 960 --nhidlast 620 --emsize 280 --n_experts 15 --save PTB/PTB --single_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment dir : Experiments/MoS2-20180120-215158\n",
      "torch.Size([77465, 12])\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n",
      "Model param size: 21500620\n",
      "Args: Namespace(alpha=2, batch_size=12, beta=1, bptt=70, clip=0.25, continue_train=False, cuda=True, data='data/penn', dropout=0.4, dropoute=0.1, dropouth=0.225, dropouti=0.4, dropoutl=0.29, emsize=280, epochs=150, log_interval=200, lr=20.0, max_seq_len_delta=15, moc=False, model='LSTM', mos=True, n_experts=15, nhid=960, nhidlast=620, nlayers=3, nonmono=5, save='Experiments/MoS2-20180120-215158', seed=20, single_gpu=False, small_batch_size=12, tied=True, wdecay=1.2e-06, wdrop=0.5)\n",
      "Model total parameters: 21500620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pio/os/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py:224: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greately increasing memory usage. To compact weights again call flatten_parameters().\n",
      "  result = self.forward(*input, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 1106 batches | lr 20.00 | ms/batch 160.52 | loss  6.96 | ppl  1050.87\n",
      "| epoch   1 |   400/ 1106 batches | lr 20.00 | ms/batch 160.19 | loss  6.62 | ppl   750.65\n",
      "| epoch   1 |   600/ 1106 batches | lr 20.00 | ms/batch 161.74 | loss  6.40 | ppl   599.30\n",
      "| epoch   1 |   800/ 1106 batches | lr 20.00 | ms/batch 160.81 | loss  6.21 | ppl   496.24\n",
      "| epoch   1 |  1000/ 1106 batches | lr 20.00 | ms/batch 165.30 | loss  6.01 | ppl   405.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 189.79s | valid loss  5.80 | valid ppl   330.08\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pio/os/anaconda3/lib/python3.6/site-packages/torch/serialization.py:147: UserWarning: Couldn't retrieve source code for container of type RNNModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Normal!\n",
      "| epoch   2 |   200/ 1106 batches | lr 20.00 | ms/batch 163.22 | loss  5.78 | ppl   323.54\n",
      "| epoch   2 |   400/ 1106 batches | lr 20.00 | ms/batch 163.97 | loss  5.63 | ppl   278.61\n",
      "| epoch   2 |   600/ 1106 batches | lr 20.00 | ms/batch 163.92 | loss  5.53 | ppl   251.76\n",
      "| epoch   2 |   800/ 1106 batches | lr 20.00 | ms/batch 160.29 | loss  5.51 | ppl   246.62\n",
      "| epoch   2 |  1000/ 1106 batches | lr 20.00 | ms/batch 160.58 | loss  5.46 | ppl   235.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 191.83s | valid loss  5.26 | valid ppl   193.30\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   3 |   200/ 1106 batches | lr 20.00 | ms/batch 162.11 | loss  5.39 | ppl   220.29\n",
      "| epoch   3 |   400/ 1106 batches | lr 20.00 | ms/batch 164.03 | loss  5.28 | ppl   197.11\n",
      "| epoch   3 |   600/ 1106 batches | lr 20.00 | ms/batch 164.64 | loss  5.24 | ppl   188.14\n",
      "| epoch   3 |   800/ 1106 batches | lr 20.00 | ms/batch 165.29 | loss  5.24 | ppl   189.28\n",
      "| epoch   3 |  1000/ 1106 batches | lr 20.00 | ms/batch 163.00 | loss  5.22 | ppl   185.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 192.12s | valid loss  5.04 | valid ppl   154.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   4 |   200/ 1106 batches | lr 20.00 | ms/batch 164.06 | loss  5.19 | ppl   179.72\n",
      "| epoch   4 |   400/ 1106 batches | lr 20.00 | ms/batch 162.41 | loss  5.06 | ppl   157.85\n",
      "| epoch   4 |   600/ 1106 batches | lr 20.00 | ms/batch 162.02 | loss  5.05 | ppl   155.36\n",
      "| epoch   4 |   800/ 1106 batches | lr 20.00 | ms/batch 162.90 | loss  5.06 | ppl   157.05\n",
      "| epoch   4 |  1000/ 1106 batches | lr 20.00 | ms/batch 164.87 | loss  5.06 | ppl   157.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 192.48s | valid loss  4.92 | valid ppl   137.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   5 |   200/ 1106 batches | lr 20.00 | ms/batch 163.53 | loss  5.04 | ppl   154.61\n",
      "| epoch   5 |   400/ 1106 batches | lr 20.00 | ms/batch 162.77 | loss  4.92 | ppl   137.39\n",
      "| epoch   5 |   600/ 1106 batches | lr 20.00 | ms/batch 160.01 | loss  4.90 | ppl   134.36\n",
      "| epoch   5 |   800/ 1106 batches | lr 20.00 | ms/batch 161.99 | loss  4.92 | ppl   136.38\n",
      "| epoch   5 |  1000/ 1106 batches | lr 20.00 | ms/batch 162.17 | loss  4.93 | ppl   138.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 192.11s | valid loss  4.83 | valid ppl   124.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   6 |   200/ 1106 batches | lr 20.00 | ms/batch 162.67 | loss  4.92 | ppl   137.30\n",
      "| epoch   6 |   400/ 1106 batches | lr 20.00 | ms/batch 159.92 | loss  4.79 | ppl   120.58\n",
      "| epoch   6 |   600/ 1106 batches | lr 20.00 | ms/batch 163.65 | loss  4.79 | ppl   119.91\n",
      "| epoch   6 |   800/ 1106 batches | lr 20.00 | ms/batch 164.73 | loss  4.80 | ppl   121.87\n",
      "| epoch   6 |  1000/ 1106 batches | lr 20.00 | ms/batch 164.69 | loss  4.84 | ppl   126.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 192.71s | valid loss  4.73 | valid ppl   113.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   7 |   200/ 1106 batches | lr 20.00 | ms/batch 163.70 | loss  4.83 | ppl   125.57\n",
      "| epoch   7 |   400/ 1106 batches | lr 20.00 | ms/batch 159.11 | loss  4.72 | ppl   112.01\n",
      "| epoch   7 |   600/ 1106 batches | lr 20.00 | ms/batch 162.31 | loss  4.71 | ppl   110.95\n",
      "| epoch   7 |   800/ 1106 batches | lr 20.00 | ms/batch 161.26 | loss  4.72 | ppl   112.06\n",
      "| epoch   7 |  1000/ 1106 batches | lr 20.00 | ms/batch 163.91 | loss  4.77 | ppl   117.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 192.49s | valid loss  4.68 | valid ppl   107.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   8 |   200/ 1106 batches | lr 20.00 | ms/batch 162.33 | loss  4.75 | ppl   115.72\n",
      "| epoch   8 |   400/ 1106 batches | lr 20.00 | ms/batch 162.30 | loss  4.66 | ppl   105.19\n",
      "| epoch   8 |   600/ 1106 batches | lr 20.00 | ms/batch 163.68 | loss  4.64 | ppl   103.69\n",
      "| epoch   8 |   800/ 1106 batches | lr 20.00 | ms/batch 161.51 | loss  4.66 | ppl   105.24\n",
      "| epoch   8 |  1000/ 1106 batches | lr 20.00 | ms/batch 161.18 | loss  4.70 | ppl   109.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 192.37s | valid loss  4.63 | valid ppl   102.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   9 |   200/ 1106 batches | lr 20.00 | ms/batch 163.16 | loss  4.69 | ppl   108.95\n",
      "| epoch   9 |   400/ 1106 batches | lr 20.00 | ms/batch 164.11 | loss  4.57 | ppl    96.69\n",
      "| epoch   9 |   600/ 1106 batches | lr 20.00 | ms/batch 160.66 | loss  4.58 | ppl    97.61\n",
      "| epoch   9 |   800/ 1106 batches | lr 20.00 | ms/batch 158.78 | loss  4.59 | ppl    98.93\n",
      "| epoch   9 |  1000/ 1106 batches | lr 20.00 | ms/batch 161.81 | loss  4.64 | ppl   103.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 191.74s | valid loss  4.60 | valid ppl    99.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  10 |   200/ 1106 batches | lr 20.00 | ms/batch 166.02 | loss  4.64 | ppl   103.20\n",
      "| epoch  10 |   400/ 1106 batches | lr 20.00 | ms/batch 161.65 | loss  4.52 | ppl    91.85\n",
      "| epoch  10 |   600/ 1106 batches | lr 20.00 | ms/batch 161.08 | loss  4.52 | ppl    92.02\n",
      "| epoch  10 |   800/ 1106 batches | lr 20.00 | ms/batch 159.14 | loss  4.54 | ppl    93.47\n",
      "| epoch  10 |  1000/ 1106 batches | lr 20.00 | ms/batch 162.30 | loss  4.58 | ppl    97.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 191.26s | valid loss  4.56 | valid ppl    95.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  11 |   200/ 1106 batches | lr 20.00 | ms/batch 164.33 | loss  4.60 | ppl    99.38\n",
      "| epoch  11 |   400/ 1106 batches | lr 20.00 | ms/batch 161.35 | loss  4.48 | ppl    88.03\n",
      "| epoch  11 |   600/ 1106 batches | lr 20.00 | ms/batch 160.36 | loss  4.49 | ppl    88.69\n",
      "| epoch  11 |   800/ 1106 batches | lr 20.00 | ms/batch 163.48 | loss  4.51 | ppl    91.10\n",
      "| epoch  11 |  1000/ 1106 batches | lr 20.00 | ms/batch 164.25 | loss  4.53 | ppl    93.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 191.79s | valid loss  4.54 | valid ppl    93.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  12 |   200/ 1106 batches | lr 20.00 | ms/batch 162.90 | loss  4.55 | ppl    94.86\n",
      "| epoch  12 |   400/ 1106 batches | lr 20.00 | ms/batch 162.49 | loss  4.43 | ppl    84.05\n",
      "| epoch  12 |   600/ 1106 batches | lr 20.00 | ms/batch 163.18 | loss  4.44 | ppl    84.88\n",
      "| epoch  12 |   800/ 1106 batches | lr 20.00 | ms/batch 164.71 | loss  4.47 | ppl    87.23\n",
      "| epoch  12 |  1000/ 1106 batches | lr 20.00 | ms/batch 164.48 | loss  4.50 | ppl    90.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 192.53s | valid loss  4.51 | valid ppl    90.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  13 |   200/ 1106 batches | lr 20.00 | ms/batch 164.05 | loss  4.52 | ppl    92.10\n",
      "| epoch  13 |   400/ 1106 batches | lr 20.00 | ms/batch 163.22 | loss  4.39 | ppl    81.03\n",
      "| epoch  13 |   600/ 1106 batches | lr 20.00 | ms/batch 165.76 | loss  4.40 | ppl    81.59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  13 |   800/ 1106 batches | lr 20.00 | ms/batch 163.76 | loss  4.43 | ppl    83.70\n",
      "| epoch  13 |  1000/ 1106 batches | lr 20.00 | ms/batch 163.81 | loss  4.45 | ppl    85.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 192.09s | valid loss  4.49 | valid ppl    88.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  14 |   200/ 1106 batches | lr 20.00 | ms/batch 162.43 | loss  4.47 | ppl    87.14\n",
      "| epoch  14 |   400/ 1106 batches | lr 20.00 | ms/batch 162.96 | loss  4.37 | ppl    78.82\n",
      "| epoch  14 |   600/ 1106 batches | lr 20.00 | ms/batch 160.76 | loss  4.37 | ppl    79.22\n",
      "| epoch  14 |   800/ 1106 batches | lr 20.00 | ms/batch 162.68 | loss  4.40 | ppl    81.16\n",
      "| epoch  14 |  1000/ 1106 batches | lr 20.00 | ms/batch 163.26 | loss  4.43 | ppl    83.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 192.13s | valid loss  4.47 | valid ppl    87.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  15 |   200/ 1106 batches | lr 20.00 | ms/batch 162.07 | loss  4.45 | ppl    85.49\n",
      "| epoch  15 |   400/ 1106 batches | lr 20.00 | ms/batch 162.88 | loss  4.34 | ppl    76.59\n",
      "| epoch  15 |   600/ 1106 batches | lr 20.00 | ms/batch 162.94 | loss  4.34 | ppl    77.02\n",
      "| epoch  15 |   800/ 1106 batches | lr 20.00 | ms/batch 162.98 | loss  4.38 | ppl    79.68\n",
      "| epoch  15 |  1000/ 1106 batches | lr 20.00 | ms/batch 162.69 | loss  4.40 | ppl    81.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 192.17s | valid loss  4.45 | valid ppl    85.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  16 |   200/ 1106 batches | lr 20.00 | ms/batch 162.36 | loss  4.43 | ppl    83.78\n",
      "| epoch  16 |   400/ 1106 batches | lr 20.00 | ms/batch 164.14 | loss  4.31 | ppl    74.73\n",
      "| epoch  16 |   600/ 1106 batches | lr 20.00 | ms/batch 164.03 | loss  4.31 | ppl    74.73\n",
      "| epoch  16 |   800/ 1106 batches | lr 20.00 | ms/batch 164.23 | loss  4.34 | ppl    76.98\n",
      "| epoch  16 |  1000/ 1106 batches | lr 20.00 | ms/batch 162.13 | loss  4.37 | ppl    79.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 192.13s | valid loss  4.44 | valid ppl    84.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  17 |   200/ 1106 batches | lr 20.00 | ms/batch 163.73 | loss  4.40 | ppl    81.37\n",
      "| epoch  17 |   400/ 1106 batches | lr 20.00 | ms/batch 163.94 | loss  4.29 | ppl    72.68\n",
      "| epoch  17 |   600/ 1106 batches | lr 20.00 | ms/batch 163.17 | loss  4.29 | ppl    72.87\n",
      "| epoch  17 |   800/ 1106 batches | lr 20.00 | ms/batch 161.36 | loss  4.32 | ppl    75.53\n",
      "| epoch  17 |  1000/ 1106 batches | lr 20.00 | ms/batch 163.25 | loss  4.35 | ppl    77.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 191.88s | valid loss  4.42 | valid ppl    83.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  18 |   200/ 1106 batches | lr 20.00 | ms/batch 162.27 | loss  4.37 | ppl    79.00\n",
      "| epoch  18 |   400/ 1106 batches | lr 20.00 | ms/batch 162.10 | loss  4.26 | ppl    70.57\n",
      "| epoch  18 |   600/ 1106 batches | lr 20.00 | ms/batch 164.36 | loss  4.27 | ppl    71.57\n",
      "| epoch  18 |   800/ 1106 batches | lr 20.00 | ms/batch 163.18 | loss  4.29 | ppl    73.03\n",
      "| epoch  18 |  1000/ 1106 batches | lr 20.00 | ms/batch 160.78 | loss  4.33 | ppl    76.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 192.05s | valid loss  4.41 | valid ppl    82.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  19 |   200/ 1106 batches | lr 20.00 | ms/batch 165.36 | loss  4.35 | ppl    77.53\n",
      "| epoch  19 |   400/ 1106 batches | lr 20.00 | ms/batch 164.45 | loss  4.24 | ppl    69.22\n",
      "| epoch  19 |   600/ 1106 batches | lr 20.00 | ms/batch 162.62 | loss  4.23 | ppl    69.04\n",
      "| epoch  19 |   800/ 1106 batches | lr 20.00 | ms/batch 161.80 | loss  4.29 | ppl    72.96\n",
      "| epoch  19 |  1000/ 1106 batches | lr 20.00 | ms/batch 164.25 | loss  4.31 | ppl    74.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 191.77s | valid loss  4.41 | valid ppl    82.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  20 |   200/ 1106 batches | lr 20.00 | ms/batch 163.90 | loss  4.32 | ppl    75.37\n",
      "| epoch  20 |   400/ 1106 batches | lr 20.00 | ms/batch 165.12 | loss  4.22 | ppl    68.03\n",
      "| epoch  20 |   600/ 1106 batches | lr 20.00 | ms/batch 165.89 | loss  4.23 | ppl    68.86\n",
      "| epoch  20 |   800/ 1106 batches | lr 20.00 | ms/batch 160.09 | loss  4.25 | ppl    70.27\n",
      "| epoch  20 |  1000/ 1106 batches | lr 20.00 | ms/batch 160.45 | loss  4.28 | ppl    72.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 192.13s | valid loss  4.40 | valid ppl    81.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  21 |   200/ 1106 batches | lr 20.00 | ms/batch 160.58 | loss  4.32 | ppl    74.97\n",
      "| epoch  21 |   400/ 1106 batches | lr 20.00 | ms/batch 163.23 | loss  4.21 | ppl    67.32\n",
      "| epoch  21 |   600/ 1106 batches | lr 20.00 | ms/batch 165.78 | loss  4.21 | ppl    67.22\n",
      "| epoch  21 |   800/ 1106 batches | lr 20.00 | ms/batch 163.72 | loss  4.24 | ppl    69.46\n",
      "| epoch  21 |  1000/ 1106 batches | lr 20.00 | ms/batch 163.15 | loss  4.28 | ppl    72.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time: 192.11s | valid loss  4.38 | valid ppl    79.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  22 |   200/ 1106 batches | lr 20.00 | ms/batch 162.11 | loss  4.30 | ppl    73.58\n",
      "| epoch  22 |   400/ 1106 batches | lr 20.00 | ms/batch 161.68 | loss  4.17 | ppl    64.73\n",
      "| epoch  22 |   600/ 1106 batches | lr 20.00 | ms/batch 161.99 | loss  4.20 | ppl    66.56\n",
      "| epoch  22 |   800/ 1106 batches | lr 20.00 | ms/batch 163.58 | loss  4.23 | ppl    68.49\n",
      "| epoch  22 |  1000/ 1106 batches | lr 20.00 | ms/batch 163.50 | loss  4.26 | ppl    71.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time: 191.99s | valid loss  4.37 | valid ppl    79.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  23 |   200/ 1106 batches | lr 20.00 | ms/batch 161.97 | loss  4.28 | ppl    72.26\n",
      "| epoch  23 |   400/ 1106 batches | lr 20.00 | ms/batch 162.89 | loss  4.15 | ppl    63.70\n",
      "| epoch  23 |   600/ 1106 batches | lr 20.00 | ms/batch 162.80 | loss  4.18 | ppl    65.22\n",
      "| epoch  23 |   800/ 1106 batches | lr 20.00 | ms/batch 161.03 | loss  4.20 | ppl    66.97\n",
      "| epoch  23 |  1000/ 1106 batches | lr 20.00 | ms/batch 162.82 | loss  4.24 | ppl    69.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time: 192.02s | valid loss  4.36 | valid ppl    78.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  24 |   200/ 1106 batches | lr 20.00 | ms/batch 161.60 | loss  4.26 | ppl    70.73\n",
      "| epoch  24 |   400/ 1106 batches | lr 20.00 | ms/batch 163.04 | loss  4.14 | ppl    62.77\n",
      "| epoch  24 |   600/ 1106 batches | lr 20.00 | ms/batch 164.00 | loss  4.16 | ppl    64.29\n",
      "| epoch  24 |   800/ 1106 batches | lr 20.00 | ms/batch 163.00 | loss  4.19 | ppl    65.93\n",
      "| epoch  24 |  1000/ 1106 batches | lr 20.00 | ms/batch 162.34 | loss  4.23 | ppl    68.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time: 192.30s | valid loss  4.36 | valid ppl    78.26\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Normal!\n",
      "| epoch  25 |   200/ 1106 batches | lr 20.00 | ms/batch 164.45 | loss  4.25 | ppl    69.96\n",
      "| epoch  25 |   400/ 1106 batches | lr 20.00 | ms/batch 163.35 | loss  4.12 | ppl    61.80\n",
      "| epoch  25 |   600/ 1106 batches | lr 20.00 | ms/batch 163.82 | loss  4.14 | ppl    62.98\n",
      "| epoch  25 |   800/ 1106 batches | lr 20.00 | ms/batch 163.32 | loss  4.18 | ppl    65.05\n",
      "| epoch  25 |  1000/ 1106 batches | lr 20.00 | ms/batch 162.04 | loss  4.21 | ppl    67.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time: 191.94s | valid loss  4.35 | valid ppl    77.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  26 |   200/ 1106 batches | lr 20.00 | ms/batch 163.54 | loss  4.22 | ppl    68.10\n",
      "| epoch  26 |   400/ 1106 batches | lr 20.00 | ms/batch 164.86 | loss  4.12 | ppl    61.55\n",
      "| epoch  26 |   600/ 1106 batches | lr 20.00 | ms/batch 163.44 | loss  4.14 | ppl    62.88\n",
      "| epoch  26 |   800/ 1106 batches | lr 20.00 | ms/batch 162.02 | loss  4.18 | ppl    65.07\n",
      "| epoch  26 |  1000/ 1106 batches | lr 20.00 | ms/batch 160.93 | loss  4.20 | ppl    66.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time: 192.43s | valid loss  4.34 | valid ppl    76.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  27 |   200/ 1106 batches | lr 20.00 | ms/batch 166.18 | loss  4.23 | ppl    68.45\n",
      "| epoch  27 |   400/ 1106 batches | lr 20.00 | ms/batch 163.50 | loss  4.10 | ppl    60.55\n",
      "| epoch  27 |   600/ 1106 batches | lr 20.00 | ms/batch 162.02 | loss  4.12 | ppl    61.83\n",
      "| epoch  27 |   800/ 1106 batches | lr 20.00 | ms/batch 164.59 | loss  4.15 | ppl    63.21\n",
      "| epoch  27 |  1000/ 1106 batches | lr 20.00 | ms/batch 164.53 | loss  4.18 | ppl    65.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time: 191.97s | valid loss  4.35 | valid ppl    77.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  28 |   200/ 1106 batches | lr 20.00 | ms/batch 160.37 | loss  4.21 | ppl    67.23\n",
      "| epoch  28 |   400/ 1106 batches | lr 20.00 | ms/batch 162.12 | loss  4.10 | ppl    60.09\n",
      "| epoch  28 |   600/ 1106 batches | lr 20.00 | ms/batch 161.02 | loss  4.12 | ppl    61.30\n",
      "| epoch  28 |   800/ 1106 batches | lr 20.00 | ms/batch 163.50 | loss  4.14 | ppl    62.79\n",
      "| epoch  28 |  1000/ 1106 batches | lr 20.00 | ms/batch 159.88 | loss  4.18 | ppl    65.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time: 192.37s | valid loss  4.33 | valid ppl    76.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  29 |   200/ 1106 batches | lr 20.00 | ms/batch 163.32 | loss  4.20 | ppl    66.35\n",
      "| epoch  29 |   400/ 1106 batches | lr 20.00 | ms/batch 162.33 | loss  4.08 | ppl    59.25\n",
      "| epoch  29 |   600/ 1106 batches | lr 20.00 | ms/batch 162.67 | loss  4.09 | ppl    59.78\n",
      "| epoch  29 |   800/ 1106 batches | lr 20.00 | ms/batch 162.35 | loss  4.12 | ppl    61.44\n",
      "| epoch  29 |  1000/ 1106 batches | lr 20.00 | ms/batch 159.82 | loss  4.17 | ppl    64.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time: 192.01s | valid loss  4.33 | valid ppl    75.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  30 |   200/ 1106 batches | lr 20.00 | ms/batch 163.00 | loss  4.17 | ppl    64.49\n",
      "| epoch  30 |   400/ 1106 batches | lr 20.00 | ms/batch 162.55 | loss  4.07 | ppl    58.40\n",
      "| epoch  30 |   600/ 1106 batches | lr 20.00 | ms/batch 161.52 | loss  4.09 | ppl    59.65\n",
      "| epoch  30 |   800/ 1106 batches | lr 20.00 | ms/batch 161.09 | loss  4.11 | ppl    61.19\n",
      "| epoch  30 |  1000/ 1106 batches | lr 20.00 | ms/batch 162.36 | loss  4.16 | ppl    63.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time: 192.25s | valid loss  4.33 | valid ppl    76.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  31 |   200/ 1106 batches | lr 20.00 | ms/batch 165.86 | loss  4.17 | ppl    64.51\n",
      "| epoch  31 |   400/ 1106 batches | lr 20.00 | ms/batch 161.20 | loss  4.05 | ppl    57.61\n",
      "| epoch  31 |   600/ 1106 batches | lr 20.00 | ms/batch 162.62 | loss  4.09 | ppl    59.58\n",
      "| epoch  31 |   800/ 1106 batches | lr 20.00 | ms/batch 163.32 | loss  4.10 | ppl    60.18\n",
      "| epoch  31 |  1000/ 1106 batches | lr 20.00 | ms/batch 163.12 | loss  4.14 | ppl    63.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time: 192.34s | valid loss  4.33 | valid ppl    75.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  32 |   200/ 1106 batches | lr 20.00 | ms/batch 164.90 | loss  4.16 | ppl    63.85\n",
      "| epoch  32 |   400/ 1106 batches | lr 20.00 | ms/batch 160.71 | loss  4.05 | ppl    57.12\n",
      "| epoch  32 |   600/ 1106 batches | lr 20.00 | ms/batch 164.55 | loss  4.07 | ppl    58.78\n",
      "| epoch  32 |   800/ 1106 batches | lr 20.00 | ms/batch 163.55 | loss  4.10 | ppl    60.22\n",
      "| epoch  32 |  1000/ 1106 batches | lr 20.00 | ms/batch 161.57 | loss  4.12 | ppl    61.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time: 192.11s | valid loss  4.31 | valid ppl    74.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  33 |   200/ 1106 batches | lr 20.00 | ms/batch 164.86 | loss  4.15 | ppl    63.49\n",
      "| epoch  33 |   400/ 1106 batches | lr 20.00 | ms/batch 161.26 | loss  4.04 | ppl    56.84\n",
      "| epoch  33 |   600/ 1106 batches | lr 20.00 | ms/batch 160.95 | loss  4.06 | ppl    57.86\n",
      "| epoch  33 |   800/ 1106 batches | lr 20.00 | ms/batch 162.73 | loss  4.08 | ppl    59.08\n",
      "| epoch  33 |  1000/ 1106 batches | lr 20.00 | ms/batch 160.84 | loss  4.13 | ppl    62.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time: 192.45s | valid loss  4.31 | valid ppl    74.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  34 |   200/ 1106 batches | lr 20.00 | ms/batch 163.97 | loss  4.14 | ppl    62.75\n",
      "| epoch  34 |   400/ 1106 batches | lr 20.00 | ms/batch 162.88 | loss  4.03 | ppl    55.99\n",
      "| epoch  34 |   600/ 1106 batches | lr 20.00 | ms/batch 163.06 | loss  4.05 | ppl    57.48\n",
      "| epoch  34 |   800/ 1106 batches | lr 20.00 | ms/batch 162.15 | loss  4.08 | ppl    59.05\n",
      "| epoch  34 |  1000/ 1106 batches | lr 20.00 | ms/batch 166.21 | loss  4.11 | ppl    61.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | time: 192.11s | valid loss  4.30 | valid ppl    73.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  35 |   200/ 1106 batches | lr 20.00 | ms/batch 164.20 | loss  4.14 | ppl    62.57\n",
      "| epoch  35 |   400/ 1106 batches | lr 20.00 | ms/batch 160.94 | loss  4.02 | ppl    55.43\n",
      "| epoch  35 |   600/ 1106 batches | lr 20.00 | ms/batch 162.70 | loss  4.03 | ppl    56.49\n",
      "| epoch  35 |   800/ 1106 batches | lr 20.00 | ms/batch 162.17 | loss  4.06 | ppl    57.78\n",
      "| epoch  35 |  1000/ 1106 batches | lr 20.00 | ms/batch 163.80 | loss  4.09 | ppl    59.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | time: 192.15s | valid loss  4.30 | valid ppl    74.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  36 |   200/ 1106 batches | lr 20.00 | ms/batch 161.07 | loss  4.12 | ppl    61.66\n",
      "| epoch  36 |   400/ 1106 batches | lr 20.00 | ms/batch 162.43 | loss  4.02 | ppl    55.77\n",
      "| epoch  36 |   600/ 1106 batches | lr 20.00 | ms/batch 164.42 | loss  4.03 | ppl    56.23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  36 |   800/ 1106 batches | lr 20.00 | ms/batch 161.76 | loss  4.05 | ppl    57.60\n",
      "| epoch  36 |  1000/ 1106 batches | lr 20.00 | ms/batch 164.32 | loss  4.09 | ppl    59.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  36 | time: 192.16s | valid loss  4.30 | valid ppl    73.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  37 |   200/ 1106 batches | lr 20.00 | ms/batch 163.24 | loss  4.10 | ppl    60.57\n",
      "| epoch  37 |   400/ 1106 batches | lr 20.00 | ms/batch 164.80 | loss  4.00 | ppl    54.79\n",
      "| epoch  37 |   600/ 1106 batches | lr 20.00 | ms/batch 163.61 | loss  4.02 | ppl    55.66\n",
      "| epoch  37 |   800/ 1106 batches | lr 20.00 | ms/batch 164.90 | loss  4.06 | ppl    57.82\n",
      "| epoch  37 |  1000/ 1106 batches | lr 20.00 | ms/batch 162.97 | loss  4.08 | ppl    59.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  37 | time: 192.01s | valid loss  4.30 | valid ppl    73.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  38 |   200/ 1106 batches | lr 20.00 | ms/batch 162.53 | loss  4.11 | ppl    60.70\n",
      "| epoch  38 |   400/ 1106 batches | lr 20.00 | ms/batch 161.74 | loss  3.99 | ppl    54.09\n",
      "| epoch  38 |   600/ 1106 batches | lr 20.00 | ms/batch 163.95 | loss  4.02 | ppl    55.62\n",
      "| epoch  38 |   800/ 1106 batches | lr 20.00 | ms/batch 162.43 | loss  4.04 | ppl    56.61\n",
      "| epoch  38 |  1000/ 1106 batches | lr 20.00 | ms/batch 161.54 | loss  4.07 | ppl    58.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  38 | time: 192.14s | valid loss  4.29 | valid ppl    73.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  39 |   200/ 1106 batches | lr 20.00 | ms/batch 162.28 | loss  4.10 | ppl    60.32\n",
      "| epoch  39 |   400/ 1106 batches | lr 20.00 | ms/batch 159.65 | loss  3.99 | ppl    53.88\n",
      "| epoch  39 |   600/ 1106 batches | lr 20.00 | ms/batch 166.21 | loss  4.01 | ppl    55.36\n",
      "| epoch  39 |   800/ 1106 batches | lr 20.00 | ms/batch 161.82 | loss  4.04 | ppl    56.93\n",
      "| epoch  39 |  1000/ 1106 batches | lr 20.00 | ms/batch 162.93 | loss  4.07 | ppl    58.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  39 | time: 191.90s | valid loss  4.29 | valid ppl    72.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  40 |   200/ 1106 batches | lr 20.00 | ms/batch 165.44 | loss  4.08 | ppl    59.41\n",
      "| epoch  40 |   400/ 1106 batches | lr 20.00 | ms/batch 162.75 | loss  3.97 | ppl    53.03\n",
      "| epoch  40 |   600/ 1106 batches | lr 20.00 | ms/batch 162.06 | loss  4.00 | ppl    54.47\n",
      "| epoch  40 |   800/ 1106 batches | lr 20.00 | ms/batch 162.61 | loss  4.03 | ppl    56.24\n",
      "| epoch  40 |  1000/ 1106 batches | lr 20.00 | ms/batch 162.59 | loss  4.05 | ppl    57.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  40 | time: 192.16s | valid loss  4.29 | valid ppl    72.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  41 |   200/ 1106 batches | lr 20.00 | ms/batch 164.25 | loss  4.08 | ppl    58.90\n",
      "| epoch  41 |   400/ 1106 batches | lr 20.00 | ms/batch 164.35 | loss  3.98 | ppl    53.49\n",
      "| epoch  41 |   600/ 1106 batches | lr 20.00 | ms/batch 161.73 | loss  3.98 | ppl    53.50\n",
      "| epoch  41 |   800/ 1106 batches | lr 20.00 | ms/batch 163.50 | loss  4.03 | ppl    56.21\n",
      "| epoch  41 |  1000/ 1106 batches | lr 20.00 | ms/batch 163.11 | loss  4.04 | ppl    56.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  41 | time: 192.04s | valid loss  4.30 | valid ppl    73.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  42 |   200/ 1106 batches | lr 20.00 | ms/batch 164.36 | loss  4.07 | ppl    58.75\n",
      "| epoch  42 |   400/ 1106 batches | lr 20.00 | ms/batch 162.08 | loss  3.97 | ppl    53.09\n",
      "| epoch  42 |   600/ 1106 batches | lr 20.00 | ms/batch 163.17 | loss  3.97 | ppl    53.21\n",
      "| epoch  42 |   800/ 1106 batches | lr 20.00 | ms/batch 165.08 | loss  4.01 | ppl    55.14\n",
      "| epoch  42 |  1000/ 1106 batches | lr 20.00 | ms/batch 163.05 | loss  4.04 | ppl    56.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  42 | time: 192.07s | valid loss  4.29 | valid ppl    72.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  43 |   200/ 1106 batches | lr 20.00 | ms/batch 162.29 | loss  4.07 | ppl    58.44\n",
      "| epoch  43 |   400/ 1106 batches | lr 20.00 | ms/batch 163.01 | loss  3.95 | ppl    51.85\n",
      "| epoch  43 |   600/ 1106 batches | lr 20.00 | ms/batch 160.49 | loss  3.97 | ppl    53.05\n",
      "| epoch  43 |   800/ 1106 batches | lr 20.00 | ms/batch 164.05 | loss  4.00 | ppl    54.40\n",
      "| epoch  43 |  1000/ 1106 batches | lr 20.00 | ms/batch 162.08 | loss  4.04 | ppl    56.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  43 | time: 192.09s | valid loss  4.27 | valid ppl    71.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  44 |   200/ 1106 batches | lr 20.00 | ms/batch 165.31 | loss  4.07 | ppl    58.72\n",
      "| epoch  44 |   400/ 1106 batches | lr 20.00 | ms/batch 161.89 | loss  3.94 | ppl    51.51\n",
      "| epoch  44 |   600/ 1106 batches | lr 20.00 | ms/batch 159.78 | loss  3.97 | ppl    52.81\n",
      "| epoch  44 |   800/ 1106 batches | lr 20.00 | ms/batch 162.50 | loss  3.99 | ppl    54.01\n",
      "| epoch  44 |  1000/ 1106 batches | lr 20.00 | ms/batch 162.93 | loss  4.05 | ppl    57.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  44 | time: 192.10s | valid loss  4.27 | valid ppl    71.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  45 |   200/ 1106 batches | lr 20.00 | ms/batch 163.20 | loss  4.05 | ppl    57.38\n",
      "| epoch  45 |   400/ 1106 batches | lr 20.00 | ms/batch 163.21 | loss  3.94 | ppl    51.58\n",
      "| epoch  45 |   600/ 1106 batches | lr 20.00 | ms/batch 165.09 | loss  3.95 | ppl    52.16\n",
      "| epoch  45 |   800/ 1106 batches | lr 20.00 | ms/batch 163.04 | loss  3.99 | ppl    54.17\n",
      "| epoch  45 |  1000/ 1106 batches | lr 20.00 | ms/batch 161.38 | loss  4.03 | ppl    56.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  45 | time: 192.17s | valid loss  4.27 | valid ppl    71.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  46 |   200/ 1106 batches | lr 20.00 | ms/batch 164.62 | loss  4.05 | ppl    57.43\n",
      "| epoch  46 |   400/ 1106 batches | lr 20.00 | ms/batch 164.42 | loss  3.93 | ppl    50.98\n",
      "| epoch  46 |   600/ 1106 batches | lr 20.00 | ms/batch 162.91 | loss  3.95 | ppl    51.94\n",
      "| epoch  46 |   800/ 1106 batches | lr 20.00 | ms/batch 164.60 | loss  3.99 | ppl    53.83\n",
      "| epoch  46 |  1000/ 1106 batches | lr 20.00 | ms/batch 162.46 | loss  4.01 | ppl    55.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  46 | time: 192.19s | valid loss  4.28 | valid ppl    72.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  47 |   200/ 1106 batches | lr 20.00 | ms/batch 161.37 | loss  4.04 | ppl    56.90\n",
      "| epoch  47 |   400/ 1106 batches | lr 20.00 | ms/batch 163.26 | loss  3.94 | ppl    51.21\n",
      "| epoch  47 |   600/ 1106 batches | lr 20.00 | ms/batch 162.16 | loss  3.96 | ppl    52.38\n",
      "| epoch  47 |   800/ 1106 batches | lr 20.00 | ms/batch 162.94 | loss  3.98 | ppl    53.41\n",
      "| epoch  47 |  1000/ 1106 batches | lr 20.00 | ms/batch 160.93 | loss  4.02 | ppl    55.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  47 | time: 192.22s | valid loss  4.27 | valid ppl    71.46\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Normal!\n",
      "| epoch  48 |   200/ 1106 batches | lr 20.00 | ms/batch 161.61 | loss  4.04 | ppl    56.72\n",
      "| epoch  48 |   400/ 1106 batches | lr 20.00 | ms/batch 163.23 | loss  3.93 | ppl    50.85\n",
      "| epoch  48 |   600/ 1106 batches | lr 20.00 | ms/batch 163.03 | loss  3.94 | ppl    51.60\n",
      "| epoch  48 |   800/ 1106 batches | lr 20.00 | ms/batch 162.39 | loss  3.97 | ppl    52.83\n",
      "| epoch  48 |  1000/ 1106 batches | lr 20.00 | ms/batch 164.38 | loss  4.00 | ppl    54.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  48 | time: 192.20s | valid loss  4.26 | valid ppl    70.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  49 |   200/ 1106 batches | lr 20.00 | ms/batch 164.92 | loss  4.03 | ppl    56.07\n",
      "| epoch  49 |   400/ 1106 batches | lr 20.00 | ms/batch 164.27 | loss  3.93 | ppl    50.71\n",
      "| epoch  49 |   600/ 1106 batches | lr 20.00 | ms/batch 162.43 | loss  3.93 | ppl    50.92\n",
      "| epoch  49 |   800/ 1106 batches | lr 20.00 | ms/batch 163.71 | loss  3.97 | ppl    52.90\n",
      "| epoch  49 |  1000/ 1106 batches | lr 20.00 | ms/batch 162.35 | loss  4.01 | ppl    55.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  49 | time: 192.33s | valid loss  4.28 | valid ppl    72.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "Switching!\n",
      "| epoch  50 |   200/ 1106 batches | lr 20.00 | ms/batch 167.11 | loss  4.04 | ppl    56.60\n",
      "| epoch  50 |   400/ 1106 batches | lr 20.00 | ms/batch 163.77 | loss  3.91 | ppl    50.10\n",
      "| epoch  50 |   600/ 1106 batches | lr 20.00 | ms/batch 165.01 | loss  3.94 | ppl    51.24\n",
      "| epoch  50 |   800/ 1106 batches | lr 20.00 | ms/batch 165.25 | loss  3.95 | ppl    51.86\n",
      "| epoch  50 |  1000/ 1106 batches | lr 20.00 | ms/batch 165.83 | loss  4.02 | ppl    55.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  50 | time: 196.79s | valid loss  4.17 | valid ppl    64.49\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  51 |   200/ 1106 batches | lr 20.00 | ms/batch 167.96 | loss  4.02 | ppl    55.79\n",
      "| epoch  51 |   400/ 1106 batches | lr 20.00 | ms/batch 162.30 | loss  3.91 | ppl    49.89\n",
      "| epoch  51 |   600/ 1106 batches | lr 20.00 | ms/batch 167.30 | loss  3.94 | ppl    51.20\n",
      "| epoch  51 |   800/ 1106 batches | lr 20.00 | ms/batch 164.83 | loss  3.96 | ppl    52.37\n",
      "| epoch  51 |  1000/ 1106 batches | lr 20.00 | ms/batch 169.92 | loss  3.99 | ppl    54.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  51 | time: 196.44s | valid loss  4.16 | valid ppl    64.06\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  52 |   200/ 1106 batches | lr 20.00 | ms/batch 167.00 | loss  4.01 | ppl    55.09\n",
      "| epoch  52 |   400/ 1106 batches | lr 20.00 | ms/batch 168.24 | loss  3.91 | ppl    49.96\n",
      "| epoch  52 |   600/ 1106 batches | lr 20.00 | ms/batch 165.57 | loss  3.94 | ppl    51.24\n",
      "| epoch  52 |   800/ 1106 batches | lr 20.00 | ms/batch 168.56 | loss  3.96 | ppl    52.65\n",
      "| epoch  52 |  1000/ 1106 batches | lr 20.00 | ms/batch 165.51 | loss  3.99 | ppl    53.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  52 | time: 196.69s | valid loss  4.16 | valid ppl    63.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  53 |   200/ 1106 batches | lr 20.00 | ms/batch 165.33 | loss  4.01 | ppl    55.14\n",
      "| epoch  53 |   400/ 1106 batches | lr 20.00 | ms/batch 168.07 | loss  3.90 | ppl    49.47\n",
      "| epoch  53 |   600/ 1106 batches | lr 20.00 | ms/batch 162.49 | loss  3.92 | ppl    50.17\n",
      "| epoch  53 |   800/ 1106 batches | lr 20.00 | ms/batch 166.12 | loss  3.95 | ppl    51.79\n",
      "| epoch  53 |  1000/ 1106 batches | lr 20.00 | ms/batch 166.49 | loss  3.99 | ppl    53.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  53 | time: 196.40s | valid loss  4.15 | valid ppl    63.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  54 |   200/ 1106 batches | lr 20.00 | ms/batch 165.76 | loss  4.01 | ppl    55.28\n",
      "| epoch  54 |   400/ 1106 batches | lr 20.00 | ms/batch 166.61 | loss  3.89 | ppl    48.99\n",
      "| epoch  54 |   600/ 1106 batches | lr 20.00 | ms/batch 165.95 | loss  3.92 | ppl    50.63\n",
      "| epoch  54 |   800/ 1106 batches | lr 20.00 | ms/batch 165.54 | loss  3.94 | ppl    51.51\n",
      "| epoch  54 |  1000/ 1106 batches | lr 20.00 | ms/batch 164.92 | loss  3.99 | ppl    53.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  54 | time: 196.19s | valid loss  4.15 | valid ppl    63.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  55 |   200/ 1106 batches | lr 20.00 | ms/batch 166.95 | loss  3.99 | ppl    54.18\n",
      "| epoch  55 |   400/ 1106 batches | lr 20.00 | ms/batch 167.90 | loss  3.89 | ppl    48.95\n",
      "| epoch  55 |   600/ 1106 batches | lr 20.00 | ms/batch 162.81 | loss  3.92 | ppl    50.31\n",
      "| epoch  55 |   800/ 1106 batches | lr 20.00 | ms/batch 166.11 | loss  3.93 | ppl    50.81\n",
      "| epoch  55 |  1000/ 1106 batches | lr 20.00 | ms/batch 163.83 | loss  3.99 | ppl    53.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  55 | time: 196.61s | valid loss  4.15 | valid ppl    63.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  56 |   200/ 1106 batches | lr 20.00 | ms/batch 166.90 | loss  4.00 | ppl    54.50\n",
      "| epoch  56 |   400/ 1106 batches | lr 20.00 | ms/batch 165.56 | loss  3.89 | ppl    48.93\n",
      "| epoch  56 |   600/ 1106 batches | lr 20.00 | ms/batch 168.40 | loss  3.91 | ppl    49.66\n",
      "| epoch  56 |   800/ 1106 batches | lr 20.00 | ms/batch 165.75 | loss  3.94 | ppl    51.47\n",
      "| epoch  56 |  1000/ 1106 batches | lr 20.00 | ms/batch 165.34 | loss  3.97 | ppl    52.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  56 | time: 196.27s | valid loss  4.14 | valid ppl    63.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  57 |   200/ 1106 batches | lr 20.00 | ms/batch 167.13 | loss  4.01 | ppl    54.94\n",
      "| epoch  57 |   400/ 1106 batches | lr 20.00 | ms/batch 166.64 | loss  3.88 | ppl    48.60\n",
      "| epoch  57 |   600/ 1106 batches | lr 20.00 | ms/batch 166.08 | loss  3.90 | ppl    49.44\n",
      "| epoch  57 |   800/ 1106 batches | lr 20.00 | ms/batch 165.22 | loss  3.93 | ppl    50.73\n",
      "| epoch  57 |  1000/ 1106 batches | lr 20.00 | ms/batch 169.41 | loss  3.97 | ppl    52.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  57 | time: 196.38s | valid loss  4.14 | valid ppl    62.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  58 |   200/ 1106 batches | lr 20.00 | ms/batch 168.79 | loss  3.99 | ppl    54.03\n",
      "| epoch  58 |   400/ 1106 batches | lr 20.00 | ms/batch 165.02 | loss  3.88 | ppl    48.61\n",
      "| epoch  58 |   600/ 1106 batches | lr 20.00 | ms/batch 169.10 | loss  3.90 | ppl    49.43\n",
      "| epoch  58 |   800/ 1106 batches | lr 20.00 | ms/batch 166.15 | loss  3.92 | ppl    50.63\n",
      "| epoch  58 |  1000/ 1106 batches | lr 20.00 | ms/batch 166.01 | loss  3.95 | ppl    52.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  58 | time: 196.35s | valid loss  4.14 | valid ppl    62.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  59 |   200/ 1106 batches | lr 20.00 | ms/batch 167.24 | loss  3.99 | ppl    54.11\n",
      "| epoch  59 |   400/ 1106 batches | lr 20.00 | ms/batch 165.81 | loss  3.88 | ppl    48.22\n",
      "| epoch  59 |   600/ 1106 batches | lr 20.00 | ms/batch 165.57 | loss  3.89 | ppl    49.02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  59 |   800/ 1106 batches | lr 20.00 | ms/batch 168.52 | loss  3.92 | ppl    50.45\n",
      "| epoch  59 |  1000/ 1106 batches | lr 20.00 | ms/batch 165.30 | loss  3.96 | ppl    52.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  59 | time: 196.44s | valid loss  4.14 | valid ppl    62.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  60 |   200/ 1106 batches | lr 20.00 | ms/batch 166.16 | loss  3.99 | ppl    53.79\n",
      "| epoch  60 |   400/ 1106 batches | lr 20.00 | ms/batch 163.90 | loss  3.88 | ppl    48.23\n",
      "| epoch  60 |   600/ 1106 batches | lr 20.00 | ms/batch 166.64 | loss  3.89 | ppl    48.91\n",
      "| epoch  60 |   800/ 1106 batches | lr 20.00 | ms/batch 165.61 | loss  3.92 | ppl    50.48\n",
      "| epoch  60 |  1000/ 1106 batches | lr 20.00 | ms/batch 163.40 | loss  3.97 | ppl    52.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  60 | time: 196.43s | valid loss  4.14 | valid ppl    62.54\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  61 |   200/ 1106 batches | lr 20.00 | ms/batch 167.38 | loss  3.97 | ppl    53.05\n",
      "| epoch  61 |   400/ 1106 batches | lr 20.00 | ms/batch 166.78 | loss  3.86 | ppl    47.45\n",
      "| epoch  61 |   600/ 1106 batches | lr 20.00 | ms/batch 164.25 | loss  3.88 | ppl    48.28\n",
      "| epoch  61 |   800/ 1106 batches | lr 20.00 | ms/batch 167.45 | loss  3.91 | ppl    49.77\n",
      "| epoch  61 |  1000/ 1106 batches | lr 20.00 | ms/batch 168.67 | loss  3.94 | ppl    51.54\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  61 | time: 196.19s | valid loss  4.13 | valid ppl    62.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  62 |   200/ 1106 batches | lr 20.00 | ms/batch 166.04 | loss  3.98 | ppl    53.46\n",
      "| epoch  62 |   400/ 1106 batches | lr 20.00 | ms/batch 161.63 | loss  3.86 | ppl    47.69\n",
      "| epoch  62 |   600/ 1106 batches | lr 20.00 | ms/batch 167.65 | loss  3.89 | ppl    48.81\n",
      "| epoch  62 |   800/ 1106 batches | lr 20.00 | ms/batch 166.65 | loss  3.91 | ppl    49.82\n",
      "| epoch  62 |  1000/ 1106 batches | lr 20.00 | ms/batch 167.43 | loss  3.94 | ppl    51.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  62 | time: 196.33s | valid loss  4.13 | valid ppl    62.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  63 |   200/ 1106 batches | lr 20.00 | ms/batch 167.22 | loss  3.98 | ppl    53.55\n",
      "| epoch  63 |   400/ 1106 batches | lr 20.00 | ms/batch 166.12 | loss  3.87 | ppl    47.82\n",
      "| epoch  63 |   600/ 1106 batches | lr 20.00 | ms/batch 166.64 | loss  3.88 | ppl    48.41\n",
      "| epoch  63 |   800/ 1106 batches | lr 20.00 | ms/batch 167.81 | loss  3.90 | ppl    49.58\n",
      "| epoch  63 |  1000/ 1106 batches | lr 20.00 | ms/batch 166.40 | loss  3.94 | ppl    51.54\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  63 | time: 196.31s | valid loss  4.13 | valid ppl    62.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  64 |   200/ 1106 batches | lr 20.00 | ms/batch 167.90 | loss  3.97 | ppl    53.18\n",
      "| epoch  64 |   400/ 1106 batches | lr 20.00 | ms/batch 166.42 | loss  3.87 | ppl    47.71\n",
      "| epoch  64 |   600/ 1106 batches | lr 20.00 | ms/batch 164.95 | loss  3.87 | ppl    47.90\n",
      "| epoch  64 |   800/ 1106 batches | lr 20.00 | ms/batch 169.09 | loss  3.89 | ppl    49.07\n",
      "| epoch  64 |  1000/ 1106 batches | lr 20.00 | ms/batch 169.04 | loss  3.93 | ppl    51.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  64 | time: 196.50s | valid loss  4.13 | valid ppl    62.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  65 |   200/ 1106 batches | lr 20.00 | ms/batch 165.17 | loss  3.97 | ppl    52.74\n",
      "| epoch  65 |   400/ 1106 batches | lr 20.00 | ms/batch 168.70 | loss  3.86 | ppl    47.67\n",
      "| epoch  65 |   600/ 1106 batches | lr 20.00 | ms/batch 165.49 | loss  3.86 | ppl    47.61\n",
      "| epoch  65 |   800/ 1106 batches | lr 20.00 | ms/batch 166.35 | loss  3.90 | ppl    49.31\n",
      "| epoch  65 |  1000/ 1106 batches | lr 20.00 | ms/batch 167.85 | loss  3.93 | ppl    51.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  65 | time: 196.29s | valid loss  4.13 | valid ppl    62.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  66 |   200/ 1106 batches | lr 20.00 | ms/batch 167.69 | loss  3.96 | ppl    52.26\n",
      "| epoch  66 |   400/ 1106 batches | lr 20.00 | ms/batch 165.73 | loss  3.86 | ppl    47.51\n",
      "| epoch  66 |   600/ 1106 batches | lr 20.00 | ms/batch 168.34 | loss  3.86 | ppl    47.65\n",
      "| epoch  66 |   800/ 1106 batches | lr 20.00 | ms/batch 164.29 | loss  3.90 | ppl    49.26\n",
      "| epoch  66 |  1000/ 1106 batches | lr 20.00 | ms/batch 167.56 | loss  3.93 | ppl    50.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  66 | time: 196.18s | valid loss  4.13 | valid ppl    62.06\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  67 |   200/ 1106 batches | lr 20.00 | ms/batch 168.07 | loss  3.95 | ppl    51.91\n",
      "| epoch  67 |   400/ 1106 batches | lr 20.00 | ms/batch 163.53 | loss  3.85 | ppl    46.95\n",
      "| epoch  67 |   600/ 1106 batches | lr 20.00 | ms/batch 164.71 | loss  3.86 | ppl    47.64\n",
      "| epoch  67 |   800/ 1106 batches | lr 20.00 | ms/batch 167.04 | loss  3.87 | ppl    48.12\n",
      "| epoch  67 |  1000/ 1106 batches | lr 20.00 | ms/batch 166.74 | loss  3.93 | ppl    50.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  67 | time: 196.11s | valid loss  4.13 | valid ppl    61.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  68 |   200/ 1106 batches | lr 20.00 | ms/batch 168.03 | loss  3.95 | ppl    52.04\n",
      "| epoch  68 |   400/ 1106 batches | lr 20.00 | ms/batch 165.29 | loss  3.84 | ppl    46.57\n",
      "| epoch  68 |   600/ 1106 batches | lr 20.00 | ms/batch 170.34 | loss  3.86 | ppl    47.29\n",
      "| epoch  68 |   800/ 1106 batches | lr 20.00 | ms/batch 166.76 | loss  3.89 | ppl    48.79\n",
      "| epoch  68 |  1000/ 1106 batches | lr 20.00 | ms/batch 166.95 | loss  3.92 | ppl    50.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  68 | time: 196.27s | valid loss  4.13 | valid ppl    61.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  69 |   200/ 1106 batches | lr 20.00 | ms/batch 166.36 | loss  3.95 | ppl    52.10\n",
      "| epoch  69 |   400/ 1106 batches | lr 20.00 | ms/batch 164.32 | loss  3.84 | ppl    46.60\n",
      "| epoch  69 |   600/ 1106 batches | lr 20.00 | ms/batch 168.87 | loss  3.85 | ppl    47.19\n",
      "| epoch  69 |   800/ 1106 batches | lr 20.00 | ms/batch 164.54 | loss  3.89 | ppl    48.90\n",
      "| epoch  69 |  1000/ 1106 batches | lr 20.00 | ms/batch 169.03 | loss  3.92 | ppl    50.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  69 | time: 196.21s | valid loss  4.12 | valid ppl    61.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  70 |   200/ 1106 batches | lr 20.00 | ms/batch 167.36 | loss  3.94 | ppl    51.35\n",
      "| epoch  70 |   400/ 1106 batches | lr 20.00 | ms/batch 167.57 | loss  3.83 | ppl    46.08\n",
      "| epoch  70 |   600/ 1106 batches | lr 20.00 | ms/batch 164.79 | loss  3.86 | ppl    47.44\n",
      "| epoch  70 |   800/ 1106 batches | lr 20.00 | ms/batch 165.88 | loss  3.88 | ppl    48.57\n",
      "| epoch  70 |  1000/ 1106 batches | lr 20.00 | ms/batch 168.01 | loss  3.90 | ppl    49.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  70 | time: 196.42s | valid loss  4.12 | valid ppl    61.80\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Averaged!\n",
      "| epoch  71 |   200/ 1106 batches | lr 20.00 | ms/batch 167.02 | loss  3.93 | ppl    51.04\n",
      "| epoch  71 |   400/ 1106 batches | lr 20.00 | ms/batch 167.21 | loss  3.84 | ppl    46.40\n",
      "| epoch  71 |   600/ 1106 batches | lr 20.00 | ms/batch 165.83 | loss  3.85 | ppl    47.10\n",
      "| epoch  71 |   800/ 1106 batches | lr 20.00 | ms/batch 167.95 | loss  3.89 | ppl    48.72\n",
      "| epoch  71 |  1000/ 1106 batches | lr 20.00 | ms/batch 165.92 | loss  3.90 | ppl    49.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  71 | time: 196.23s | valid loss  4.12 | valid ppl    61.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  72 |   200/ 1106 batches | lr 20.00 | ms/batch 168.79 | loss  3.93 | ppl    51.09\n",
      "| epoch  72 |   400/ 1106 batches | lr 20.00 | ms/batch 165.84 | loss  3.82 | ppl    45.72\n",
      "| epoch  72 |   600/ 1106 batches | lr 20.00 | ms/batch 168.32 | loss  3.85 | ppl    47.21\n",
      "| epoch  72 |   800/ 1106 batches | lr 20.00 | ms/batch 167.72 | loss  3.88 | ppl    48.46\n",
      "| epoch  72 |  1000/ 1106 batches | lr 20.00 | ms/batch 168.20 | loss  3.91 | ppl    49.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  72 | time: 196.40s | valid loss  4.12 | valid ppl    61.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  73 |   200/ 1106 batches | lr 20.00 | ms/batch 169.96 | loss  3.94 | ppl    51.35\n",
      "| epoch  73 |   400/ 1106 batches | lr 20.00 | ms/batch 165.74 | loss  3.84 | ppl    46.34\n",
      "| epoch  73 |   600/ 1106 batches | lr 20.00 | ms/batch 165.43 | loss  3.84 | ppl    46.67\n",
      "| epoch  73 |   800/ 1106 batches | lr 20.00 | ms/batch 165.10 | loss  3.87 | ppl    47.93\n",
      "| epoch  73 |  1000/ 1106 batches | lr 20.00 | ms/batch 167.21 | loss  3.90 | ppl    49.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  73 | time: 196.53s | valid loss  4.12 | valid ppl    61.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  74 |   200/ 1106 batches | lr 20.00 | ms/batch 167.56 | loss  3.93 | ppl    50.72\n",
      "| epoch  74 |   400/ 1106 batches | lr 20.00 | ms/batch 164.06 | loss  3.82 | ppl    45.71\n",
      "| epoch  74 |   600/ 1106 batches | lr 20.00 | ms/batch 167.70 | loss  3.84 | ppl    46.31\n",
      "| epoch  74 |   800/ 1106 batches | lr 20.00 | ms/batch 164.49 | loss  3.86 | ppl    47.59\n",
      "| epoch  74 |  1000/ 1106 batches | lr 20.00 | ms/batch 167.54 | loss  3.89 | ppl    49.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  74 | time: 196.38s | valid loss  4.12 | valid ppl    61.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  75 |   200/ 1106 batches | lr 20.00 | ms/batch 166.46 | loss  3.93 | ppl    50.81\n",
      "| epoch  75 |   400/ 1106 batches | lr 20.00 | ms/batch 164.80 | loss  3.83 | ppl    45.91\n"
     ]
    }
   ],
   "source": [
    "# MoS\n",
    "args = new_params(\"--data data/penn --dropouti 0.4 --dropoutl 0.29 --dropouth 0.225 --seed 20 --batch_size 12\\\n",
    "    --max_seq_len_delta 15 --lr 20.0 --epoch 150 --nhid 960 --nhidlast 620 --mos --emsize 280 --n_experts 15 --save Experiments/MoS2\")\n",
    "train_data = batchify(corpus.train, args.batch_size, args)\n",
    "\n",
    "model, parallel_model = new_model(args, corpus)\n",
    "SGD(model, parallel_model, args, corpus)\n",
    "\n",
    "## MoC\n",
    "\n",
    "args = new_params(\"--data data/penn --dropouti 0.4 --dropoutl 0.29 --dropouth 0.225 --seed 20 --batch_size 12\\\n",
    "    --max_seq_len_delta 40 --lr 20.0 --epoch 150 --nhid 960 --nhidlast 620 --moc --emsize 280 --n_experts 15 --save Experiments/MoC2\")\n",
    "train_data = batchify(corpus.train, args.batch_size, args)\n",
    "\n",
    "model, parallel_model = new_model(args, corpus)\n",
    "SGD(model, parallel_model, args, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment dir : Experiments/PTB-MoC-Experts5-20180124-164354\n",
      "torch.Size([77465, 12])\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n",
      "Model param size: 21434410\n",
      "Args: Namespace(alpha=2, batch_size=12, beta=1, bptt=70, clip=0.25, continue_train=False, cuda=True, data='data/penn', dropout=0.4, dropoute=0.1, dropouth=0.225, dropouti=0.4, dropoutl=0.29, emsize=280, epochs=150, log_interval=200, lr=20.0, max_seq_len_delta=15, moc=False, model='LSTM', mos=True, n_experts=5, nhid=1010, nhidlast=650, nlayers=3, nonmono=5, save='Experiments/PTB-MoC-Experts5-20180124-164354', seed=20, single_gpu=False, small_batch_size=12, tied=True, wdecay=1.2e-06, wdrop=0.5)\n",
      "Model total parameters: 21434410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pio/os/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py:224: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greately increasing memory usage. To compact weights again call flatten_parameters().\n",
      "  result = self.forward(*input, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 1106 batches | lr 20.00 | ms/batch 100.77 | loss  7.42 | ppl  1666.25\n",
      "| epoch   1 |   400/ 1106 batches | lr 20.00 | ms/batch 100.18 | loss  6.81 | ppl   907.43\n",
      "| epoch   1 |   600/ 1106 batches | lr 20.00 | ms/batch 100.32 | loss  6.53 | ppl   684.23\n",
      "| epoch   1 |   800/ 1106 batches | lr 20.00 | ms/batch 100.08 | loss  6.51 | ppl   670.00\n",
      "| epoch   1 |  1000/ 1106 batches | lr 20.00 | ms/batch 102.05 | loss  6.41 | ppl   606.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 118.68s | valid loss  6.38 | valid ppl   589.99\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pio/os/anaconda3/lib/python3.6/site-packages/torch/serialization.py:147: UserWarning: Couldn't retrieve source code for container of type RNNModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Normal!\n",
      "| epoch   2 |   200/ 1106 batches | lr 20.00 | ms/batch 113.34 | loss  6.29 | ppl   541.21\n",
      "| epoch   2 |   400/ 1106 batches | lr 20.00 | ms/batch 101.80 | loss  6.11 | ppl   449.05\n",
      "| epoch   2 |   600/ 1106 batches | lr 20.00 | ms/batch 101.47 | loss  5.96 | ppl   387.49\n",
      "| epoch   2 |   800/ 1106 batches | lr 20.00 | ms/batch 99.62 | loss  5.90 | ppl   363.52\n",
      "| epoch   2 |  1000/ 1106 batches | lr 20.00 | ms/batch 99.64 | loss  5.82 | ppl   338.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 122.10s | valid loss  5.61 | valid ppl   272.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   3 |   200/ 1106 batches | lr 20.00 | ms/batch 100.41 | loss  5.72 | ppl   304.55\n",
      "| epoch   3 |   400/ 1106 batches | lr 20.00 | ms/batch 101.98 | loss  5.59 | ppl   268.05\n",
      "| epoch   3 |   600/ 1106 batches | lr 20.00 | ms/batch 102.37 | loss  5.50 | ppl   244.52\n",
      "| epoch   3 |   800/ 1106 batches | lr 20.00 | ms/batch 102.15 | loss  5.50 | ppl   244.16\n",
      "| epoch   3 |  1000/ 1106 batches | lr 20.00 | ms/batch 103.70 | loss  5.49 | ppl   243.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 122.37s | valid loss  5.30 | valid ppl   199.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   4 |   200/ 1106 batches | lr 20.00 | ms/batch 101.83 | loss  5.41 | ppl   223.99\n",
      "| epoch   4 |   400/ 1106 batches | lr 20.00 | ms/batch 100.96 | loss  5.31 | ppl   201.47\n",
      "| epoch   4 |   600/ 1106 batches | lr 20.00 | ms/batch 100.82 | loss  5.27 | ppl   194.56\n",
      "| epoch   4 |   800/ 1106 batches | lr 20.00 | ms/batch 101.53 | loss  5.28 | ppl   196.45\n",
      "| epoch   4 |  1000/ 1106 batches | lr 20.00 | ms/batch 102.83 | loss  5.27 | ppl   194.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 120.43s | valid loss  5.14 | valid ppl   169.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   5 |   200/ 1106 batches | lr 20.00 | ms/batch 102.70 | loss  5.23 | ppl   186.84\n",
      "| epoch   5 |   400/ 1106 batches | lr 20.00 | ms/batch 101.77 | loss  5.12 | ppl   167.91\n",
      "| epoch   5 |   600/ 1106 batches | lr 20.00 | ms/batch 100.60 | loss  5.09 | ppl   162.95\n",
      "| epoch   5 |   800/ 1106 batches | lr 20.00 | ms/batch 102.50 | loss  5.10 | ppl   164.84\n",
      "| epoch   5 |  1000/ 1106 batches | lr 20.00 | ms/batch 102.56 | loss  5.12 | ppl   167.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 121.61s | valid loss  5.04 | valid ppl   153.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   6 |   200/ 1106 batches | lr 20.00 | ms/batch 101.99 | loss  5.09 | ppl   162.82\n",
      "| epoch   6 |   400/ 1106 batches | lr 20.00 | ms/batch 99.75 | loss  4.98 | ppl   145.14\n",
      "| epoch   6 |   600/ 1106 batches | lr 20.00 | ms/batch 102.13 | loss  4.96 | ppl   142.43\n",
      "| epoch   6 |   800/ 1106 batches | lr 20.00 | ms/batch 103.65 | loss  4.97 | ppl   143.80\n",
      "| epoch   6 |  1000/ 1106 batches | lr 20.00 | ms/batch 103.28 | loss  4.99 | ppl   147.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 121.26s | valid loss  4.95 | valid ppl   140.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   7 |   200/ 1106 batches | lr 20.00 | ms/batch 102.31 | loss  4.98 | ppl   145.63\n",
      "| epoch   7 |   400/ 1106 batches | lr 20.00 | ms/batch 99.71 | loss  4.87 | ppl   129.80\n",
      "| epoch   7 |   600/ 1106 batches | lr 20.00 | ms/batch 102.48 | loss  4.86 | ppl   128.53\n",
      "| epoch   7 |   800/ 1106 batches | lr 20.00 | ms/batch 101.04 | loss  4.86 | ppl   128.82\n",
      "| epoch   7 |  1000/ 1106 batches | lr 20.00 | ms/batch 102.56 | loss  4.91 | ppl   135.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 123.62s | valid loss  4.85 | valid ppl   128.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   8 |   200/ 1106 batches | lr 20.00 | ms/batch 100.96 | loss  4.89 | ppl   133.25\n",
      "| epoch   8 |   400/ 1106 batches | lr 20.00 | ms/batch 101.42 | loss  4.78 | ppl   118.78\n",
      "| epoch   8 |   600/ 1106 batches | lr 20.00 | ms/batch 102.16 | loss  4.78 | ppl   118.69\n",
      "| epoch   8 |   800/ 1106 batches | lr 20.00 | ms/batch 101.03 | loss  4.80 | ppl   121.63\n",
      "| epoch   8 |  1000/ 1106 batches | lr 20.00 | ms/batch 100.82 | loss  4.82 | ppl   124.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 120.69s | valid loss  4.79 | valid ppl   120.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   9 |   200/ 1106 batches | lr 20.00 | ms/batch 102.18 | loss  4.82 | ppl   123.42\n",
      "| epoch   9 |   400/ 1106 batches | lr 20.00 | ms/batch 103.05 | loss  4.71 | ppl   110.78\n",
      "| epoch   9 |   600/ 1106 batches | lr 20.00 | ms/batch 100.95 | loss  4.70 | ppl   109.76\n",
      "| epoch   9 |   800/ 1106 batches | lr 20.00 | ms/batch 108.03 | loss  4.71 | ppl   111.26\n",
      "| epoch   9 |  1000/ 1106 batches | lr 20.00 | ms/batch 105.22 | loss  4.75 | ppl   115.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 123.38s | valid loss  4.74 | valid ppl   114.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  10 |   200/ 1106 batches | lr 20.00 | ms/batch 104.43 | loss  4.76 | ppl   116.66\n",
      "| epoch  10 |   400/ 1106 batches | lr 20.00 | ms/batch 101.75 | loss  4.63 | ppl   102.68\n",
      "| epoch  10 |   600/ 1106 batches | lr 20.00 | ms/batch 101.03 | loss  4.64 | ppl   103.05\n",
      "| epoch  10 |   800/ 1106 batches | lr 20.00 | ms/batch 99.91 | loss  4.67 | ppl   106.49\n",
      "| epoch  10 |  1000/ 1106 batches | lr 20.00 | ms/batch 102.25 | loss  4.69 | ppl   108.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 120.73s | valid loss  4.68 | valid ppl   108.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  11 |   200/ 1106 batches | lr 20.00 | ms/batch 102.96 | loss  4.71 | ppl   110.95\n",
      "| epoch  11 |   400/ 1106 batches | lr 20.00 | ms/batch 101.08 | loss  4.58 | ppl    97.87\n",
      "| epoch  11 |   600/ 1106 batches | lr 20.00 | ms/batch 103.02 | loss  4.58 | ppl    97.94\n",
      "| epoch  11 |   800/ 1106 batches | lr 20.00 | ms/batch 103.33 | loss  4.61 | ppl   100.94\n",
      "| epoch  11 |  1000/ 1106 batches | lr 20.00 | ms/batch 102.89 | loss  4.64 | ppl   103.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 121.53s | valid loss  4.68 | valid ppl   107.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  12 |   200/ 1106 batches | lr 20.00 | ms/batch 102.07 | loss  4.66 | ppl   105.41\n",
      "| epoch  12 |   400/ 1106 batches | lr 20.00 | ms/batch 101.43 | loss  4.53 | ppl    92.44\n",
      "| epoch  12 |   600/ 1106 batches | lr 20.00 | ms/batch 101.49 | loss  4.53 | ppl    93.05\n",
      "| epoch  12 |   800/ 1106 batches | lr 20.00 | ms/batch 102.89 | loss  4.57 | ppl    96.24\n",
      "| epoch  12 |  1000/ 1106 batches | lr 20.00 | ms/batch 102.43 | loss  4.61 | ppl   100.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 120.80s | valid loss  4.64 | valid ppl   103.06\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  13 |   200/ 1106 batches | lr 20.00 | ms/batch 102.67 | loss  4.60 | ppl    99.95\n",
      "| epoch  13 |   400/ 1106 batches | lr 20.00 | ms/batch 103.35 | loss  4.49 | ppl    89.41\n",
      "| epoch  13 |   600/ 1106 batches | lr 20.00 | ms/batch 103.88 | loss  4.51 | ppl    90.66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  13 |   800/ 1106 batches | lr 20.00 | ms/batch 102.82 | loss  4.54 | ppl    93.31\n",
      "| epoch  13 |  1000/ 1106 batches | lr 20.00 | ms/batch 102.32 | loss  4.55 | ppl    95.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 121.09s | valid loss  4.61 | valid ppl   100.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  14 |   200/ 1106 batches | lr 20.00 | ms/batch 101.36 | loss  4.57 | ppl    96.22\n",
      "| epoch  14 |   400/ 1106 batches | lr 20.00 | ms/batch 102.26 | loss  4.46 | ppl    86.62\n",
      "| epoch  14 |   600/ 1106 batches | lr 20.00 | ms/batch 100.59 | loss  4.46 | ppl    86.52\n",
      "| epoch  14 |   800/ 1106 batches | lr 20.00 | ms/batch 102.32 | loss  4.50 | ppl    89.60\n",
      "| epoch  14 |  1000/ 1106 batches | lr 20.00 | ms/batch 101.75 | loss  4.53 | ppl    93.06\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 120.87s | valid loss  4.56 | valid ppl    95.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  15 |   200/ 1106 batches | lr 20.00 | ms/batch 112.85 | loss  4.54 | ppl    93.37\n",
      "| epoch  15 |   400/ 1106 batches | lr 20.00 | ms/batch 101.87 | loss  4.41 | ppl    82.64\n",
      "| epoch  15 |   600/ 1106 batches | lr 20.00 | ms/batch 102.60 | loss  4.43 | ppl    84.07\n",
      "| epoch  15 |   800/ 1106 batches | lr 20.00 | ms/batch 101.95 | loss  4.45 | ppl    85.60\n",
      "| epoch  15 |  1000/ 1106 batches | lr 20.00 | ms/batch 101.81 | loss  4.49 | ppl    89.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 123.28s | valid loss  4.56 | valid ppl    95.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  16 |   200/ 1106 batches | lr 20.00 | ms/batch 101.63 | loss  4.51 | ppl    90.74\n",
      "| epoch  16 |   400/ 1106 batches | lr 20.00 | ms/batch 102.41 | loss  4.39 | ppl    80.77\n",
      "| epoch  16 |   600/ 1106 batches | lr 20.00 | ms/batch 102.64 | loss  4.40 | ppl    81.43\n",
      "| epoch  16 |   800/ 1106 batches | lr 20.00 | ms/batch 102.68 | loss  4.43 | ppl    84.03\n",
      "| epoch  16 |  1000/ 1106 batches | lr 20.00 | ms/batch 101.46 | loss  4.46 | ppl    86.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 120.87s | valid loss  4.56 | valid ppl    95.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  17 |   200/ 1106 batches | lr 20.00 | ms/batch 102.62 | loss  4.49 | ppl    89.23\n",
      "| epoch  17 |   400/ 1106 batches | lr 20.00 | ms/batch 102.52 | loss  4.37 | ppl    78.72\n",
      "| epoch  17 |   600/ 1106 batches | lr 20.00 | ms/batch 102.11 | loss  4.38 | ppl    79.45\n",
      "| epoch  17 |   800/ 1106 batches | lr 20.00 | ms/batch 100.71 | loss  4.40 | ppl    81.40\n",
      "| epoch  17 |  1000/ 1106 batches | lr 20.00 | ms/batch 102.12 | loss  4.44 | ppl    84.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 120.60s | valid loss  4.52 | valid ppl    91.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  18 |   200/ 1106 batches | lr 20.00 | ms/batch 101.40 | loss  4.45 | ppl    85.44\n",
      "| epoch  18 |   400/ 1106 batches | lr 20.00 | ms/batch 101.03 | loss  4.34 | ppl    76.52\n",
      "| epoch  18 |   600/ 1106 batches | lr 20.00 | ms/batch 102.73 | loss  4.35 | ppl    77.53\n",
      "| epoch  18 |   800/ 1106 batches | lr 20.00 | ms/batch 102.96 | loss  4.38 | ppl    79.55\n",
      "| epoch  18 |  1000/ 1106 batches | lr 20.00 | ms/batch 101.82 | loss  4.41 | ppl    82.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 121.02s | valid loss  4.50 | valid ppl    90.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  19 |   200/ 1106 batches | lr 20.00 | ms/batch 103.33 | loss  4.42 | ppl    83.31\n",
      "| epoch  19 |   400/ 1106 batches | lr 20.00 | ms/batch 102.95 | loss  4.31 | ppl    74.41\n",
      "| epoch  19 |   600/ 1106 batches | lr 20.00 | ms/batch 102.26 | loss  4.32 | ppl    74.95\n",
      "| epoch  19 |   800/ 1106 batches | lr 20.00 | ms/batch 101.50 | loss  4.36 | ppl    78.10\n",
      "| epoch  19 |  1000/ 1106 batches | lr 20.00 | ms/batch 102.66 | loss  4.38 | ppl    80.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 120.67s | valid loss  4.52 | valid ppl    92.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  20 |   200/ 1106 batches | lr 20.00 | ms/batch 102.49 | loss  4.40 | ppl    81.68\n",
      "| epoch  20 |   400/ 1106 batches | lr 20.00 | ms/batch 103.30 | loss  4.30 | ppl    73.39\n",
      "| epoch  20 |   600/ 1106 batches | lr 20.00 | ms/batch 104.17 | loss  4.30 | ppl    73.98\n",
      "| epoch  20 |   800/ 1106 batches | lr 20.00 | ms/batch 100.60 | loss  4.33 | ppl    75.61\n",
      "| epoch  20 |  1000/ 1106 batches | lr 20.00 | ms/batch 100.62 | loss  4.36 | ppl    78.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 121.03s | valid loss  4.53 | valid ppl    93.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  21 |   200/ 1106 batches | lr 20.00 | ms/batch 100.69 | loss  4.38 | ppl    80.00\n",
      "| epoch  21 |   400/ 1106 batches | lr 20.00 | ms/batch 101.72 | loss  4.27 | ppl    71.56\n",
      "| epoch  21 |   600/ 1106 batches | lr 20.00 | ms/batch 103.56 | loss  4.29 | ppl    72.93\n",
      "| epoch  21 |   800/ 1106 batches | lr 20.00 | ms/batch 102.14 | loss  4.31 | ppl    74.48\n",
      "| epoch  21 |  1000/ 1106 batches | lr 20.00 | ms/batch 101.97 | loss  4.34 | ppl    76.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time: 120.61s | valid loss  4.47 | valid ppl    87.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  22 |   200/ 1106 batches | lr 20.00 | ms/batch 112.07 | loss  4.36 | ppl    78.10\n",
      "| epoch  22 |   400/ 1106 batches | lr 20.00 | ms/batch 102.88 | loss  4.25 | ppl    69.89\n",
      "| epoch  22 |   600/ 1106 batches | lr 20.00 | ms/batch 101.67 | loss  4.27 | ppl    71.18\n",
      "| epoch  22 |   800/ 1106 batches | lr 20.00 | ms/batch 102.69 | loss  4.29 | ppl    72.67\n",
      "| epoch  22 |  1000/ 1106 batches | lr 20.00 | ms/batch 102.66 | loss  4.33 | ppl    76.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time: 123.37s | valid loss  4.45 | valid ppl    85.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  23 |   200/ 1106 batches | lr 20.00 | ms/batch 101.49 | loss  4.36 | ppl    78.06\n",
      "| epoch  23 |   400/ 1106 batches | lr 20.00 | ms/batch 102.19 | loss  4.23 | ppl    68.82\n",
      "| epoch  23 |   600/ 1106 batches | lr 20.00 | ms/batch 101.66 | loss  4.25 | ppl    69.84\n",
      "| epoch  23 |   800/ 1106 batches | lr 20.00 | ms/batch 100.94 | loss  4.26 | ppl    70.79\n",
      "| epoch  23 |  1000/ 1106 batches | lr 20.00 | ms/batch 101.46 | loss  4.30 | ppl    73.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time: 120.91s | valid loss  4.45 | valid ppl    86.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  24 |   200/ 1106 batches | lr 20.00 | ms/batch 101.27 | loss  4.33 | ppl    76.09\n",
      "| epoch  24 |   400/ 1106 batches | lr 20.00 | ms/batch 101.94 | loss  4.21 | ppl    67.44\n",
      "| epoch  24 |   600/ 1106 batches | lr 20.00 | ms/batch 102.15 | loss  4.22 | ppl    68.36\n",
      "| epoch  24 |   800/ 1106 batches | lr 20.00 | ms/batch 101.85 | loss  4.25 | ppl    70.00\n",
      "| epoch  24 |  1000/ 1106 batches | lr 20.00 | ms/batch 101.50 | loss  4.28 | ppl    72.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time: 120.87s | valid loss  4.42 | valid ppl    83.18\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Normal!\n",
      "| epoch  25 |   200/ 1106 batches | lr 20.00 | ms/batch 102.47 | loss  4.32 | ppl    74.82\n",
      "| epoch  25 |   400/ 1106 batches | lr 20.00 | ms/batch 102.57 | loss  4.20 | ppl    66.67\n",
      "| epoch  25 |   600/ 1106 batches | lr 20.00 | ms/batch 102.21 | loss  4.21 | ppl    67.49\n",
      "| epoch  25 |   800/ 1106 batches | lr 20.00 | ms/batch 103.36 | loss  4.25 | ppl    69.96\n",
      "| epoch  25 |  1000/ 1106 batches | lr 20.00 | ms/batch 102.42 | loss  4.26 | ppl    71.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time: 121.05s | valid loss  4.42 | valid ppl    82.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  26 |   200/ 1106 batches | lr 20.00 | ms/batch 102.61 | loss  4.30 | ppl    73.43\n",
      "| epoch  26 |   400/ 1106 batches | lr 20.00 | ms/batch 103.41 | loss  4.18 | ppl    65.16\n",
      "| epoch  26 |   600/ 1106 batches | lr 20.00 | ms/batch 102.53 | loss  4.20 | ppl    66.52\n",
      "| epoch  26 |   800/ 1106 batches | lr 20.00 | ms/batch 101.65 | loss  4.23 | ppl    68.63\n",
      "| epoch  26 |  1000/ 1106 batches | lr 20.00 | ms/batch 101.53 | loss  4.26 | ppl    71.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time: 121.42s | valid loss  4.42 | valid ppl    82.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  27 |   200/ 1106 batches | lr 20.00 | ms/batch 103.90 | loss  4.29 | ppl    72.74\n",
      "| epoch  27 |   400/ 1106 batches | lr 20.00 | ms/batch 103.01 | loss  4.17 | ppl    64.76\n",
      "| epoch  27 |   600/ 1106 batches | lr 20.00 | ms/batch 101.93 | loss  4.18 | ppl    65.06\n",
      "| epoch  27 |   800/ 1106 batches | lr 20.00 | ms/batch 103.84 | loss  4.21 | ppl    67.42\n",
      "| epoch  27 |  1000/ 1106 batches | lr 20.00 | ms/batch 103.33 | loss  4.24 | ppl    69.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time: 121.33s | valid loss  4.42 | valid ppl    82.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  28 |   200/ 1106 batches | lr 20.00 | ms/batch 101.17 | loss  4.28 | ppl    72.23\n",
      "| epoch  28 |   400/ 1106 batches | lr 20.00 | ms/batch 101.83 | loss  4.15 | ppl    63.36\n",
      "| epoch  28 |   600/ 1106 batches | lr 20.00 | ms/batch 101.36 | loss  4.18 | ppl    65.18\n",
      "| epoch  28 |   800/ 1106 batches | lr 20.00 | ms/batch 102.76 | loss  4.18 | ppl    65.60\n",
      "| epoch  28 |  1000/ 1106 batches | lr 20.00 | ms/batch 102.19 | loss  4.25 | ppl    70.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time: 122.15s | valid loss  4.40 | valid ppl    81.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  29 |   200/ 1106 batches | lr 20.00 | ms/batch 102.51 | loss  4.25 | ppl    70.41\n",
      "| epoch  29 |   400/ 1106 batches | lr 20.00 | ms/batch 101.59 | loss  4.14 | ppl    63.02\n",
      "| epoch  29 |   600/ 1106 batches | lr 20.00 | ms/batch 102.77 | loss  4.17 | ppl    64.60\n",
      "| epoch  29 |   800/ 1106 batches | lr 20.00 | ms/batch 102.00 | loss  4.19 | ppl    65.75\n",
      "| epoch  29 |  1000/ 1106 batches | lr 20.00 | ms/batch 100.83 | loss  4.23 | ppl    68.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time: 121.32s | valid loss  4.38 | valid ppl    80.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  30 |   200/ 1106 batches | lr 20.00 | ms/batch 102.09 | loss  4.24 | ppl    69.68\n",
      "| epoch  30 |   400/ 1106 batches | lr 20.00 | ms/batch 101.82 | loss  4.13 | ppl    62.08\n",
      "| epoch  30 |   600/ 1106 batches | lr 20.00 | ms/batch 100.80 | loss  4.14 | ppl    62.99\n",
      "| epoch  30 |   800/ 1106 batches | lr 20.00 | ms/batch 100.70 | loss  4.16 | ppl    64.29\n",
      "| epoch  30 |  1000/ 1106 batches | lr 20.00 | ms/batch 101.80 | loss  4.22 | ppl    67.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time: 120.87s | valid loss  4.40 | valid ppl    81.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  31 |   200/ 1106 batches | lr 20.00 | ms/batch 115.01 | loss  4.24 | ppl    69.07\n",
      "| epoch  31 |   400/ 1106 batches | lr 20.00 | ms/batch 101.13 | loss  4.11 | ppl    61.07\n",
      "| epoch  31 |   600/ 1106 batches | lr 20.00 | ms/batch 102.17 | loss  4.13 | ppl    62.41\n",
      "| epoch  31 |   800/ 1106 batches | lr 20.00 | ms/batch 101.95 | loss  4.15 | ppl    63.71\n",
      "| epoch  31 |  1000/ 1106 batches | lr 20.00 | ms/batch 102.19 | loss  4.20 | ppl    66.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time: 123.32s | valid loss  4.38 | valid ppl    79.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  32 |   200/ 1106 batches | lr 20.00 | ms/batch 102.75 | loss  4.22 | ppl    68.00\n",
      "| epoch  32 |   400/ 1106 batches | lr 20.00 | ms/batch 100.32 | loss  4.10 | ppl    60.20\n",
      "| epoch  32 |   600/ 1106 batches | lr 20.00 | ms/batch 102.98 | loss  4.13 | ppl    62.25\n",
      "| epoch  32 |   800/ 1106 batches | lr 20.00 | ms/batch 102.73 | loss  4.15 | ppl    63.47\n",
      "| epoch  32 |  1000/ 1106 batches | lr 20.00 | ms/batch 101.91 | loss  4.18 | ppl    65.59\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time: 121.10s | valid loss  4.37 | valid ppl    78.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  33 |   200/ 1106 batches | lr 20.00 | ms/batch 103.62 | loss  4.20 | ppl    67.00\n",
      "| epoch  33 |   400/ 1106 batches | lr 20.00 | ms/batch 102.05 | loss  4.08 | ppl    59.38\n",
      "| epoch  33 |   600/ 1106 batches | lr 20.00 | ms/batch 100.93 | loss  4.12 | ppl    61.25\n",
      "| epoch  33 |   800/ 1106 batches | lr 20.00 | ms/batch 101.99 | loss  4.14 | ppl    62.54\n",
      "| epoch  33 |  1000/ 1106 batches | lr 20.00 | ms/batch 100.74 | loss  4.18 | ppl    65.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time: 121.42s | valid loss  4.37 | valid ppl    79.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  34 |   200/ 1106 batches | lr 20.00 | ms/batch 102.74 | loss  4.20 | ppl    66.56\n",
      "| epoch  34 |   400/ 1106 batches | lr 20.00 | ms/batch 102.10 | loss  4.09 | ppl    59.61\n",
      "| epoch  34 |   600/ 1106 batches | lr 20.00 | ms/batch 101.76 | loss  4.10 | ppl    60.64\n",
      "| epoch  34 |   800/ 1106 batches | lr 20.00 | ms/batch 101.71 | loss  4.13 | ppl    62.01\n",
      "| epoch  34 |  1000/ 1106 batches | lr 20.00 | ms/batch 103.81 | loss  4.16 | ppl    64.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | time: 122.65s | valid loss  4.37 | valid ppl    78.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  35 |   200/ 1106 batches | lr 20.00 | ms/batch 104.88 | loss  4.19 | ppl    66.26\n",
      "| epoch  35 |   400/ 1106 batches | lr 20.00 | ms/batch 101.52 | loss  4.07 | ppl    58.46\n",
      "| epoch  35 |   600/ 1106 batches | lr 20.00 | ms/batch 101.83 | loss  4.09 | ppl    59.55\n",
      "| epoch  35 |   800/ 1106 batches | lr 20.00 | ms/batch 101.71 | loss  4.13 | ppl    62.04\n",
      "| epoch  35 |  1000/ 1106 batches | lr 20.00 | ms/batch 102.40 | loss  4.15 | ppl    63.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | time: 121.53s | valid loss  4.36 | valid ppl    78.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  36 |   200/ 1106 batches | lr 20.00 | ms/batch 100.26 | loss  4.18 | ppl    65.20\n",
      "| epoch  36 |   400/ 1106 batches | lr 20.00 | ms/batch 101.55 | loss  4.07 | ppl    58.39\n",
      "| epoch  36 |   600/ 1106 batches | lr 20.00 | ms/batch 102.98 | loss  4.09 | ppl    59.60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  36 |   800/ 1106 batches | lr 20.00 | ms/batch 101.76 | loss  4.11 | ppl    60.71\n",
      "| epoch  36 |  1000/ 1106 batches | lr 20.00 | ms/batch 105.33 | loss  4.14 | ppl    63.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  36 | time: 121.45s | valid loss  4.35 | valid ppl    77.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  37 |   200/ 1106 batches | lr 20.00 | ms/batch 102.03 | loss  4.16 | ppl    64.33\n",
      "| epoch  37 |   400/ 1106 batches | lr 20.00 | ms/batch 102.87 | loss  4.06 | ppl    58.16\n",
      "| epoch  37 |   600/ 1106 batches | lr 20.00 | ms/batch 102.06 | loss  4.07 | ppl    58.30\n",
      "| epoch  37 |   800/ 1106 batches | lr 20.00 | ms/batch 103.63 | loss  4.10 | ppl    60.59\n",
      "| epoch  37 |  1000/ 1106 batches | lr 20.00 | ms/batch 101.71 | loss  4.13 | ppl    62.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  37 | time: 120.69s | valid loss  4.36 | valid ppl    78.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  38 |   200/ 1106 batches | lr 20.00 | ms/batch 102.04 | loss  4.17 | ppl    64.97\n",
      "| epoch  38 |   400/ 1106 batches | lr 20.00 | ms/batch 101.28 | loss  4.06 | ppl    57.79\n",
      "| epoch  38 |   600/ 1106 batches | lr 20.00 | ms/batch 110.34 | loss  4.07 | ppl    58.65\n",
      "| epoch  38 |   800/ 1106 batches | lr 20.00 | ms/batch 106.90 | loss  4.08 | ppl    59.29\n",
      "| epoch  38 |  1000/ 1106 batches | lr 20.00 | ms/batch 102.04 | loss  4.14 | ppl    62.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  38 | time: 123.72s | valid loss  4.34 | valid ppl    76.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  39 |   200/ 1106 batches | lr 20.00 | ms/batch 101.64 | loss  4.16 | ppl    63.76\n",
      "| epoch  39 |   400/ 1106 batches | lr 20.00 | ms/batch 99.90 | loss  4.04 | ppl    56.55\n",
      "| epoch  39 |   600/ 1106 batches | lr 20.00 | ms/batch 103.83 | loss  4.05 | ppl    57.61\n",
      "| epoch  39 |   800/ 1106 batches | lr 20.00 | ms/batch 101.21 | loss  4.08 | ppl    59.25\n",
      "| epoch  39 |  1000/ 1106 batches | lr 20.00 | ms/batch 102.24 | loss  4.12 | ppl    61.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  39 | time: 120.67s | valid loss  4.34 | valid ppl    76.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  40 |   200/ 1106 batches | lr 20.00 | ms/batch 103.43 | loss  4.14 | ppl    62.76\n",
      "| epoch  40 |   400/ 1106 batches | lr 20.00 | ms/batch 103.43 | loss  4.04 | ppl    56.85\n",
      "| epoch  40 |   600/ 1106 batches | lr 20.00 | ms/batch 102.75 | loss  4.05 | ppl    57.21\n",
      "| epoch  40 |   800/ 1106 batches | lr 20.00 | ms/batch 102.77 | loss  4.09 | ppl    59.65\n",
      "| epoch  40 |  1000/ 1106 batches | lr 20.00 | ms/batch 103.07 | loss  4.10 | ppl    60.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  40 | time: 121.95s | valid loss  4.35 | valid ppl    77.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  41 |   200/ 1106 batches | lr 20.00 | ms/batch 104.11 | loss  4.13 | ppl    62.45\n",
      "| epoch  41 |   400/ 1106 batches | lr 20.00 | ms/batch 103.79 | loss  4.03 | ppl    56.30\n",
      "| epoch  41 |   600/ 1106 batches | lr 20.00 | ms/batch 101.70 | loss  4.04 | ppl    56.73\n",
      "| epoch  41 |   800/ 1106 batches | lr 20.00 | ms/batch 102.47 | loss  4.07 | ppl    58.83\n",
      "| epoch  41 |  1000/ 1106 batches | lr 20.00 | ms/batch 101.97 | loss  4.10 | ppl    60.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  41 | time: 121.37s | valid loss  4.34 | valid ppl    76.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  42 |   200/ 1106 batches | lr 20.00 | ms/batch 102.36 | loss  4.13 | ppl    62.01\n",
      "| epoch  42 |   400/ 1106 batches | lr 20.00 | ms/batch 101.28 | loss  3.99 | ppl    54.25\n",
      "| epoch  42 |   600/ 1106 batches | lr 20.00 | ms/batch 102.02 | loss  4.04 | ppl    56.58\n",
      "| epoch  42 |   800/ 1106 batches | lr 20.00 | ms/batch 103.49 | loss  4.05 | ppl    57.55\n",
      "| epoch  42 |  1000/ 1106 batches | lr 20.00 | ms/batch 102.02 | loss  4.09 | ppl    59.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  42 | time: 120.68s | valid loss  4.34 | valid ppl    77.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  43 |   200/ 1106 batches | lr 20.00 | ms/batch 113.86 | loss  4.12 | ppl    61.29\n",
      "| epoch  43 |   400/ 1106 batches | lr 20.00 | ms/batch 102.48 | loss  4.01 | ppl    55.05\n",
      "| epoch  43 |   600/ 1106 batches | lr 20.00 | ms/batch 100.61 | loss  4.03 | ppl    56.28\n",
      "| epoch  43 |   800/ 1106 batches | lr 20.00 | ms/batch 102.96 | loss  4.04 | ppl    57.03\n",
      "| epoch  43 |  1000/ 1106 batches | lr 20.00 | ms/batch 101.38 | loss  4.09 | ppl    59.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  43 | time: 123.48s | valid loss  4.35 | valid ppl    77.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  44 |   200/ 1106 batches | lr 20.00 | ms/batch 103.33 | loss  4.12 | ppl    61.30\n",
      "| epoch  44 |   400/ 1106 batches | lr 20.00 | ms/batch 101.20 | loss  3.99 | ppl    53.92\n",
      "| epoch  44 |   600/ 1106 batches | lr 20.00 | ms/batch 100.52 | loss  4.02 | ppl    55.75\n",
      "| epoch  44 |   800/ 1106 batches | lr 20.00 | ms/batch 101.48 | loss  4.05 | ppl    57.20\n",
      "| epoch  44 |  1000/ 1106 batches | lr 20.00 | ms/batch 104.18 | loss  4.08 | ppl    59.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  44 | time: 123.29s | valid loss  4.32 | valid ppl    75.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  45 |   200/ 1106 batches | lr 20.00 | ms/batch 101.90 | loss  4.11 | ppl    60.87\n",
      "| epoch  45 |   400/ 1106 batches | lr 20.00 | ms/batch 101.91 | loss  3.99 | ppl    54.00\n",
      "| epoch  45 |   600/ 1106 batches | lr 20.00 | ms/batch 102.87 | loss  4.02 | ppl    55.84\n",
      "| epoch  45 |   800/ 1106 batches | lr 20.00 | ms/batch 101.91 | loss  4.05 | ppl    57.51\n",
      "| epoch  45 |  1000/ 1106 batches | lr 20.00 | ms/batch 101.05 | loss  4.07 | ppl    58.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  45 | time: 120.60s | valid loss  4.34 | valid ppl    76.54\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  46 |   200/ 1106 batches | lr 20.00 | ms/batch 103.10 | loss  4.12 | ppl    61.63\n",
      "| epoch  46 |   400/ 1106 batches | lr 20.00 | ms/batch 102.55 | loss  3.99 | ppl    54.08\n",
      "| epoch  46 |   600/ 1106 batches | lr 20.00 | ms/batch 102.71 | loss  4.02 | ppl    55.44\n",
      "| epoch  46 |   800/ 1106 batches | lr 20.00 | ms/batch 103.98 | loss  4.04 | ppl    56.84\n",
      "| epoch  46 |  1000/ 1106 batches | lr 20.00 | ms/batch 103.02 | loss  4.06 | ppl    57.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  46 | time: 121.54s | valid loss  4.34 | valid ppl    76.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  47 |   200/ 1106 batches | lr 20.00 | ms/batch 102.93 | loss  4.09 | ppl    59.94\n",
      "| epoch  47 |   400/ 1106 batches | lr 20.00 | ms/batch 103.07 | loss  3.98 | ppl    53.56\n",
      "| epoch  47 |   600/ 1106 batches | lr 20.00 | ms/batch 102.61 | loss  4.01 | ppl    54.91\n",
      "| epoch  47 |   800/ 1106 batches | lr 20.00 | ms/batch 102.79 | loss  4.02 | ppl    55.77\n",
      "| epoch  47 |  1000/ 1106 batches | lr 20.00 | ms/batch 102.03 | loss  4.07 | ppl    58.30\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  47 | time: 122.15s | valid loss  4.34 | valid ppl    76.42\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  48 |   200/ 1106 batches | lr 20.00 | ms/batch 101.98 | loss  4.09 | ppl    59.68\n",
      "| epoch  48 |   400/ 1106 batches | lr 20.00 | ms/batch 102.01 | loss  3.97 | ppl    52.75\n",
      "| epoch  48 |   600/ 1106 batches | lr 20.00 | ms/batch 101.96 | loss  3.99 | ppl    54.05\n",
      "| epoch  48 |   800/ 1106 batches | lr 20.00 | ms/batch 101.95 | loss  4.02 | ppl    55.47\n",
      "| epoch  48 |  1000/ 1106 batches | lr 20.00 | ms/batch 103.08 | loss  4.04 | ppl    56.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  48 | time: 121.14s | valid loss  4.32 | valid ppl    74.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  49 |   200/ 1106 batches | lr 20.00 | ms/batch 102.73 | loss  4.08 | ppl    58.88\n",
      "| epoch  49 |   400/ 1106 batches | lr 20.00 | ms/batch 102.71 | loss  3.98 | ppl    53.26\n",
      "| epoch  49 |   600/ 1106 batches | lr 20.00 | ms/batch 103.45 | loss  3.99 | ppl    54.02\n",
      "| epoch  49 |   800/ 1106 batches | lr 20.00 | ms/batch 112.48 | loss  4.01 | ppl    55.01\n",
      "| epoch  49 |  1000/ 1106 batches | lr 20.00 | ms/batch 101.73 | loss  4.04 | ppl    57.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  49 | time: 123.19s | valid loss  4.36 | valid ppl    77.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "Switching!\n",
      "| epoch  50 |   200/ 1106 batches | lr 20.00 | ms/batch 105.65 | loss  4.09 | ppl    59.79\n",
      "| epoch  50 |   400/ 1106 batches | lr 20.00 | ms/batch 104.12 | loss  3.97 | ppl    52.96\n",
      "| epoch  50 |   600/ 1106 batches | lr 20.00 | ms/batch 104.49 | loss  3.99 | ppl    54.05\n",
      "| epoch  50 |   800/ 1106 batches | lr 20.00 | ms/batch 104.33 | loss  4.00 | ppl    54.35\n",
      "| epoch  50 |  1000/ 1106 batches | lr 20.00 | ms/batch 104.83 | loss  4.07 | ppl    58.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  50 | time: 125.15s | valid loss  4.18 | valid ppl    65.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  51 |   200/ 1106 batches | lr 20.00 | ms/batch 106.46 | loss  4.07 | ppl    58.63\n",
      "| epoch  51 |   400/ 1106 batches | lr 20.00 | ms/batch 103.97 | loss  3.96 | ppl    52.31\n",
      "| epoch  51 |   600/ 1106 batches | lr 20.00 | ms/batch 106.45 | loss  3.98 | ppl    53.41\n",
      "| epoch  51 |   800/ 1106 batches | lr 20.00 | ms/batch 104.57 | loss  4.00 | ppl    54.73\n",
      "| epoch  51 |  1000/ 1106 batches | lr 20.00 | ms/batch 107.33 | loss  4.03 | ppl    56.54\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  51 | time: 125.19s | valid loss  4.18 | valid ppl    65.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  52 |   200/ 1106 batches | lr 20.00 | ms/batch 105.90 | loss  4.08 | ppl    59.16\n",
      "| epoch  52 |   400/ 1106 batches | lr 20.00 | ms/batch 105.88 | loss  3.96 | ppl    52.47\n",
      "| epoch  52 |   600/ 1106 batches | lr 20.00 | ms/batch 104.59 | loss  3.98 | ppl    53.27\n",
      "| epoch  52 |   800/ 1106 batches | lr 20.00 | ms/batch 106.56 | loss  4.01 | ppl    55.04\n",
      "| epoch  52 |  1000/ 1106 batches | lr 20.00 | ms/batch 104.84 | loss  4.03 | ppl    56.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  52 | time: 125.07s | valid loss  4.17 | valid ppl    64.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  53 |   200/ 1106 batches | lr 20.00 | ms/batch 105.32 | loss  4.07 | ppl    58.39\n",
      "| epoch  53 |   400/ 1106 batches | lr 20.00 | ms/batch 106.35 | loss  3.96 | ppl    52.35\n",
      "| epoch  53 |   600/ 1106 batches | lr 20.00 | ms/batch 103.04 | loss  3.97 | ppl    52.88\n",
      "| epoch  53 |   800/ 1106 batches | lr 20.00 | ms/batch 104.85 | loss  3.99 | ppl    53.95\n",
      "| epoch  53 |  1000/ 1106 batches | lr 20.00 | ms/batch 105.59 | loss  4.04 | ppl    56.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  53 | time: 124.93s | valid loss  4.17 | valid ppl    64.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  54 |   200/ 1106 batches | lr 20.00 | ms/batch 105.50 | loss  4.07 | ppl    58.36\n",
      "| epoch  54 |   400/ 1106 batches | lr 20.00 | ms/batch 105.81 | loss  3.94 | ppl    51.57\n",
      "| epoch  54 |   600/ 1106 batches | lr 20.00 | ms/batch 105.91 | loss  3.96 | ppl    52.61\n",
      "| epoch  54 |   800/ 1106 batches | lr 20.00 | ms/batch 107.02 | loss  3.99 | ppl    54.11\n",
      "| epoch  54 |  1000/ 1106 batches | lr 20.00 | ms/batch 105.73 | loss  4.04 | ppl    56.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  54 | time: 126.06s | valid loss  4.16 | valid ppl    64.30\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  55 |   200/ 1106 batches | lr 20.00 | ms/batch 107.52 | loss  4.06 | ppl    57.81\n",
      "| epoch  55 |   400/ 1106 batches | lr 20.00 | ms/batch 107.02 | loss  3.94 | ppl    51.44\n",
      "| epoch  55 |   600/ 1106 batches | lr 20.00 | ms/batch 104.57 | loss  3.96 | ppl    52.38\n",
      "| epoch  55 |   800/ 1106 batches | lr 20.00 | ms/batch 106.77 | loss  3.98 | ppl    53.59\n",
      "| epoch  55 |  1000/ 1106 batches | lr 20.00 | ms/batch 105.40 | loss  4.02 | ppl    55.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  55 | time: 126.71s | valid loss  4.16 | valid ppl    64.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  56 |   200/ 1106 batches | lr 20.00 | ms/batch 105.76 | loss  4.05 | ppl    57.35\n",
      "| epoch  56 |   400/ 1106 batches | lr 20.00 | ms/batch 105.00 | loss  3.93 | ppl    51.00\n",
      "| epoch  56 |   600/ 1106 batches | lr 20.00 | ms/batch 106.76 | loss  3.95 | ppl    51.93\n",
      "| epoch  56 |   800/ 1106 batches | lr 20.00 | ms/batch 104.65 | loss  3.98 | ppl    53.41\n",
      "| epoch  56 |  1000/ 1106 batches | lr 20.00 | ms/batch 104.71 | loss  4.02 | ppl    55.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  56 | time: 124.83s | valid loss  4.16 | valid ppl    64.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  57 |   200/ 1106 batches | lr 20.00 | ms/batch 106.03 | loss  4.04 | ppl    56.92\n",
      "| epoch  57 |   400/ 1106 batches | lr 20.00 | ms/batch 105.35 | loss  3.94 | ppl    51.39\n",
      "| epoch  57 |   600/ 1106 batches | lr 20.00 | ms/batch 105.41 | loss  3.95 | ppl    52.04\n",
      "| epoch  57 |   800/ 1106 batches | lr 20.00 | ms/batch 116.87 | loss  3.98 | ppl    53.47\n",
      "| epoch  57 |  1000/ 1106 batches | lr 20.00 | ms/batch 107.22 | loss  4.02 | ppl    55.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  57 | time: 127.38s | valid loss  4.16 | valid ppl    63.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  58 |   200/ 1106 batches | lr 20.00 | ms/batch 106.87 | loss  4.04 | ppl    56.97\n",
      "| epoch  58 |   400/ 1106 batches | lr 20.00 | ms/batch 104.68 | loss  3.93 | ppl    50.82\n",
      "| epoch  58 |   600/ 1106 batches | lr 20.00 | ms/batch 106.91 | loss  3.96 | ppl    52.28\n",
      "| epoch  58 |   800/ 1106 batches | lr 20.00 | ms/batch 105.32 | loss  3.97 | ppl    53.12\n",
      "| epoch  58 |  1000/ 1106 batches | lr 20.00 | ms/batch 105.46 | loss  4.00 | ppl    54.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  58 | time: 124.91s | valid loss  4.16 | valid ppl    63.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  59 |   200/ 1106 batches | lr 20.00 | ms/batch 105.97 | loss  4.04 | ppl    56.79\n",
      "| epoch  59 |   400/ 1106 batches | lr 20.00 | ms/batch 106.05 | loss  3.92 | ppl    50.42\n",
      "| epoch  59 |   600/ 1106 batches | lr 20.00 | ms/batch 105.34 | loss  3.94 | ppl    51.46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  59 |   800/ 1106 batches | lr 20.00 | ms/batch 106.91 | loss  3.96 | ppl    52.24\n",
      "| epoch  59 |  1000/ 1106 batches | lr 20.00 | ms/batch 105.39 | loss  3.99 | ppl    54.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  59 | time: 125.44s | valid loss  4.15 | valid ppl    63.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  60 |   200/ 1106 batches | lr 20.00 | ms/batch 105.30 | loss  4.03 | ppl    56.33\n",
      "| epoch  60 |   400/ 1106 batches | lr 20.00 | ms/batch 104.68 | loss  3.93 | ppl    50.76\n",
      "| epoch  60 |   600/ 1106 batches | lr 20.00 | ms/batch 105.56 | loss  3.95 | ppl    51.89\n",
      "| epoch  60 |   800/ 1106 batches | lr 20.00 | ms/batch 104.76 | loss  3.96 | ppl    52.30\n",
      "| epoch  60 |  1000/ 1106 batches | lr 20.00 | ms/batch 103.58 | loss  4.01 | ppl    54.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  60 | time: 125.49s | valid loss  4.15 | valid ppl    63.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  61 |   200/ 1106 batches | lr 20.00 | ms/batch 106.37 | loss  4.03 | ppl    56.51\n",
      "| epoch  61 |   400/ 1106 batches | lr 20.00 | ms/batch 105.90 | loss  3.91 | ppl    49.68\n",
      "| epoch  61 |   600/ 1106 batches | lr 20.00 | ms/batch 104.53 | loss  3.94 | ppl    51.26\n",
      "| epoch  61 |   800/ 1106 batches | lr 20.00 | ms/batch 106.52 | loss  3.95 | ppl    52.00\n",
      "| epoch  61 |  1000/ 1106 batches | lr 20.00 | ms/batch 107.28 | loss  3.99 | ppl    54.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  61 | time: 125.28s | valid loss  4.15 | valid ppl    63.54\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  62 |   200/ 1106 batches | lr 20.00 | ms/batch 106.20 | loss  4.03 | ppl    56.54\n",
      "| epoch  62 |   400/ 1106 batches | lr 20.00 | ms/batch 103.80 | loss  3.92 | ppl    50.28\n",
      "| epoch  62 |   600/ 1106 batches | lr 20.00 | ms/batch 106.65 | loss  3.92 | ppl    50.64\n",
      "| epoch  62 |   800/ 1106 batches | lr 20.00 | ms/batch 106.61 | loss  3.96 | ppl    52.45\n",
      "| epoch  62 |  1000/ 1106 batches | lr 20.00 | ms/batch 106.55 | loss  3.98 | ppl    53.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  62 | time: 126.17s | valid loss  4.15 | valid ppl    63.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  63 |   200/ 1106 batches | lr 20.00 | ms/batch 106.06 | loss  4.01 | ppl    55.36\n",
      "| epoch  63 |   400/ 1106 batches | lr 20.00 | ms/batch 105.12 | loss  3.92 | ppl    50.50\n",
      "| epoch  63 |   600/ 1106 batches | lr 20.00 | ms/batch 106.08 | loss  3.93 | ppl    50.89\n",
      "| epoch  63 |   800/ 1106 batches | lr 20.00 | ms/batch 106.36 | loss  3.95 | ppl    51.97\n",
      "| epoch  63 |  1000/ 1106 batches | lr 20.00 | ms/batch 105.55 | loss  3.99 | ppl    54.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  63 | time: 125.07s | valid loss  4.15 | valid ppl    63.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  64 |   200/ 1106 batches | lr 20.00 | ms/batch 106.60 | loss  4.01 | ppl    55.24\n",
      "| epoch  64 |   400/ 1106 batches | lr 20.00 | ms/batch 106.49 | loss  3.91 | ppl    49.84\n",
      "| epoch  64 |   600/ 1106 batches | lr 20.00 | ms/batch 105.33 | loss  3.93 | ppl    50.92\n",
      "| epoch  64 |   800/ 1106 batches | lr 20.00 | ms/batch 107.24 | loss  3.95 | ppl    51.93\n",
      "| epoch  64 |  1000/ 1106 batches | lr 20.00 | ms/batch 106.85 | loss  3.98 | ppl    53.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  64 | time: 125.42s | valid loss  4.15 | valid ppl    63.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  65 |   200/ 1106 batches | lr 20.00 | ms/batch 105.50 | loss  4.01 | ppl    55.07\n",
      "| epoch  65 |   400/ 1106 batches | lr 20.00 | ms/batch 106.89 | loss  3.90 | ppl    49.64\n",
      "| epoch  65 |   600/ 1106 batches | lr 20.00 | ms/batch 105.01 | loss  3.92 | ppl    50.24\n",
      "| epoch  65 |   800/ 1106 batches | lr 20.00 | ms/batch 105.47 | loss  3.94 | ppl    51.26\n",
      "| epoch  65 |  1000/ 1106 batches | lr 20.00 | ms/batch 107.68 | loss  3.97 | ppl    53.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  65 | time: 125.40s | valid loss  4.15 | valid ppl    63.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  66 |   200/ 1106 batches | lr 20.00 | ms/batch 106.24 | loss  4.01 | ppl    55.34\n",
      "| epoch  66 |   400/ 1106 batches | lr 20.00 | ms/batch 105.48 | loss  3.90 | ppl    49.25\n",
      "| epoch  66 |   600/ 1106 batches | lr 20.00 | ms/batch 106.90 | loss  3.92 | ppl    50.48\n",
      "| epoch  66 |   800/ 1106 batches | lr 20.00 | ms/batch 104.48 | loss  3.94 | ppl    51.40\n",
      "| epoch  66 |  1000/ 1106 batches | lr 20.00 | ms/batch 107.06 | loss  3.97 | ppl    53.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  66 | time: 125.21s | valid loss  4.15 | valid ppl    63.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  67 |   200/ 1106 batches | lr 20.00 | ms/batch 106.63 | loss  4.00 | ppl    54.55\n",
      "| epoch  67 |   400/ 1106 batches | lr 20.00 | ms/batch 104.38 | loss  3.89 | ppl    48.98\n",
      "| epoch  67 |   600/ 1106 batches | lr 20.00 | ms/batch 105.71 | loss  3.91 | ppl    49.97\n",
      "| epoch  67 |   800/ 1106 batches | lr 20.00 | ms/batch 106.41 | loss  3.92 | ppl    50.58\n",
      "| epoch  67 |  1000/ 1106 batches | lr 20.00 | ms/batch 106.60 | loss  3.97 | ppl    53.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  67 | time: 125.72s | valid loss  4.14 | valid ppl    63.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  68 |   200/ 1106 batches | lr 20.00 | ms/batch 106.29 | loss  3.99 | ppl    54.18\n",
      "| epoch  68 |   400/ 1106 batches | lr 20.00 | ms/batch 105.20 | loss  3.90 | ppl    49.33\n",
      "| epoch  68 |   600/ 1106 batches | lr 20.00 | ms/batch 107.65 | loss  3.91 | ppl    49.73\n",
      "| epoch  68 |   800/ 1106 batches | lr 20.00 | ms/batch 106.55 | loss  3.93 | ppl    51.13\n",
      "| epoch  68 |  1000/ 1106 batches | lr 20.00 | ms/batch 106.32 | loss  3.96 | ppl    52.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  68 | time: 125.25s | valid loss  4.14 | valid ppl    63.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  69 |   200/ 1106 batches | lr 20.00 | ms/batch 106.50 | loss  4.01 | ppl    55.01\n",
      "| epoch  69 |   400/ 1106 batches | lr 20.00 | ms/batch 105.26 | loss  3.88 | ppl    48.48\n",
      "| epoch  69 |   600/ 1106 batches | lr 20.00 | ms/batch 107.27 | loss  3.90 | ppl    49.58\n",
      "| epoch  69 |   800/ 1106 batches | lr 20.00 | ms/batch 105.25 | loss  3.93 | ppl    50.82\n",
      "| epoch  69 |  1000/ 1106 batches | lr 20.00 | ms/batch 107.64 | loss  3.96 | ppl    52.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  69 | time: 125.69s | valid loss  4.14 | valid ppl    63.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  70 |   200/ 1106 batches | lr 20.00 | ms/batch 106.40 | loss  3.99 | ppl    53.99\n",
      "| epoch  70 |   400/ 1106 batches | lr 20.00 | ms/batch 106.41 | loss  3.88 | ppl    48.59\n",
      "| epoch  70 |   600/ 1106 batches | lr 20.00 | ms/batch 104.68 | loss  3.90 | ppl    49.20\n",
      "| epoch  70 |   800/ 1106 batches | lr 20.00 | ms/batch 105.80 | loss  3.93 | ppl    50.67\n",
      "| epoch  70 |  1000/ 1106 batches | lr 20.00 | ms/batch 118.36 | loss  3.96 | ppl    52.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  70 | time: 127.78s | valid loss  4.14 | valid ppl    62.94\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Averaged!\n",
      "| epoch  71 |   200/ 1106 batches | lr 20.00 | ms/batch 106.14 | loss  3.98 | ppl    53.61\n",
      "| epoch  71 |   400/ 1106 batches | lr 20.00 | ms/batch 106.19 | loss  3.88 | ppl    48.40\n",
      "| epoch  71 |   600/ 1106 batches | lr 20.00 | ms/batch 105.50 | loss  3.89 | ppl    49.00\n",
      "| epoch  71 |   800/ 1106 batches | lr 20.00 | ms/batch 106.81 | loss  3.93 | ppl    50.77\n",
      "| epoch  71 |  1000/ 1106 batches | lr 20.00 | ms/batch 106.52 | loss  3.96 | ppl    52.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  71 | time: 125.54s | valid loss  4.14 | valid ppl    62.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  72 |   200/ 1106 batches | lr 20.00 | ms/batch 107.55 | loss  3.99 | ppl    53.81\n",
      "| epoch  72 |   400/ 1106 batches | lr 20.00 | ms/batch 106.23 | loss  3.88 | ppl    48.38\n",
      "| epoch  72 |   600/ 1106 batches | lr 20.00 | ms/batch 107.37 | loss  3.89 | ppl    48.92\n",
      "| epoch  72 |   800/ 1106 batches | lr 20.00 | ms/batch 108.15 | loss  3.92 | ppl    50.30\n",
      "| epoch  72 |  1000/ 1106 batches | lr 20.00 | ms/batch 106.80 | loss  3.95 | ppl    51.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  72 | time: 125.99s | valid loss  4.14 | valid ppl    62.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  73 |   200/ 1106 batches | lr 20.00 | ms/batch 107.93 | loss  4.00 | ppl    54.34\n",
      "| epoch  73 |   400/ 1106 batches | lr 20.00 | ms/batch 104.89 | loss  3.88 | ppl    48.28\n",
      "| epoch  73 |   600/ 1106 batches | lr 20.00 | ms/batch 105.14 | loss  3.87 | ppl    48.08\n",
      "| epoch  73 |   800/ 1106 batches | lr 20.00 | ms/batch 104.83 | loss  3.91 | ppl    50.06\n",
      "| epoch  73 |  1000/ 1106 batches | lr 20.00 | ms/batch 105.91 | loss  3.94 | ppl    51.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  73 | time: 125.16s | valid loss  4.14 | valid ppl    62.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  74 |   200/ 1106 batches | lr 20.00 | ms/batch 106.23 | loss  3.97 | ppl    52.90\n",
      "| epoch  74 |   400/ 1106 batches | lr 20.00 | ms/batch 116.21 | loss  3.88 | ppl    48.36\n",
      "| epoch  74 |   600/ 1106 batches | lr 20.00 | ms/batch 106.95 | loss  3.88 | ppl    48.43\n",
      "| epoch  74 |   800/ 1106 batches | lr 20.00 | ms/batch 104.85 | loss  3.91 | ppl    49.66\n",
      "| epoch  74 |  1000/ 1106 batches | lr 20.00 | ms/batch 106.53 | loss  3.94 | ppl    51.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  74 | time: 127.81s | valid loss  4.14 | valid ppl    62.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  75 |   200/ 1106 batches | lr 20.00 | ms/batch 105.73 | loss  3.98 | ppl    53.75\n",
      "| epoch  75 |   400/ 1106 batches | lr 20.00 | ms/batch 104.58 | loss  3.87 | ppl    48.12\n",
      "| epoch  75 |   600/ 1106 batches | lr 20.00 | ms/batch 105.01 | loss  3.87 | ppl    47.96\n",
      "| epoch  75 |   800/ 1106 batches | lr 20.00 | ms/batch 106.08 | loss  3.91 | ppl    49.81\n",
      "| epoch  75 |  1000/ 1106 batches | lr 20.00 | ms/batch 104.31 | loss  3.94 | ppl    51.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  75 | time: 127.34s | valid loss  4.14 | valid ppl    62.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  76 |   200/ 1106 batches | lr 20.00 | ms/batch 107.15 | loss  3.97 | ppl    52.91\n",
      "| epoch  76 |   400/ 1106 batches | lr 20.00 | ms/batch 105.24 | loss  3.87 | ppl    47.86\n",
      "| epoch  76 |   600/ 1106 batches | lr 20.00 | ms/batch 108.01 | loss  3.87 | ppl    48.11\n",
      "| epoch  76 |   800/ 1106 batches | lr 20.00 | ms/batch 106.36 | loss  3.90 | ppl    49.59\n",
      "| epoch  76 |  1000/ 1106 batches | lr 20.00 | ms/batch 105.72 | loss  3.92 | ppl    50.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  76 | time: 125.59s | valid loss  4.14 | valid ppl    62.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  77 |   200/ 1106 batches | lr 20.00 | ms/batch 106.42 | loss  3.97 | ppl    53.17\n",
      "| epoch  77 |   400/ 1106 batches | lr 20.00 | ms/batch 113.27 | loss  3.85 | ppl    47.18\n",
      "| epoch  77 |   600/ 1106 batches | lr 20.00 | ms/batch 105.19 | loss  3.87 | ppl    47.99\n",
      "| epoch  77 |   800/ 1106 batches | lr 20.00 | ms/batch 111.55 | loss  3.90 | ppl    49.50\n",
      "| epoch  77 |  1000/ 1106 batches | lr 20.00 | ms/batch 111.99 | loss  3.93 | ppl    50.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  77 | time: 128.97s | valid loss  4.14 | valid ppl    62.59\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  78 |   200/ 1106 batches | lr 20.00 | ms/batch 107.16 | loss  3.97 | ppl    52.96\n",
      "| epoch  78 |   400/ 1106 batches | lr 20.00 | ms/batch 107.55 | loss  3.86 | ppl    47.70\n",
      "| epoch  78 |   600/ 1106 batches | lr 20.00 | ms/batch 105.76 | loss  3.87 | ppl    48.01\n",
      "| epoch  78 |   800/ 1106 batches | lr 20.00 | ms/batch 106.61 | loss  3.90 | ppl    49.30\n",
      "| epoch  78 |  1000/ 1106 batches | lr 20.00 | ms/batch 107.31 | loss  3.92 | ppl    50.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  78 | time: 126.80s | valid loss  4.14 | valid ppl    62.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  79 |   200/ 1106 batches | lr 20.00 | ms/batch 106.63 | loss  3.96 | ppl    52.61\n",
      "| epoch  79 |   400/ 1106 batches | lr 20.00 | ms/batch 107.57 | loss  3.86 | ppl    47.52\n",
      "| epoch  79 |   600/ 1106 batches | lr 20.00 | ms/batch 105.66 | loss  3.86 | ppl    47.69\n",
      "| epoch  79 |   800/ 1106 batches | lr 20.00 | ms/batch 107.33 | loss  3.89 | ppl    48.94\n",
      "| epoch  79 |  1000/ 1106 batches | lr 20.00 | ms/batch 104.51 | loss  3.93 | ppl    50.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  79 | time: 126.86s | valid loss  4.14 | valid ppl    62.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  80 |   200/ 1106 batches | lr 20.00 | ms/batch 107.82 | loss  3.95 | ppl    52.03\n",
      "| epoch  80 |   400/ 1106 batches | lr 20.00 | ms/batch 106.72 | loss  3.85 | ppl    46.97\n",
      "| epoch  80 |   600/ 1106 batches | lr 20.00 | ms/batch 105.72 | loss  3.86 | ppl    47.37\n",
      "| epoch  80 |   800/ 1106 batches | lr 20.00 | ms/batch 107.79 | loss  3.88 | ppl    48.19\n",
      "| epoch  80 |  1000/ 1106 batches | lr 20.00 | ms/batch 107.25 | loss  3.92 | ppl    50.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  80 | time: 126.79s | valid loss  4.13 | valid ppl    62.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  81 |   200/ 1106 batches | lr 20.00 | ms/batch 105.25 | loss  3.97 | ppl    52.90\n",
      "| epoch  81 |   400/ 1106 batches | lr 20.00 | ms/batch 105.05 | loss  3.85 | ppl    46.80\n",
      "| epoch  81 |   600/ 1106 batches | lr 20.00 | ms/batch 104.09 | loss  3.87 | ppl    47.90\n",
      "| epoch  81 |   800/ 1106 batches | lr 20.00 | ms/batch 105.85 | loss  3.88 | ppl    48.23\n",
      "| epoch  81 |  1000/ 1106 batches | lr 20.00 | ms/batch 105.39 | loss  3.94 | ppl    51.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  81 | time: 125.29s | valid loss  4.13 | valid ppl    62.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  82 |   200/ 1106 batches | lr 20.00 | ms/batch 105.36 | loss  3.95 | ppl    51.79\n",
      "| epoch  82 |   400/ 1106 batches | lr 20.00 | ms/batch 104.14 | loss  3.85 | ppl    46.78\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  82 |   600/ 1106 batches | lr 20.00 | ms/batch 107.11 | loss  3.86 | ppl    47.58\n",
      "| epoch  82 |   800/ 1106 batches | lr 20.00 | ms/batch 105.65 | loss  3.89 | ppl    48.73\n",
      "| epoch  82 |  1000/ 1106 batches | lr 20.00 | ms/batch 107.02 | loss  3.92 | ppl    50.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  82 | time: 125.26s | valid loss  4.13 | valid ppl    62.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  83 |   200/ 1106 batches | lr 20.00 | ms/batch 107.06 | loss  3.95 | ppl    52.02\n",
      "| epoch  83 |   400/ 1106 batches | lr 20.00 | ms/batch 106.22 | loss  3.86 | ppl    47.23\n",
      "| epoch  83 |   600/ 1106 batches | lr 20.00 | ms/batch 104.75 | loss  3.86 | ppl    47.64\n",
      "| epoch  83 |   800/ 1106 batches | lr 20.00 | ms/batch 106.09 | loss  3.88 | ppl    48.60\n",
      "| epoch  83 |  1000/ 1106 batches | lr 20.00 | ms/batch 104.76 | loss  3.92 | ppl    50.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  83 | time: 124.95s | valid loss  4.13 | valid ppl    62.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  84 |   200/ 1106 batches | lr 20.00 | ms/batch 106.08 | loss  3.94 | ppl    51.38\n",
      "| epoch  84 |   400/ 1106 batches | lr 20.00 | ms/batch 105.98 | loss  3.84 | ppl    46.70\n",
      "| epoch  84 |   600/ 1106 batches | lr 20.00 | ms/batch 107.02 | loss  3.86 | ppl    47.39\n",
      "| epoch  84 |   800/ 1106 batches | lr 20.00 | ms/batch 107.04 | loss  3.88 | ppl    48.63\n",
      "| epoch  84 |  1000/ 1106 batches | lr 20.00 | ms/batch 106.27 | loss  3.92 | ppl    50.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  84 | time: 125.45s | valid loss  4.13 | valid ppl    62.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  85 |   200/ 1106 batches | lr 20.00 | ms/batch 106.88 | loss  3.94 | ppl    51.59\n",
      "| epoch  85 |   400/ 1106 batches | lr 20.00 | ms/batch 105.97 | loss  3.84 | ppl    46.74\n",
      "| epoch  85 |   600/ 1106 batches | lr 20.00 | ms/batch 106.29 | loss  3.85 | ppl    47.23\n",
      "| epoch  85 |   800/ 1106 batches | lr 20.00 | ms/batch 105.94 | loss  3.87 | ppl    48.14\n",
      "| epoch  85 |  1000/ 1106 batches | lr 20.00 | ms/batch 106.20 | loss  3.90 | ppl    49.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  85 | time: 125.34s | valid loss  4.13 | valid ppl    62.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  86 |   200/ 1106 batches | lr 20.00 | ms/batch 105.97 | loss  3.94 | ppl    51.39\n",
      "| epoch  86 |   400/ 1106 batches | lr 20.00 | ms/batch 107.12 | loss  3.83 | ppl    45.91\n",
      "| epoch  86 |   600/ 1106 batches | lr 20.00 | ms/batch 106.29 | loss  3.86 | ppl    47.29\n",
      "| epoch  86 |   800/ 1106 batches | lr 20.00 | ms/batch 106.92 | loss  3.86 | ppl    47.61\n",
      "| epoch  86 |  1000/ 1106 batches | lr 20.00 | ms/batch 106.50 | loss  3.92 | ppl    50.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  86 | time: 126.57s | valid loss  4.13 | valid ppl    62.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  87 |   200/ 1106 batches | lr 20.00 | ms/batch 106.66 | loss  3.94 | ppl    51.67\n",
      "| epoch  87 |   400/ 1106 batches | lr 20.00 | ms/batch 105.87 | loss  3.83 | ppl    46.01\n",
      "| epoch  87 |   600/ 1106 batches | lr 20.00 | ms/batch 107.64 | loss  3.85 | ppl    47.22\n",
      "| epoch  87 |   800/ 1106 batches | lr 20.00 | ms/batch 107.65 | loss  3.87 | ppl    47.95\n",
      "| epoch  87 |  1000/ 1106 batches | lr 20.00 | ms/batch 106.48 | loss  3.90 | ppl    49.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  87 | time: 126.60s | valid loss  4.13 | valid ppl    62.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  88 |   200/ 1106 batches | lr 20.00 | ms/batch 107.25 | loss  3.94 | ppl    51.35\n",
      "| epoch  88 |   400/ 1106 batches | lr 20.00 | ms/batch 106.64 | loss  3.83 | ppl    46.25\n",
      "| epoch  88 |   600/ 1106 batches | lr 20.00 | ms/batch 107.65 | loss  3.84 | ppl    46.64\n",
      "| epoch  88 |   800/ 1106 batches | lr 20.00 | ms/batch 106.47 | loss  3.87 | ppl    47.82\n",
      "| epoch  88 |  1000/ 1106 batches | lr 20.00 | ms/batch 107.08 | loss  3.89 | ppl    48.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  88 | time: 126.40s | valid loss  4.13 | valid ppl    62.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  89 |   200/ 1106 batches | lr 20.00 | ms/batch 107.00 | loss  3.93 | ppl    50.94\n",
      "| epoch  89 |   400/ 1106 batches | lr 20.00 | ms/batch 107.14 | loss  3.82 | ppl    45.83\n",
      "| epoch  89 |   600/ 1106 batches | lr 20.00 | ms/batch 106.06 | loss  3.83 | ppl    46.26\n",
      "| epoch  89 |   800/ 1106 batches | lr 20.00 | ms/batch 107.88 | loss  3.87 | ppl    47.74\n",
      "| epoch  89 |  1000/ 1106 batches | lr 20.00 | ms/batch 106.07 | loss  3.91 | ppl    49.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  89 | time: 126.02s | valid loss  4.13 | valid ppl    62.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  90 |   200/ 1106 batches | lr 20.00 | ms/batch 107.00 | loss  3.94 | ppl    51.59\n",
      "| epoch  90 |   400/ 1106 batches | lr 20.00 | ms/batch 105.50 | loss  3.82 | ppl    45.59\n",
      "| epoch  90 |   600/ 1106 batches | lr 20.00 | ms/batch 104.94 | loss  3.84 | ppl    46.60\n",
      "| epoch  90 |   800/ 1106 batches | lr 20.00 | ms/batch 104.89 | loss  3.87 | ppl    47.87\n",
      "| epoch  90 |  1000/ 1106 batches | lr 20.00 | ms/batch 105.86 | loss  3.90 | ppl    49.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  90 | time: 125.16s | valid loss  4.13 | valid ppl    62.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  91 |   200/ 1106 batches | lr 20.00 | ms/batch 105.81 | loss  3.94 | ppl    51.23\n",
      "| epoch  91 |   400/ 1106 batches | lr 20.00 | ms/batch 105.28 | loss  3.81 | ppl    45.23\n",
      "| epoch  91 |   600/ 1106 batches | lr 20.00 | ms/batch 106.22 | loss  3.85 | ppl    46.96\n",
      "| epoch  91 |   800/ 1106 batches | lr 20.00 | ms/batch 106.65 | loss  3.86 | ppl    47.31\n",
      "| epoch  91 |  1000/ 1106 batches | lr 20.00 | ms/batch 105.61 | loss  3.90 | ppl    49.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  91 | time: 125.41s | valid loss  4.13 | valid ppl    62.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  92 |   200/ 1106 batches | lr 20.00 | ms/batch 104.90 | loss  3.92 | ppl    50.43\n",
      "| epoch  92 |   400/ 1106 batches | lr 20.00 | ms/batch 105.52 | loss  3.82 | ppl    45.40\n",
      "| epoch  92 |   600/ 1106 batches | lr 20.00 | ms/batch 105.47 | loss  3.85 | ppl    47.14\n",
      "| epoch  92 |   800/ 1106 batches | lr 20.00 | ms/batch 104.93 | loss  3.86 | ppl    47.43\n",
      "| epoch  92 |  1000/ 1106 batches | lr 20.00 | ms/batch 105.69 | loss  3.88 | ppl    48.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  92 | time: 124.80s | valid loss  4.13 | valid ppl    62.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  93 |   200/ 1106 batches | lr 20.00 | ms/batch 104.72 | loss  3.93 | ppl    50.88\n",
      "| epoch  93 |   400/ 1106 batches | lr 20.00 | ms/batch 106.14 | loss  3.82 | ppl    45.70\n",
      "| epoch  93 |   600/ 1106 batches | lr 20.00 | ms/batch 106.17 | loss  3.84 | ppl    46.55\n",
      "| epoch  93 |   800/ 1106 batches | lr 20.00 | ms/batch 105.85 | loss  3.85 | ppl    46.92\n",
      "| epoch  93 |  1000/ 1106 batches | lr 20.00 | ms/batch 107.27 | loss  3.89 | ppl    48.71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  93 | time: 125.13s | valid loss  4.13 | valid ppl    62.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  94 |   200/ 1106 batches | lr 20.00 | ms/batch 105.61 | loss  3.92 | ppl    50.40\n",
      "| epoch  94 |   400/ 1106 batches | lr 20.00 | ms/batch 104.65 | loss  3.82 | ppl    45.57\n",
      "| epoch  94 |   600/ 1106 batches | lr 20.00 | ms/batch 106.46 | loss  3.84 | ppl    46.61\n",
      "| epoch  94 |   800/ 1106 batches | lr 20.00 | ms/batch 105.45 | loss  3.85 | ppl    47.05\n",
      "| epoch  94 |  1000/ 1106 batches | lr 20.00 | ms/batch 107.43 | loss  3.88 | ppl    48.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  94 | time: 125.43s | valid loss  4.13 | valid ppl    61.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  95 |   200/ 1106 batches | lr 20.00 | ms/batch 106.85 | loss  3.92 | ppl    50.27\n",
      "| epoch  95 |   400/ 1106 batches | lr 20.00 | ms/batch 106.27 | loss  3.81 | ppl    45.28\n",
      "| epoch  95 |   600/ 1106 batches | lr 20.00 | ms/batch 106.31 | loss  3.83 | ppl    46.17\n",
      "| epoch  95 |   800/ 1106 batches | lr 20.00 | ms/batch 105.44 | loss  3.86 | ppl    47.46\n",
      "| epoch  95 |  1000/ 1106 batches | lr 20.00 | ms/batch 104.71 | loss  3.89 | ppl    48.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  95 | time: 125.53s | valid loss  4.13 | valid ppl    61.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  96 |   200/ 1106 batches | lr 20.00 | ms/batch 105.13 | loss  3.91 | ppl    49.94\n",
      "| epoch  96 |   400/ 1106 batches | lr 20.00 | ms/batch 105.52 | loss  3.82 | ppl    45.63\n",
      "| epoch  96 |   600/ 1106 batches | lr 20.00 | ms/batch 105.40 | loss  3.84 | ppl    46.34\n",
      "| epoch  96 |   800/ 1106 batches | lr 20.00 | ms/batch 106.48 | loss  3.85 | ppl    47.12\n",
      "| epoch  96 |  1000/ 1106 batches | lr 20.00 | ms/batch 105.84 | loss  3.87 | ppl    48.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  96 | time: 125.09s | valid loss  4.13 | valid ppl    61.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  97 |   200/ 1106 batches | lr 20.00 | ms/batch 106.25 | loss  3.91 | ppl    49.85\n",
      "| epoch  97 |   400/ 1106 batches | lr 20.00 | ms/batch 106.37 | loss  3.81 | ppl    45.24\n",
      "| epoch  97 |   600/ 1106 batches | lr 20.00 | ms/batch 105.44 | loss  3.82 | ppl    45.42\n",
      "| epoch  97 |   800/ 1106 batches | lr 20.00 | ms/batch 105.88 | loss  3.86 | ppl    47.27\n",
      "| epoch  97 |  1000/ 1106 batches | lr 20.00 | ms/batch 105.25 | loss  3.88 | ppl    48.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  97 | time: 124.82s | valid loss  4.13 | valid ppl    61.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  98 |   200/ 1106 batches | lr 20.00 | ms/batch 106.40 | loss  3.92 | ppl    50.26\n",
      "| epoch  98 |   400/ 1106 batches | lr 20.00 | ms/batch 105.33 | loss  3.80 | ppl    44.85\n",
      "| epoch  98 |   600/ 1106 batches | lr 20.00 | ms/batch 105.88 | loss  3.81 | ppl    45.22\n",
      "| epoch  98 |   800/ 1106 batches | lr 20.00 | ms/batch 104.19 | loss  3.84 | ppl    46.64\n",
      "| epoch  98 |  1000/ 1106 batches | lr 20.00 | ms/batch 105.77 | loss  3.89 | ppl    48.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  98 | time: 125.14s | valid loss  4.12 | valid ppl    61.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  99 |   200/ 1106 batches | lr 20.00 | ms/batch 106.36 | loss  3.91 | ppl    49.94\n",
      "| epoch  99 |   400/ 1106 batches | lr 20.00 | ms/batch 105.63 | loss  3.81 | ppl    45.27\n",
      "| epoch  99 |   600/ 1106 batches | lr 20.00 | ms/batch 106.40 | loss  3.82 | ppl    45.71\n",
      "| epoch  99 |   800/ 1106 batches | lr 20.00 | ms/batch 106.14 | loss  3.85 | ppl    46.95\n",
      "| epoch  99 |  1000/ 1106 batches | lr 20.00 | ms/batch 106.17 | loss  3.87 | ppl    47.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  99 | time: 124.81s | valid loss  4.12 | valid ppl    61.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch 100 |   200/ 1106 batches | lr 20.00 | ms/batch 104.24 | loss  3.90 | ppl    49.34\n",
      "| epoch 100 |   400/ 1106 batches | lr 20.00 | ms/batch 106.77 | loss  3.80 | ppl    44.77\n",
      "| epoch 100 |   600/ 1106 batches | lr 20.00 | ms/batch 106.46 | loss  3.82 | ppl    45.59\n",
      "| epoch 100 |   800/ 1106 batches | lr 20.00 | ms/batch 105.25 | loss  3.83 | ppl    46.14\n",
      "| epoch 100 |  1000/ 1106 batches | lr 20.00 | ms/batch 107.14 | loss  3.87 | ppl    48.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 100 | time: 125.40s | valid loss  4.12 | valid ppl    61.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch 101 |   200/ 1106 batches | lr 20.00 | ms/batch 105.83 | loss  3.91 | ppl    49.73\n",
      "| epoch 101 |   400/ 1106 batches | lr 20.00 | ms/batch 105.72 | loss  3.79 | ppl    44.37\n",
      "| epoch 101 |   600/ 1106 batches | lr 20.00 | ms/batch 105.73 | loss  3.81 | ppl    44.97\n",
      "| epoch 101 |   800/ 1106 batches | lr 20.00 | ms/batch 106.56 | loss  3.84 | ppl    46.67\n",
      "| epoch 101 |  1000/ 1106 batches | lr 20.00 | ms/batch 108.03 | loss  3.87 | ppl    47.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 101 | time: 125.29s | valid loss  4.12 | valid ppl    61.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch 102 |   200/ 1106 batches | lr 20.00 | ms/batch 105.59 | loss  3.91 | ppl    49.86\n",
      "| epoch 102 |   400/ 1106 batches | lr 20.00 | ms/batch 104.61 | loss  3.79 | ppl    44.08\n",
      "| epoch 102 |   600/ 1106 batches | lr 20.00 | ms/batch 105.28 | loss  3.83 | ppl    45.88\n",
      "| epoch 102 |   800/ 1106 batches | lr 20.00 | ms/batch 104.91 | loss  3.83 | ppl    46.21\n",
      "| epoch 102 |  1000/ 1106 batches | lr 20.00 | ms/batch 104.67 | loss  3.88 | ppl    48.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 102 | time: 125.20s | valid loss  4.12 | valid ppl    61.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch 103 |   200/ 1106 batches | lr 20.00 | ms/batch 105.80 | loss  3.90 | ppl    49.45\n",
      "| epoch 103 |   400/ 1106 batches | lr 20.00 | ms/batch 106.66 | loss  3.80 | ppl    44.92\n",
      "| epoch 103 |   600/ 1106 batches | lr 20.00 | ms/batch 106.61 | loss  3.82 | ppl    45.62\n",
      "| epoch 103 |   800/ 1106 batches | lr 20.00 | ms/batch 105.73 | loss  3.83 | ppl    46.22\n",
      "| epoch 103 |  1000/ 1106 batches | lr 20.00 | ms/batch 107.16 | loss  3.87 | ppl    47.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 103 | time: 125.62s | valid loss  4.12 | valid ppl    61.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch 104 |   200/ 1106 batches | lr 20.00 | ms/batch 105.24 | loss  3.90 | ppl    49.18\n",
      "| epoch 104 |   400/ 1106 batches | lr 20.00 | ms/batch 106.33 | loss  3.79 | ppl    44.15\n",
      "| epoch 104 |   600/ 1106 batches | lr 20.00 | ms/batch 104.66 | loss  3.80 | ppl    44.78\n",
      "| epoch 104 |   800/ 1106 batches | lr 20.00 | ms/batch 106.96 | loss  3.84 | ppl    46.60\n",
      "| epoch 104 |  1000/ 1106 batches | lr 20.00 | ms/batch 104.71 | loss  3.87 | ppl    47.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 104 | time: 125.16s | valid loss  4.12 | valid ppl    61.71\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Averaged!\n",
      "| epoch 105 |   200/ 1106 batches | lr 20.00 | ms/batch 105.75 | loss  3.90 | ppl    49.52\n",
      "| epoch 105 |   400/ 1106 batches | lr 20.00 | ms/batch 106.15 | loss  3.78 | ppl    44.03\n",
      "| epoch 105 |   600/ 1106 batches | lr 20.00 | ms/batch 106.12 | loss  3.80 | ppl    44.73\n",
      "| epoch 105 |   800/ 1106 batches | lr 20.00 | ms/batch 105.82 | loss  3.83 | ppl    46.14\n",
      "| epoch 105 |  1000/ 1106 batches | lr 20.00 | ms/batch 104.33 | loss  3.87 | ppl    48.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 105 | time: 125.25s | valid loss  4.12 | valid ppl    61.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch 106 |   200/ 1106 batches | lr 20.00 | ms/batch 105.09 | loss  3.90 | ppl    49.39\n",
      "| epoch 106 |   400/ 1106 batches | lr 20.00 | ms/batch 105.53 | loss  3.79 | ppl    44.46\n",
      "| epoch 106 |   600/ 1106 batches | lr 20.00 | ms/batch 105.41 | loss  3.81 | ppl    45.16\n",
      "| epoch 106 |   800/ 1106 batches | lr 20.00 | ms/batch 105.29 | loss  3.82 | ppl    45.70\n",
      "| epoch 106 |  1000/ 1106 batches | lr 20.00 | ms/batch 105.62 | loss  3.87 | ppl    47.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 106 | time: 124.93s | valid loss  4.12 | valid ppl    61.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch 107 |   200/ 1106 batches | lr 20.00 | ms/batch 107.65 | loss  3.90 | ppl    49.31\n",
      "| epoch 107 |   400/ 1106 batches | lr 20.00 | ms/batch 107.78 | loss  3.79 | ppl    44.44\n",
      "| epoch 107 |   600/ 1106 batches | lr 20.00 | ms/batch 106.03 | loss  3.79 | ppl    44.38\n",
      "| epoch 107 |   800/ 1106 batches | lr 20.00 | ms/batch 106.41 | loss  3.83 | ppl    46.09\n",
      "| epoch 107 |  1000/ 1106 batches | lr 20.00 | ms/batch 106.95 | loss  3.86 | ppl    47.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 107 | time: 126.45s | valid loss  4.12 | valid ppl    61.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch 108 |   200/ 1106 batches | lr 20.00 | ms/batch 107.13 | loss  3.89 | ppl    49.13\n",
      "| epoch 108 |   400/ 1106 batches | lr 20.00 | ms/batch 105.92 | loss  3.78 | ppl    43.77\n",
      "| epoch 108 |   600/ 1106 batches | lr 20.00 | ms/batch 106.50 | loss  3.80 | ppl    44.53\n",
      "| epoch 108 |   800/ 1106 batches | lr 20.00 | ms/batch 107.73 | loss  3.82 | ppl    45.52\n",
      "| epoch 108 |  1000/ 1106 batches | lr 20.00 | ms/batch 106.80 | loss  3.86 | ppl    47.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 108 | time: 126.77s | valid loss  4.12 | valid ppl    61.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch 109 |   200/ 1106 batches | lr 20.00 | ms/batch 107.54 | loss  3.90 | ppl    49.60\n",
      "| epoch 109 |   400/ 1106 batches | lr 20.00 | ms/batch 106.44 | loss  3.79 | ppl    44.04\n",
      "| epoch 109 |   600/ 1106 batches | lr 20.00 | ms/batch 108.95 | loss  3.80 | ppl    44.69\n",
      "| epoch 109 |   800/ 1106 batches | lr 20.00 | ms/batch 108.68 | loss  3.82 | ppl    45.82\n",
      "| epoch 109 |  1000/ 1106 batches | lr 20.00 | ms/batch 106.45 | loss  3.85 | ppl    46.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 109 | time: 126.45s | valid loss  4.12 | valid ppl    61.59\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch 110 |   200/ 1106 batches | lr 20.00 | ms/batch 106.82 | loss  3.88 | ppl    48.46\n",
      "| epoch 110 |   400/ 1106 batches | lr 20.00 | ms/batch 105.36 | loss  3.78 | ppl    43.76\n",
      "| epoch 110 |   600/ 1106 batches | lr 20.00 | ms/batch 107.48 | loss  3.81 | ppl    45.09\n",
      "| epoch 110 |   800/ 1106 batches | lr 20.00 | ms/batch 107.09 | loss  3.82 | ppl    45.53\n",
      "| epoch 110 |  1000/ 1106 batches | lr 20.00 | ms/batch 105.59 | loss  3.88 | ppl    48.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 110 | time: 127.10s | valid loss  4.12 | valid ppl    61.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch 111 |   200/ 1106 batches | lr 20.00 | ms/batch 106.18 | loss  3.89 | ppl    48.69\n",
      "| epoch 111 |   400/ 1106 batches | lr 20.00 | ms/batch 107.59 | loss  3.78 | ppl    43.97\n",
      "| epoch 111 |   600/ 1106 batches | lr 20.00 | ms/batch 107.31 | loss  3.80 | ppl    44.54\n",
      "| epoch 111 |   800/ 1106 batches | lr 20.00 | ms/batch 107.28 | loss  3.82 | ppl    45.46\n",
      "| epoch 111 |  1000/ 1106 batches | lr 20.00 | ms/batch 106.32 | loss  3.86 | ppl    47.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 111 | time: 126.08s | valid loss  4.12 | valid ppl    61.54\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch 112 |   200/ 1106 batches | lr 20.00 | ms/batch 107.63 | loss  3.89 | ppl    48.72\n",
      "| epoch 112 |   400/ 1106 batches | lr 20.00 | ms/batch 105.53 | loss  3.78 | ppl    43.71\n",
      "| epoch 112 |   600/ 1106 batches | lr 20.00 | ms/batch 104.54 | loss  3.79 | ppl    44.27\n",
      "| epoch 112 |   800/ 1106 batches | lr 20.00 | ms/batch 104.67 | loss  3.81 | ppl    45.37\n",
      "| epoch 112 |  1000/ 1106 batches | lr 20.00 | ms/batch 105.50 | loss  3.84 | ppl    46.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 112 | time: 124.75s | valid loss  4.12 | valid ppl    61.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch 113 |   200/ 1106 batches | lr 20.00 | ms/batch 105.65 | loss  3.89 | ppl    48.72\n",
      "| epoch 113 |   400/ 1106 batches | lr 20.00 | ms/batch 105.25 | loss  3.77 | ppl    43.18\n",
      "| epoch 113 |   600/ 1106 batches | lr 20.00 | ms/batch 106.51 | loss  3.80 | ppl    44.54\n",
      "| epoch 113 |   800/ 1106 batches | lr 20.00 | ms/batch 107.62 | loss  3.83 | ppl    46.27\n",
      "| epoch 113 |  1000/ 1106 batches | lr 20.00 | ms/batch 105.81 | loss  3.85 | ppl    46.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 113 | time: 125.22s | valid loss  4.12 | valid ppl    61.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch 114 |   200/ 1106 batches | lr 20.00 | ms/batch 106.92 | loss  3.89 | ppl    48.89\n",
      "| epoch 114 |   400/ 1106 batches | lr 20.00 | ms/batch 105.18 | loss  3.77 | ppl    43.43\n",
      "| epoch 114 |   600/ 1106 batches | lr 20.00 | ms/batch 105.32 | loss  3.79 | ppl    44.28\n",
      "| epoch 114 |   800/ 1106 batches | lr 20.00 | ms/batch 105.23 | loss  3.82 | ppl    45.40\n",
      "| epoch 114 |  1000/ 1106 batches | lr 20.00 | ms/batch 106.62 | loss  3.85 | ppl    47.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 114 | time: 124.75s | valid loss  4.12 | valid ppl    61.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch 115 |   200/ 1106 batches | lr 20.00 | ms/batch 106.08 | loss  3.88 | ppl    48.49\n",
      "| epoch 115 |   400/ 1106 batches | lr 20.00 | ms/batch 106.32 | loss  3.77 | ppl    43.21\n",
      "| epoch 115 |   600/ 1106 batches | lr 20.00 | ms/batch 104.01 | loss  3.80 | ppl    44.53\n",
      "| epoch 115 |   800/ 1106 batches | lr 20.00 | ms/batch 104.90 | loss  3.82 | ppl    45.70\n",
      "| epoch 115 |  1000/ 1106 batches | lr 20.00 | ms/batch 104.80 | loss  3.85 | ppl    47.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 115 | time: 125.33s | valid loss  4.12 | valid ppl    61.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch 116 |   200/ 1106 batches | lr 20.00 | ms/batch 105.42 | loss  3.89 | ppl    48.68\n",
      "| epoch 116 |   400/ 1106 batches | lr 20.00 | ms/batch 105.96 | loss  3.78 | ppl    43.69\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 116 |   600/ 1106 batches | lr 20.00 | ms/batch 105.03 | loss  3.79 | ppl    44.07\n",
      "| epoch 116 |   800/ 1106 batches | lr 20.00 | ms/batch 105.58 | loss  3.81 | ppl    45.19\n",
      "| epoch 116 |  1000/ 1106 batches | lr 20.00 | ms/batch 104.64 | loss  3.84 | ppl    46.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 116 | time: 125.02s | valid loss  4.12 | valid ppl    61.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch 117 |   200/ 1106 batches | lr 20.00 | ms/batch 105.98 | loss  3.87 | ppl    47.95\n",
      "| epoch 117 |   400/ 1106 batches | lr 20.00 | ms/batch 105.61 | loss  3.78 | ppl    43.69\n",
      "| epoch 117 |   600/ 1106 batches | lr 20.00 | ms/batch 104.24 | loss  3.79 | ppl    44.12\n",
      "| epoch 117 |   800/ 1106 batches | lr 20.00 | ms/batch 104.89 | loss  3.80 | ppl    44.55\n",
      "| epoch 117 |  1000/ 1106 batches | lr 20.00 | ms/batch 105.95 | loss  3.85 | ppl    47.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 117 | time: 124.99s | valid loss  4.12 | valid ppl    61.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch 118 |   200/ 1106 batches | lr 20.00 | ms/batch 104.57 | loss  3.87 | ppl    47.90\n",
      "| epoch 118 |   400/ 1106 batches | lr 20.00 | ms/batch 105.03 | loss  3.77 | ppl    43.33\n",
      "| epoch 118 |   600/ 1106 batches | lr 20.00 | ms/batch 106.23 | loss  3.78 | ppl    43.96\n",
      "| epoch 118 |   800/ 1106 batches | lr 20.00 | ms/batch 106.01 | loss  3.80 | ppl    44.52\n",
      "| epoch 118 |  1000/ 1106 batches | lr 20.00 | ms/batch 106.07 | loss  3.86 | ppl    47.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 118 | time: 125.32s | valid loss  4.12 | valid ppl    61.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch 119 |   200/ 1106 batches | lr 20.00 | ms/batch 106.45 | loss  3.87 | ppl    47.73\n",
      "| epoch 119 |   400/ 1106 batches | lr 20.00 | ms/batch 105.07 | loss  3.75 | ppl    42.42\n",
      "| epoch 119 |   600/ 1106 batches | lr 20.00 | ms/batch 105.32 | loss  3.79 | ppl    44.27\n",
      "| epoch 119 |   800/ 1106 batches | lr 20.00 | ms/batch 106.85 | loss  3.81 | ppl    45.07\n",
      "| epoch 119 |  1000/ 1106 batches | lr 20.00 | ms/batch 106.85 | loss  3.84 | ppl    46.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 119 | time: 124.89s | valid loss  4.12 | valid ppl    61.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch 120 |   200/ 1106 batches | lr 20.00 | ms/batch 106.30 | loss  3.87 | ppl    47.72\n",
      "| epoch 120 |   400/ 1106 batches | lr 20.00 | ms/batch 105.76 | loss  3.78 | ppl    43.74\n",
      "| epoch 120 |   600/ 1106 batches | lr 20.00 | ms/batch 107.24 | loss  3.78 | ppl    43.66\n",
      "| epoch 120 |   800/ 1106 batches | lr 20.00 | ms/batch 106.56 | loss  3.80 | ppl    44.83\n",
      "| epoch 120 |  1000/ 1106 batches | lr 20.00 | ms/batch 106.80 | loss  3.83 | ppl    46.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 120 | time: 126.06s | valid loss  4.12 | valid ppl    61.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch 121 |   200/ 1106 batches | lr 20.00 | ms/batch 107.96 | loss  3.88 | ppl    48.31\n",
      "| epoch 121 |   400/ 1106 batches | lr 20.00 | ms/batch 106.62 | loss  3.76 | ppl    42.93\n",
      "| epoch 121 |   600/ 1106 batches | lr 20.00 | ms/batch 107.18 | loss  3.77 | ppl    43.41\n",
      "| epoch 121 |   800/ 1106 batches | lr 20.00 | ms/batch 107.17 | loss  3.81 | ppl    45.15\n",
      "| epoch 121 |  1000/ 1106 batches | lr 20.00 | ms/batch 106.99 | loss  3.82 | ppl    45.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 121 | time: 126.50s | valid loss  4.12 | valid ppl    61.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch 122 |   200/ 1106 batches | lr 20.00 | ms/batch 107.18 | loss  3.86 | ppl    47.24\n",
      "| epoch 122 |   400/ 1106 batches | lr 20.00 | ms/batch 106.38 | loss  3.77 | ppl    43.26\n",
      "| epoch 122 |   600/ 1106 batches | lr 20.00 | ms/batch 106.57 | loss  3.76 | ppl    43.10\n",
      "| epoch 122 |   800/ 1106 batches | lr 20.00 | ms/batch 106.66 | loss  3.79 | ppl    44.46\n",
      "| epoch 122 |  1000/ 1106 batches | lr 20.00 | ms/batch 106.75 | loss  3.84 | ppl    46.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 122 | time: 126.55s | valid loss  4.12 | valid ppl    61.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch 123 |   200/ 1106 batches | lr 20.00 | ms/batch 108.17 | loss  3.86 | ppl    47.63\n",
      "| epoch 123 |   400/ 1106 batches | lr 20.00 | ms/batch 107.12 | loss  3.76 | ppl    42.81\n",
      "| epoch 123 |   600/ 1106 batches | lr 20.00 | ms/batch 107.79 | loss  3.77 | ppl    43.41\n",
      "| epoch 123 |   800/ 1106 batches | lr 20.00 | ms/batch 106.75 | loss  3.79 | ppl    44.14\n",
      "| epoch 123 |  1000/ 1106 batches | lr 20.00 | ms/batch 105.60 | loss  3.84 | ppl    46.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 123 | time: 125.82s | valid loss  4.12 | valid ppl    61.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch 124 |   200/ 1106 batches | lr 20.00 | ms/batch 106.48 | loss  3.86 | ppl    47.49\n",
      "| epoch 124 |   400/ 1106 batches | lr 20.00 | ms/batch 104.33 | loss  3.75 | ppl    42.51\n",
      "| epoch 124 |   600/ 1106 batches | lr 20.00 | ms/batch 105.22 | loss  3.79 | ppl    44.25\n",
      "| epoch 124 |   800/ 1106 batches | lr 20.00 | ms/batch 104.54 | loss  3.80 | ppl    44.53\n",
      "| epoch 124 |  1000/ 1106 batches | lr 20.00 | ms/batch 104.89 | loss  3.84 | ppl    46.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 124 | time: 125.06s | valid loss  4.11 | valid ppl    61.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch 125 |   200/ 1106 batches | lr 20.00 | ms/batch 106.22 | loss  3.85 | ppl    47.00\n",
      "| epoch 125 |   400/ 1106 batches | lr 20.00 | ms/batch 105.87 | loss  3.75 | ppl    42.35\n",
      "| epoch 125 |   600/ 1106 batches | lr 20.00 | ms/batch 105.93 | loss  3.77 | ppl    43.57\n",
      "| epoch 125 |   800/ 1106 batches | lr 20.00 | ms/batch 105.14 | loss  3.80 | ppl    44.61\n",
      "| epoch 125 |  1000/ 1106 batches | lr 20.00 | ms/batch 107.19 | loss  3.84 | ppl    46.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 125 | time: 125.78s | valid loss  4.11 | valid ppl    61.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch 126 |   200/ 1106 batches | lr 20.00 | ms/batch 106.20 | loss  3.85 | ppl    47.21\n",
      "| epoch 126 |   400/ 1106 batches | lr 20.00 | ms/batch 105.44 | loss  3.76 | ppl    43.04\n",
      "| epoch 126 |   600/ 1106 batches | lr 20.00 | ms/batch 105.87 | loss  3.77 | ppl    43.22\n",
      "| epoch 126 |   800/ 1106 batches | lr 20.00 | ms/batch 105.81 | loss  3.80 | ppl    44.60\n",
      "| epoch 126 |  1000/ 1106 batches | lr 20.00 | ms/batch 106.12 | loss  3.83 | ppl    46.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 126 | time: 124.89s | valid loss  4.11 | valid ppl    61.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch 127 |   200/ 1106 batches | lr 20.00 | ms/batch 105.73 | loss  3.85 | ppl    46.96\n",
      "| epoch 127 |   400/ 1106 batches | lr 20.00 | ms/batch 106.39 | loss  3.77 | ppl    43.44\n",
      "| epoch 127 |   600/ 1106 batches | lr 20.00 | ms/batch 104.40 | loss  3.77 | ppl    43.29\n",
      "| epoch 127 |   800/ 1106 batches | lr 20.00 | ms/batch 105.93 | loss  3.80 | ppl    44.49\n",
      "| epoch 127 |  1000/ 1106 batches | lr 20.00 | ms/batch 104.91 | loss  3.83 | ppl    46.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 127 | time: 124.93s | valid loss  4.11 | valid ppl    61.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch 128 |   200/ 1106 batches | lr 20.00 | ms/batch 105.20 | loss  3.85 | ppl    47.09\n",
      "| epoch 128 |   400/ 1106 batches | lr 20.00 | ms/batch 105.03 | loss  3.75 | ppl    42.42\n",
      "| epoch 128 |   600/ 1106 batches | lr 20.00 | ms/batch 107.38 | loss  3.76 | ppl    42.77\n",
      "| epoch 128 |   800/ 1106 batches | lr 20.00 | ms/batch 106.81 | loss  3.79 | ppl    44.47\n",
      "| epoch 128 |  1000/ 1106 batches | lr 20.00 | ms/batch 107.96 | loss  3.83 | ppl    46.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 128 | time: 125.79s | valid loss  4.11 | valid ppl    61.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch 129 |   200/ 1106 batches | lr 20.00 | ms/batch 105.90 | loss  3.85 | ppl    47.16\n",
      "| epoch 129 |   400/ 1106 batches | lr 20.00 | ms/batch 104.27 | loss  3.75 | ppl    42.58\n",
      "| epoch 129 |   600/ 1106 batches | lr 20.00 | ms/batch 106.50 | loss  3.76 | ppl    43.13\n",
      "| epoch 129 |   800/ 1106 batches | lr 20.00 | ms/batch 106.33 | loss  3.80 | ppl    44.56\n",
      "| epoch 129 |  1000/ 1106 batches | lr 20.00 | ms/batch 106.68 | loss  3.82 | ppl    45.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 129 | time: 125.42s | valid loss  4.11 | valid ppl    61.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch 130 |   200/ 1106 batches | lr 20.00 | ms/batch 106.47 | loss  3.86 | ppl    47.59\n",
      "| epoch 130 |   400/ 1106 batches | lr 20.00 | ms/batch 105.10 | loss  3.75 | ppl    42.53\n",
      "| epoch 130 |   600/ 1106 batches | lr 20.00 | ms/batch 107.30 | loss  3.77 | ppl    43.39\n",
      "| epoch 130 |   800/ 1106 batches | lr 20.00 | ms/batch 105.96 | loss  3.80 | ppl    44.54\n",
      "| epoch 130 |  1000/ 1106 batches | lr 20.00 | ms/batch 107.81 | loss  3.83 | ppl    45.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 130 | time: 125.96s | valid loss  4.11 | valid ppl    61.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch 131 |   200/ 1106 batches | lr 20.00 | ms/batch 105.27 | loss  3.85 | ppl    47.00\n",
      "| epoch 131 |   400/ 1106 batches | lr 20.00 | ms/batch 105.82 | loss  3.74 | ppl    42.22\n",
      "| epoch 131 |   600/ 1106 batches | lr 20.00 | ms/batch 103.92 | loss  3.76 | ppl    43.09\n",
      "| epoch 131 |   800/ 1106 batches | lr 20.00 | ms/batch 105.60 | loss  3.79 | ppl    44.37\n",
      "| epoch 131 |  1000/ 1106 batches | lr 20.00 | ms/batch 105.90 | loss  3.82 | ppl    45.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 131 | time: 125.23s | valid loss  4.11 | valid ppl    61.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch 132 |   200/ 1106 batches | lr 20.00 | ms/batch 104.80 | loss  3.85 | ppl    46.89\n",
      "| epoch 132 |   400/ 1106 batches | lr 20.00 | ms/batch 106.30 | loss  3.75 | ppl    42.73\n",
      "| epoch 132 |   600/ 1106 batches | lr 20.00 | ms/batch 105.86 | loss  3.76 | ppl    42.95\n",
      "| epoch 132 |   800/ 1106 batches | lr 20.00 | ms/batch 111.54 | loss  3.79 | ppl    44.38\n",
      "| epoch 132 |  1000/ 1106 batches | lr 20.00 | ms/batch 109.91 | loss  3.83 | ppl    45.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 132 | time: 127.58s | valid loss  4.11 | valid ppl    61.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch 133 |   200/ 1106 batches | lr 20.00 | ms/batch 107.30 | loss  3.85 | ppl    47.17\n",
      "| epoch 133 |   400/ 1106 batches | lr 20.00 | ms/batch 106.49 | loss  3.75 | ppl    42.70\n",
      "| epoch 133 |   600/ 1106 batches | lr 20.00 | ms/batch 106.26 | loss  3.76 | ppl    43.13\n",
      "| epoch 133 |   800/ 1106 batches | lr 20.00 | ms/batch 106.52 | loss  3.79 | ppl    44.46\n",
      "| epoch 133 |  1000/ 1106 batches | lr 20.00 | ms/batch 106.69 | loss  3.81 | ppl    45.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 133 | time: 126.17s | valid loss  4.11 | valid ppl    61.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch 134 |   200/ 1106 batches | lr 20.00 | ms/batch 105.66 | loss  3.86 | ppl    47.32\n",
      "| epoch 134 |   400/ 1106 batches | lr 20.00 | ms/batch 106.23 | loss  3.74 | ppl    42.29\n",
      "| epoch 134 |   600/ 1106 batches | lr 20.00 | ms/batch 104.76 | loss  3.76 | ppl    43.09\n",
      "| epoch 134 |   800/ 1106 batches | lr 20.00 | ms/batch 117.81 | loss  3.79 | ppl    44.09\n",
      "| epoch 134 |  1000/ 1106 batches | lr 20.00 | ms/batch 104.49 | loss  3.82 | ppl    45.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 134 | time: 127.85s | valid loss  4.11 | valid ppl    61.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch 135 |   200/ 1106 batches | lr 20.00 | ms/batch 105.39 | loss  3.83 | ppl    45.97\n",
      "| epoch 135 |   400/ 1106 batches | lr 20.00 | ms/batch 106.07 | loss  3.74 | ppl    42.13\n",
      "| epoch 135 |   600/ 1106 batches | lr 20.00 | ms/batch 104.94 | loss  3.77 | ppl    43.23\n",
      "| epoch 135 |   800/ 1106 batches | lr 20.00 | ms/batch 106.44 | loss  3.78 | ppl    43.95\n",
      "| epoch 135 |  1000/ 1106 batches | lr 20.00 | ms/batch 104.80 | loss  3.83 | ppl    45.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 135 | time: 125.40s | valid loss  4.11 | valid ppl    61.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch 136 |   200/ 1106 batches | lr 20.00 | ms/batch 104.39 | loss  3.85 | ppl    46.90\n",
      "| epoch 136 |   400/ 1106 batches | lr 20.00 | ms/batch 105.43 | loss  3.73 | ppl    41.87\n",
      "| epoch 136 |   600/ 1106 batches | lr 20.00 | ms/batch 106.89 | loss  3.76 | ppl    42.90\n",
      "| epoch 136 |   800/ 1106 batches | lr 20.00 | ms/batch 105.70 | loss  3.78 | ppl    43.93\n",
      "| epoch 136 |  1000/ 1106 batches | lr 20.00 | ms/batch 106.41 | loss  3.82 | ppl    45.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 136 | time: 125.40s | valid loss  4.11 | valid ppl    61.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch 137 |   200/ 1106 batches | lr 20.00 | ms/batch 104.85 | loss  3.84 | ppl    46.65\n",
      "| epoch 137 |   400/ 1106 batches | lr 20.00 | ms/batch 105.68 | loss  3.75 | ppl    42.39\n",
      "| epoch 137 |   600/ 1106 batches | lr 20.00 | ms/batch 105.05 | loss  3.76 | ppl    43.15\n",
      "| epoch 137 |   800/ 1106 batches | lr 20.00 | ms/batch 104.78 | loss  3.77 | ppl    43.57\n",
      "| epoch 137 |  1000/ 1106 batches | lr 20.00 | ms/batch 104.23 | loss  3.83 | ppl    46.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 137 | time: 125.63s | valid loss  4.11 | valid ppl    61.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch 138 |   200/ 1106 batches | lr 20.00 | ms/batch 106.17 | loss  3.85 | ppl    46.76\n",
      "| epoch 138 |   400/ 1106 batches | lr 20.00 | ms/batch 106.84 | loss  3.73 | ppl    41.53\n",
      "| epoch 138 |   600/ 1106 batches | lr 20.00 | ms/batch 106.09 | loss  3.75 | ppl    42.52\n",
      "| epoch 138 |   800/ 1106 batches | lr 20.00 | ms/batch 105.60 | loss  3.79 | ppl    44.19\n",
      "| epoch 138 |  1000/ 1106 batches | lr 20.00 | ms/batch 105.74 | loss  3.82 | ppl    45.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 138 | time: 125.59s | valid loss  4.11 | valid ppl    60.98\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Averaged!\n",
      "| epoch 139 |   200/ 1106 batches | lr 20.00 | ms/batch 107.14 | loss  3.84 | ppl    46.72\n",
      "| epoch 139 |   400/ 1106 batches | lr 20.00 | ms/batch 105.64 | loss  3.72 | ppl    41.38\n",
      "| epoch 139 |   600/ 1106 batches | lr 20.00 | ms/batch 104.93 | loss  3.75 | ppl    42.70\n",
      "| epoch 139 |   800/ 1106 batches | lr 20.00 | ms/batch 106.74 | loss  3.78 | ppl    43.64\n",
      "| epoch 139 |  1000/ 1106 batches | lr 20.00 | ms/batch 104.82 | loss  3.81 | ppl    44.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 139 | time: 126.24s | valid loss  4.11 | valid ppl    60.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch 140 |   200/ 1106 batches | lr 20.00 | ms/batch 120.82 | loss  3.85 | ppl    47.20\n",
      "| epoch 140 |   400/ 1106 batches | lr 20.00 | ms/batch 105.72 | loss  3.72 | ppl    41.46\n",
      "| epoch 140 |   600/ 1106 batches | lr 20.00 | ms/batch 105.47 | loss  3.76 | ppl    42.80\n",
      "| epoch 140 |   800/ 1106 batches | lr 20.00 | ms/batch 104.56 | loss  3.78 | ppl    43.87\n",
      "| epoch 140 |  1000/ 1106 batches | lr 20.00 | ms/batch 105.27 | loss  3.81 | ppl    45.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 140 | time: 128.02s | valid loss  4.11 | valid ppl    60.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch 141 |   200/ 1106 batches | lr 20.00 | ms/batch 106.25 | loss  3.84 | ppl    46.36\n",
      "| epoch 141 |   400/ 1106 batches | lr 20.00 | ms/batch 105.71 | loss  3.73 | ppl    41.67\n",
      "| epoch 141 |   600/ 1106 batches | lr 20.00 | ms/batch 106.29 | loss  3.76 | ppl    43.10\n",
      "| epoch 141 |   800/ 1106 batches | lr 20.00 | ms/batch 106.37 | loss  3.78 | ppl    43.61\n",
      "| epoch 141 |  1000/ 1106 batches | lr 20.00 | ms/batch 105.72 | loss  3.81 | ppl    45.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 141 | time: 127.63s | valid loss  4.11 | valid ppl    60.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch 142 |   200/ 1106 batches | lr 20.00 | ms/batch 215.20 | loss  3.84 | ppl    46.49\n",
      "| epoch 142 |   400/ 1106 batches | lr 20.00 | ms/batch 107.68 | loss  3.73 | ppl    41.78\n",
      "| epoch 142 |   600/ 1106 batches | lr 20.00 | ms/batch 106.75 | loss  3.74 | ppl    42.17\n",
      "| epoch 142 |   800/ 1106 batches | lr 20.00 | ms/batch 106.56 | loss  3.79 | ppl    44.06\n",
      "| epoch 142 |  1000/ 1106 batches | lr 20.00 | ms/batch 105.72 | loss  3.81 | ppl    45.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 142 | time: 147.99s | valid loss  4.11 | valid ppl    60.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch 143 |   200/ 1106 batches | lr 20.00 | ms/batch 109.54 | loss  3.84 | ppl    46.69\n",
      "| epoch 143 |   400/ 1106 batches | lr 20.00 | ms/batch 110.30 | loss  3.74 | ppl    42.09\n",
      "| epoch 143 |   600/ 1106 batches | lr 20.00 | ms/batch 105.92 | loss  3.74 | ppl    42.13\n",
      "| epoch 143 |   800/ 1106 batches | lr 20.00 | ms/batch 107.36 | loss  3.77 | ppl    43.49\n",
      "| epoch 143 |  1000/ 1106 batches | lr 20.00 | ms/batch 107.44 | loss  3.80 | ppl    44.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 143 | time: 129.46s | valid loss  4.11 | valid ppl    60.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch 144 |   200/ 1106 batches | lr 20.00 | ms/batch 123.03 | loss  3.84 | ppl    46.60\n",
      "| epoch 144 |   400/ 1106 batches | lr 20.00 | ms/batch 130.96 | loss  3.74 | ppl    42.07\n",
      "| epoch 144 |   600/ 1106 batches | lr 20.00 | ms/batch 126.40 | loss  3.74 | ppl    42.27\n",
      "| epoch 144 |   800/ 1106 batches | lr 20.00 | ms/batch 124.97 | loss  3.77 | ppl    43.58\n",
      "| epoch 144 |  1000/ 1106 batches | lr 20.00 | ms/batch 107.00 | loss  3.80 | ppl    44.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 144 | time: 141.71s | valid loss  4.11 | valid ppl    60.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch 145 |   200/ 1106 batches | lr 20.00 | ms/batch 108.06 | loss  3.84 | ppl    46.45\n",
      "| epoch 145 |   400/ 1106 batches | lr 20.00 | ms/batch 107.42 | loss  3.73 | ppl    41.72\n",
      "| epoch 145 |   600/ 1106 batches | lr 20.00 | ms/batch 105.17 | loss  3.74 | ppl    42.21\n",
      "| epoch 145 |   800/ 1106 batches | lr 20.00 | ms/batch 106.77 | loss  3.77 | ppl    43.27\n",
      "| epoch 145 |  1000/ 1106 batches | lr 20.00 | ms/batch 105.50 | loss  3.82 | ppl    45.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 145 | time: 126.72s | valid loss  4.11 | valid ppl    60.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch 146 |   200/ 1106 batches | lr 20.00 | ms/batch 106.42 | loss  3.83 | ppl    46.18\n",
      "| epoch 146 |   400/ 1106 batches | lr 20.00 | ms/batch 105.91 | loss  3.72 | ppl    41.17\n",
      "| epoch 146 |   600/ 1106 batches | lr 20.00 | ms/batch 106.53 | loss  3.75 | ppl    42.42\n",
      "| epoch 146 |   800/ 1106 batches | lr 20.00 | ms/batch 106.81 | loss  3.77 | ppl    43.57\n",
      "| epoch 146 |  1000/ 1106 batches | lr 20.00 | ms/batch 107.12 | loss  3.83 | ppl    45.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 146 | time: 126.71s | valid loss  4.11 | valid ppl    60.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch 147 |   200/ 1106 batches | lr 20.00 | ms/batch 106.46 | loss  3.84 | ppl    46.55\n",
      "| epoch 147 |   400/ 1106 batches | lr 20.00 | ms/batch 107.08 | loss  3.73 | ppl    41.58\n",
      "| epoch 147 |   600/ 1106 batches | lr 20.00 | ms/batch 106.62 | loss  3.75 | ppl    42.41\n",
      "| epoch 147 |   800/ 1106 batches | lr 20.00 | ms/batch 105.73 | loss  3.76 | ppl    42.89\n",
      "| epoch 147 |  1000/ 1106 batches | lr 20.00 | ms/batch 105.44 | loss  3.81 | ppl    44.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 147 | time: 125.91s | valid loss  4.11 | valid ppl    60.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch 148 |   200/ 1106 batches | lr 20.00 | ms/batch 106.03 | loss  3.83 | ppl    46.01\n",
      "| epoch 148 |   400/ 1106 batches | lr 20.00 | ms/batch 106.10 | loss  3.72 | ppl    41.18\n",
      "| epoch 148 |   600/ 1106 batches | lr 20.00 | ms/batch 104.07 | loss  3.75 | ppl    42.38\n",
      "| epoch 148 |   800/ 1106 batches | lr 20.00 | ms/batch 105.58 | loss  3.76 | ppl    43.16\n",
      "| epoch 148 |  1000/ 1106 batches | lr 20.00 | ms/batch 106.18 | loss  3.79 | ppl    44.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 148 | time: 125.20s | valid loss  4.11 | valid ppl    60.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch 149 |   200/ 1106 batches | lr 20.00 | ms/batch 106.98 | loss  3.83 | ppl    46.00\n",
      "| epoch 149 |   400/ 1106 batches | lr 20.00 | ms/batch 106.07 | loss  3.72 | ppl    41.19\n",
      "| epoch 149 |   600/ 1106 batches | lr 20.00 | ms/batch 107.51 | loss  3.74 | ppl    41.94\n",
      "| epoch 149 |   800/ 1106 batches | lr 20.00 | ms/batch 106.98 | loss  3.77 | ppl    43.39\n",
      "| epoch 149 |  1000/ 1106 batches | lr 20.00 | ms/batch 106.54 | loss  3.79 | ppl    44.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 149 | time: 125.11s | valid loss  4.11 | valid ppl    60.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch 150 |   200/ 1106 batches | lr 20.00 | ms/batch 105.60 | loss  3.81 | ppl    45.23\n",
      "| epoch 150 |   400/ 1106 batches | lr 20.00 | ms/batch 105.78 | loss  3.72 | ppl    41.27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 150 |   600/ 1106 batches | lr 20.00 | ms/batch 105.83 | loss  3.74 | ppl    42.08\n",
      "| epoch 150 |   800/ 1106 batches | lr 20.00 | ms/batch 105.91 | loss  3.77 | ppl    43.30\n",
      "| epoch 150 |  1000/ 1106 batches | lr 20.00 | ms/batch 104.83 | loss  3.81 | ppl    45.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 150 | time: 127.73s | valid loss  4.11 | valid ppl    60.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "=========================================================================================\n",
      "| End of training | test loss  4.07 | test ppl    58.77\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "args = new_params(\"--data data/penn --dropouti 0.4 --dropoutl 0.29 --dropouth 0.225 --seed 20 --batch_size 12\\\n",
    "    --max_seq_len_delta 15 --lr 20.0 --epoch 150 --nhid 1010 --nhidlast 650 --emsize 280 --n_experts 5 --mos --save Experiments/PTB-MoC-Experts5\")\n",
    "train_data = batchify(corpus.train, args.batch_size, args)\n",
    "\n",
    "model, parallel_model = new_model(args, corpus)\n",
    "SGD(model, parallel_model, args, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment dir : PTB/3000Emb-20180130-000512\n",
      "torch.Size([77465, 12])\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n",
      "Model param size: 22615200\n",
      "Args: Namespace(alpha=2, batch_size=12, beta=1, bptt=70, clip=0.25, continue_train=False, cuda=True, data='data/penn', dropout=0.4, dropoute=0.1, dropouth=0.225, dropouti=0.4, dropoutl=0.29, emsize=1200, epochs=150, log_interval=200, lr=20.0, max_seq_len_delta=40, moc=False, model='LSTM', mos=False, n_experts=15, nhid=350, nhidlast=1200, nlayers=3, nonmono=5, save='PTB/3000Emb-20180130-000512', seed=20, single_gpu=False, small_batch_size=12, tied=True, wdecay=1.2e-06, wdrop=0.5)\n",
      "Model total parameters: 22615200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pio/os/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py:224: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greately increasing memory usage. To compact weights again call flatten_parameters().\n",
      "  result = self.forward(*input, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 1106 batches | lr 20.00 | ms/batch 88.04 | loss  7.27 | ppl  1443.30\n",
      "| epoch   1 |   400/ 1106 batches | lr 20.00 | ms/batch 73.98 | loss  6.54 | ppl   695.53\n",
      "| epoch   1 |   600/ 1106 batches | lr 20.00 | ms/batch 73.55 | loss  6.21 | ppl   499.68\n",
      "| epoch   1 |   800/ 1106 batches | lr 20.00 | ms/batch 74.98 | loss  6.05 | ppl   423.71\n",
      "| epoch   1 |  1000/ 1106 batches | lr 20.00 | ms/batch 73.92 | loss  5.90 | ppl   366.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 89.65s | valid loss  5.62 | valid ppl   274.69\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pio/os/anaconda3/lib/python3.6/site-packages/torch/serialization.py:147: UserWarning: Couldn't retrieve source code for container of type RNNModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Normal!\n",
      "| epoch   2 |   200/ 1106 batches | lr 20.00 | ms/batch 72.37 | loss  5.73 | ppl   307.26\n",
      "| epoch   2 |   400/ 1106 batches | lr 20.00 | ms/batch 73.28 | loss  5.59 | ppl   268.74\n",
      "| epoch   2 |   600/ 1106 batches | lr 20.00 | ms/batch 76.52 | loss  5.49 | ppl   241.94\n",
      "| epoch   2 |   800/ 1106 batches | lr 20.00 | ms/batch 73.07 | loss  5.48 | ppl   240.26\n",
      "| epoch   2 |  1000/ 1106 batches | lr 20.00 | ms/batch 73.32 | loss  5.44 | ppl   229.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 86.94s | valid loss  5.20 | valid ppl   180.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   3 |   200/ 1106 batches | lr 20.00 | ms/batch 75.91 | loss  5.39 | ppl   219.24\n",
      "| epoch   3 |   400/ 1106 batches | lr 20.00 | ms/batch 76.25 | loss  5.28 | ppl   195.61\n",
      "| epoch   3 |   600/ 1106 batches | lr 20.00 | ms/batch 75.21 | loss  5.23 | ppl   187.45\n",
      "| epoch   3 |   800/ 1106 batches | lr 20.00 | ms/batch 74.32 | loss  5.24 | ppl   189.06\n",
      "| epoch   3 |  1000/ 1106 batches | lr 20.00 | ms/batch 73.17 | loss  5.22 | ppl   184.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 87.54s | valid loss  5.00 | valid ppl   147.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   4 |   200/ 1106 batches | lr 20.00 | ms/batch 73.93 | loss  5.21 | ppl   182.52\n",
      "| epoch   4 |   400/ 1106 batches | lr 20.00 | ms/batch 72.40 | loss  5.09 | ppl   162.02\n",
      "| epoch   4 |   600/ 1106 batches | lr 20.00 | ms/batch 74.50 | loss  5.06 | ppl   158.09\n",
      "| epoch   4 |   800/ 1106 batches | lr 20.00 | ms/batch 71.74 | loss  5.08 | ppl   160.17\n",
      "| epoch   4 |  1000/ 1106 batches | lr 20.00 | ms/batch 74.24 | loss  5.07 | ppl   159.59\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 86.64s | valid loss  4.87 | valid ppl   129.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   5 |   200/ 1106 batches | lr 20.00 | ms/batch 74.06 | loss  5.06 | ppl   158.27\n",
      "| epoch   5 |   400/ 1106 batches | lr 20.00 | ms/batch 75.53 | loss  4.94 | ppl   140.06\n",
      "| epoch   5 |   600/ 1106 batches | lr 20.00 | ms/batch 74.61 | loss  4.92 | ppl   137.34\n",
      "| epoch   5 |   800/ 1106 batches | lr 20.00 | ms/batch 69.88 | loss  4.96 | ppl   143.17\n",
      "| epoch   5 |  1000/ 1106 batches | lr 20.00 | ms/batch 74.72 | loss  4.96 | ppl   142.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 87.68s | valid loss  4.77 | valid ppl   118.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   6 |   200/ 1106 batches | lr 20.00 | ms/batch 77.29 | loss  4.96 | ppl   142.20\n",
      "| epoch   6 |   400/ 1106 batches | lr 20.00 | ms/batch 74.17 | loss  4.82 | ppl   124.27\n",
      "| epoch   6 |   600/ 1106 batches | lr 20.00 | ms/batch 71.44 | loss  4.81 | ppl   123.15\n",
      "| epoch   6 |   800/ 1106 batches | lr 20.00 | ms/batch 73.51 | loss  4.86 | ppl   128.50\n",
      "| epoch   6 |  1000/ 1106 batches | lr 20.00 | ms/batch 76.11 | loss  4.86 | ppl   129.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 88.21s | valid loss  4.69 | valid ppl   109.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   7 |   200/ 1106 batches | lr 20.00 | ms/batch 75.31 | loss  4.87 | ppl   130.45\n",
      "| epoch   7 |   400/ 1106 batches | lr 20.00 | ms/batch 71.55 | loss  4.73 | ppl   113.45\n",
      "| epoch   7 |   600/ 1106 batches | lr 20.00 | ms/batch 73.04 | loss  4.73 | ppl   113.18\n",
      "| epoch   7 |   800/ 1106 batches | lr 20.00 | ms/batch 74.62 | loss  4.77 | ppl   117.44\n",
      "| epoch   7 |  1000/ 1106 batches | lr 20.00 | ms/batch 72.87 | loss  4.80 | ppl   121.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 87.50s | valid loss  4.63 | valid ppl   102.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   8 |   200/ 1106 batches | lr 20.00 | ms/batch 76.02 | loss  4.79 | ppl   120.88\n",
      "| epoch   8 |   400/ 1106 batches | lr 20.00 | ms/batch 73.59 | loss  4.67 | ppl   106.31\n",
      "| epoch   8 |   600/ 1106 batches | lr 20.00 | ms/batch 73.57 | loss  4.67 | ppl   106.43\n",
      "| epoch   8 |   800/ 1106 batches | lr 20.00 | ms/batch 74.10 | loss  4.70 | ppl   110.30\n",
      "| epoch   8 |  1000/ 1106 batches | lr 20.00 | ms/batch 75.93 | loss  4.72 | ppl   112.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 88.61s | valid loss  4.59 | valid ppl    98.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   9 |   200/ 1106 batches | lr 20.00 | ms/batch 74.22 | loss  4.74 | ppl   113.89\n",
      "| epoch   9 |   400/ 1106 batches | lr 20.00 | ms/batch 75.03 | loss  4.61 | ppl   100.09\n",
      "| epoch   9 |   600/ 1106 batches | lr 20.00 | ms/batch 74.38 | loss  4.61 | ppl   100.51\n",
      "| epoch   9 |   800/ 1106 batches | lr 20.00 | ms/batch 75.30 | loss  4.64 | ppl   103.62\n",
      "| epoch   9 |  1000/ 1106 batches | lr 20.00 | ms/batch 74.75 | loss  4.66 | ppl   105.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 88.34s | valid loss  4.55 | valid ppl    94.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  10 |   200/ 1106 batches | lr 20.00 | ms/batch 74.11 | loss  4.68 | ppl   107.39\n",
      "| epoch  10 |   400/ 1106 batches | lr 20.00 | ms/batch 75.95 | loss  4.55 | ppl    94.31\n",
      "| epoch  10 |   600/ 1106 batches | lr 20.00 | ms/batch 76.63 | loss  4.54 | ppl    94.13\n",
      "| epoch  10 |   800/ 1106 batches | lr 20.00 | ms/batch 74.10 | loss  4.58 | ppl    97.73\n",
      "| epoch  10 |  1000/ 1106 batches | lr 20.00 | ms/batch 75.58 | loss  4.59 | ppl    98.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 88.15s | valid loss  4.52 | valid ppl    91.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  11 |   200/ 1106 batches | lr 20.00 | ms/batch 75.43 | loss  4.63 | ppl   102.64\n",
      "| epoch  11 |   400/ 1106 batches | lr 20.00 | ms/batch 76.40 | loss  4.50 | ppl    89.92\n",
      "| epoch  11 |   600/ 1106 batches | lr 20.00 | ms/batch 76.97 | loss  4.50 | ppl    89.79\n",
      "| epoch  11 |   800/ 1106 batches | lr 20.00 | ms/batch 74.76 | loss  4.54 | ppl    93.81\n",
      "| epoch  11 |  1000/ 1106 batches | lr 20.00 | ms/batch 71.52 | loss  4.56 | ppl    95.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 88.57s | valid loss  4.49 | valid ppl    89.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  12 |   200/ 1106 batches | lr 20.00 | ms/batch 74.69 | loss  4.58 | ppl    97.52\n",
      "| epoch  12 |   400/ 1106 batches | lr 20.00 | ms/batch 75.37 | loss  4.45 | ppl    86.02\n",
      "| epoch  12 |   600/ 1106 batches | lr 20.00 | ms/batch 75.28 | loss  4.46 | ppl    86.79\n",
      "| epoch  12 |   800/ 1106 batches | lr 20.00 | ms/batch 73.75 | loss  4.51 | ppl    90.51\n",
      "| epoch  12 |  1000/ 1106 batches | lr 20.00 | ms/batch 73.64 | loss  4.53 | ppl    92.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 87.83s | valid loss  4.47 | valid ppl    87.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  13 |   200/ 1106 batches | lr 20.00 | ms/batch 75.10 | loss  4.53 | ppl    93.22\n",
      "| epoch  13 |   400/ 1106 batches | lr 20.00 | ms/batch 76.97 | loss  4.43 | ppl    83.92\n",
      "| epoch  13 |   600/ 1106 batches | lr 20.00 | ms/batch 84.54 | loss  4.42 | ppl    82.90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  13 |   800/ 1106 batches | lr 20.00 | ms/batch 71.78 | loss  4.46 | ppl    86.61\n",
      "| epoch  13 |  1000/ 1106 batches | lr 20.00 | ms/batch 75.88 | loss  4.47 | ppl    87.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 89.80s | valid loss  4.46 | valid ppl    86.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  14 |   200/ 1106 batches | lr 20.00 | ms/batch 76.48 | loss  4.50 | ppl    90.36\n",
      "| epoch  14 |   400/ 1106 batches | lr 20.00 | ms/batch 73.45 | loss  4.38 | ppl    80.01\n",
      "| epoch  14 |   600/ 1106 batches | lr 20.00 | ms/batch 71.53 | loss  4.37 | ppl    79.17\n",
      "| epoch  14 |   800/ 1106 batches | lr 20.00 | ms/batch 73.72 | loss  4.42 | ppl    83.28\n",
      "| epoch  14 |  1000/ 1106 batches | lr 20.00 | ms/batch 76.19 | loss  4.44 | ppl    85.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 87.81s | valid loss  4.45 | valid ppl    85.54\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  15 |   200/ 1106 batches | lr 20.00 | ms/batch 76.45 | loss  4.46 | ppl    86.43\n",
      "| epoch  15 |   400/ 1106 batches | lr 20.00 | ms/batch 77.13 | loss  4.35 | ppl    77.29\n",
      "| epoch  15 |   600/ 1106 batches | lr 20.00 | ms/batch 74.98 | loss  4.36 | ppl    78.32\n",
      "| epoch  15 |   800/ 1106 batches | lr 20.00 | ms/batch 72.91 | loss  4.39 | ppl    80.26\n",
      "| epoch  15 |  1000/ 1106 batches | lr 20.00 | ms/batch 74.43 | loss  4.40 | ppl    81.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 88.81s | valid loss  4.43 | valid ppl    83.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  16 |   200/ 1106 batches | lr 20.00 | ms/batch 73.83 | loss  4.45 | ppl    85.58\n",
      "| epoch  16 |   400/ 1106 batches | lr 20.00 | ms/batch 73.19 | loss  4.33 | ppl    76.30\n",
      "| epoch  16 |   600/ 1106 batches | lr 20.00 | ms/batch 75.19 | loss  4.32 | ppl    74.95\n",
      "| epoch  16 |   800/ 1106 batches | lr 20.00 | ms/batch 76.87 | loss  4.37 | ppl    78.87\n",
      "| epoch  16 |  1000/ 1106 batches | lr 20.00 | ms/batch 75.32 | loss  4.38 | ppl    79.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 88.34s | valid loss  4.42 | valid ppl    83.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  17 |   200/ 1106 batches | lr 20.00 | ms/batch 74.05 | loss  4.41 | ppl    82.46\n",
      "| epoch  17 |   400/ 1106 batches | lr 20.00 | ms/batch 75.30 | loss  4.28 | ppl    72.42\n",
      "| epoch  17 |   600/ 1106 batches | lr 20.00 | ms/batch 75.46 | loss  4.30 | ppl    73.60\n",
      "| epoch  17 |   800/ 1106 batches | lr 20.00 | ms/batch 74.18 | loss  4.34 | ppl    76.63\n",
      "| epoch  17 |  1000/ 1106 batches | lr 20.00 | ms/batch 76.46 | loss  4.36 | ppl    77.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 87.92s | valid loss  4.40 | valid ppl    81.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  18 |   200/ 1106 batches | lr 20.00 | ms/batch 74.04 | loss  4.37 | ppl    79.14\n",
      "| epoch  18 |   400/ 1106 batches | lr 20.00 | ms/batch 74.60 | loss  4.26 | ppl    70.83\n",
      "| epoch  18 |   600/ 1106 batches | lr 20.00 | ms/batch 76.43 | loss  4.27 | ppl    71.49\n",
      "| epoch  18 |   800/ 1106 batches | lr 20.00 | ms/batch 75.95 | loss  4.30 | ppl    74.00\n",
      "| epoch  18 |  1000/ 1106 batches | lr 20.00 | ms/batch 74.99 | loss  4.32 | ppl    74.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 87.94s | valid loss  4.40 | valid ppl    81.30\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  19 |   200/ 1106 batches | lr 20.00 | ms/batch 77.24 | loss  4.37 | ppl    79.10\n",
      "| epoch  19 |   400/ 1106 batches | lr 20.00 | ms/batch 77.63 | loss  4.23 | ppl    68.73\n",
      "| epoch  19 |   600/ 1106 batches | lr 20.00 | ms/batch 74.43 | loss  4.23 | ppl    68.53\n",
      "| epoch  19 |   800/ 1106 batches | lr 20.00 | ms/batch 76.90 | loss  4.29 | ppl    73.02\n",
      "| epoch  19 |  1000/ 1106 batches | lr 20.00 | ms/batch 73.50 | loss  4.29 | ppl    73.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 88.68s | valid loss  4.39 | valid ppl    81.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  20 |   200/ 1106 batches | lr 20.00 | ms/batch 74.92 | loss  4.33 | ppl    75.93\n",
      "| epoch  20 |   400/ 1106 batches | lr 20.00 | ms/batch 75.27 | loss  4.21 | ppl    67.67\n",
      "| epoch  20 |   600/ 1106 batches | lr 20.00 | ms/batch 76.91 | loss  4.23 | ppl    68.62\n",
      "| epoch  20 |   800/ 1106 batches | lr 20.00 | ms/batch 72.94 | loss  4.26 | ppl    71.10\n",
      "| epoch  20 |  1000/ 1106 batches | lr 20.00 | ms/batch 73.88 | loss  4.27 | ppl    71.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 88.09s | valid loss  4.39 | valid ppl    80.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  21 |   200/ 1106 batches | lr 20.00 | ms/batch 75.95 | loss  4.32 | ppl    74.96\n",
      "| epoch  21 |   400/ 1106 batches | lr 20.00 | ms/batch 75.46 | loss  4.18 | ppl    65.49\n",
      "| epoch  21 |   600/ 1106 batches | lr 20.00 | ms/batch 76.94 | loss  4.21 | ppl    67.02\n",
      "| epoch  21 |   800/ 1106 batches | lr 20.00 | ms/batch 74.86 | loss  4.24 | ppl    69.45\n",
      "| epoch  21 |  1000/ 1106 batches | lr 20.00 | ms/batch 74.49 | loss  4.25 | ppl    70.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time: 89.04s | valid loss  4.38 | valid ppl    79.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  22 |   200/ 1106 batches | lr 20.00 | ms/batch 75.91 | loss  4.29 | ppl    72.77\n",
      "| epoch  22 |   400/ 1106 batches | lr 20.00 | ms/batch 75.61 | loss  4.18 | ppl    65.24\n",
      "| epoch  22 |   600/ 1106 batches | lr 20.00 | ms/batch 73.76 | loss  4.18 | ppl    65.30\n",
      "| epoch  22 |   800/ 1106 batches | lr 20.00 | ms/batch 76.69 | loss  4.21 | ppl    67.47\n",
      "| epoch  22 |  1000/ 1106 batches | lr 20.00 | ms/batch 75.61 | loss  4.23 | ppl    68.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time: 88.86s | valid loss  4.37 | valid ppl    78.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  23 |   200/ 1106 batches | lr 20.00 | ms/batch 74.12 | loss  4.27 | ppl    71.32\n",
      "| epoch  23 |   400/ 1106 batches | lr 20.00 | ms/batch 72.96 | loss  4.15 | ppl    63.42\n",
      "| epoch  23 |   600/ 1106 batches | lr 20.00 | ms/batch 73.29 | loss  4.16 | ppl    63.77\n",
      "| epoch  23 |   800/ 1106 batches | lr 20.00 | ms/batch 75.14 | loss  4.20 | ppl    66.71\n",
      "| epoch  23 |  1000/ 1106 batches | lr 20.00 | ms/batch 76.79 | loss  4.21 | ppl    67.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time: 88.17s | valid loss  4.37 | valid ppl    78.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  24 |   200/ 1106 batches | lr 20.00 | ms/batch 76.17 | loss  4.25 | ppl    69.79\n",
      "| epoch  24 |   400/ 1106 batches | lr 20.00 | ms/batch 73.67 | loss  4.13 | ppl    62.34\n",
      "| epoch  24 |   600/ 1106 batches | lr 20.00 | ms/batch 72.48 | loss  4.14 | ppl    62.69\n",
      "| epoch  24 |   800/ 1106 batches | lr 20.00 | ms/batch 73.62 | loss  4.17 | ppl    64.68\n",
      "| epoch  24 |  1000/ 1106 batches | lr 20.00 | ms/batch 74.89 | loss  4.19 | ppl    66.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time: 87.54s | valid loss  4.36 | valid ppl    78.12\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Normal!\n",
      "| epoch  25 |   200/ 1106 batches | lr 20.00 | ms/batch 76.00 | loss  4.23 | ppl    68.61\n",
      "| epoch  25 |   400/ 1106 batches | lr 20.00 | ms/batch 75.49 | loss  4.10 | ppl    60.55\n",
      "| epoch  25 |   600/ 1106 batches | lr 20.00 | ms/batch 76.28 | loss  4.11 | ppl    61.19\n",
      "| epoch  25 |   800/ 1106 batches | lr 20.00 | ms/batch 75.90 | loss  4.16 | ppl    63.75\n",
      "| epoch  25 |  1000/ 1106 batches | lr 20.00 | ms/batch 74.38 | loss  4.18 | ppl    65.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time: 88.77s | valid loss  4.36 | valid ppl    77.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  26 |   200/ 1106 batches | lr 20.00 | ms/batch 73.23 | loss  4.21 | ppl    67.34\n",
      "| epoch  26 |   400/ 1106 batches | lr 20.00 | ms/batch 74.45 | loss  4.10 | ppl    60.56\n",
      "| epoch  26 |   600/ 1106 batches | lr 20.00 | ms/batch 75.46 | loss  4.11 | ppl    60.88\n",
      "| epoch  26 |   800/ 1106 batches | lr 20.00 | ms/batch 76.82 | loss  4.15 | ppl    63.37\n",
      "| epoch  26 |  1000/ 1106 batches | lr 20.00 | ms/batch 75.31 | loss  4.16 | ppl    64.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time: 88.90s | valid loss  4.36 | valid ppl    77.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  27 |   200/ 1106 batches | lr 20.00 | ms/batch 76.13 | loss  4.20 | ppl    66.82\n",
      "| epoch  27 |   400/ 1106 batches | lr 20.00 | ms/batch 75.47 | loss  4.08 | ppl    59.19\n",
      "| epoch  27 |   600/ 1106 batches | lr 20.00 | ms/batch 75.56 | loss  4.09 | ppl    59.63\n",
      "| epoch  27 |   800/ 1106 batches | lr 20.00 | ms/batch 76.78 | loss  4.13 | ppl    62.38\n",
      "| epoch  27 |  1000/ 1106 batches | lr 20.00 | ms/batch 76.45 | loss  4.15 | ppl    63.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time: 88.79s | valid loss  4.35 | valid ppl    77.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  28 |   200/ 1106 batches | lr 20.00 | ms/batch 78.06 | loss  4.19 | ppl    66.12\n",
      "| epoch  28 |   400/ 1106 batches | lr 20.00 | ms/batch 74.65 | loss  4.07 | ppl    58.34\n",
      "| epoch  28 |   600/ 1106 batches | lr 20.00 | ms/batch 74.52 | loss  4.07 | ppl    58.82\n",
      "| epoch  28 |   800/ 1106 batches | lr 20.00 | ms/batch 74.17 | loss  4.11 | ppl    60.90\n",
      "| epoch  28 |  1000/ 1106 batches | lr 20.00 | ms/batch 72.77 | loss  4.15 | ppl    63.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time: 88.86s | valid loss  4.35 | valid ppl    77.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  29 |   200/ 1106 batches | lr 20.00 | ms/batch 75.02 | loss  4.17 | ppl    64.85\n",
      "| epoch  29 |   400/ 1106 batches | lr 20.00 | ms/batch 75.36 | loss  4.04 | ppl    57.05\n",
      "| epoch  29 |   600/ 1106 batches | lr 20.00 | ms/batch 75.06 | loss  4.08 | ppl    58.86\n",
      "| epoch  29 |   800/ 1106 batches | lr 20.00 | ms/batch 72.92 | loss  4.09 | ppl    59.50\n",
      "| epoch  29 |  1000/ 1106 batches | lr 20.00 | ms/batch 73.30 | loss  4.14 | ppl    62.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time: 88.27s | valid loss  4.34 | valid ppl    77.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  30 |   200/ 1106 batches | lr 20.00 | ms/batch 77.99 | loss  4.15 | ppl    63.27\n",
      "| epoch  30 |   400/ 1106 batches | lr 20.00 | ms/batch 76.01 | loss  4.04 | ppl    56.75\n",
      "| epoch  30 |   600/ 1106 batches | lr 20.00 | ms/batch 72.89 | loss  4.06 | ppl    58.21\n",
      "| epoch  30 |   800/ 1106 batches | lr 20.00 | ms/batch 71.28 | loss  4.07 | ppl    58.55\n",
      "| epoch  30 |  1000/ 1106 batches | lr 20.00 | ms/batch 76.44 | loss  4.12 | ppl    61.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time: 88.91s | valid loss  4.35 | valid ppl    77.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  31 |   200/ 1106 batches | lr 20.00 | ms/batch 76.14 | loss  4.14 | ppl    62.86\n",
      "| epoch  31 |   400/ 1106 batches | lr 20.00 | ms/batch 77.06 | loss  4.03 | ppl    56.49\n",
      "| epoch  31 |   600/ 1106 batches | lr 20.00 | ms/batch 73.20 | loss  4.04 | ppl    56.68\n",
      "| epoch  31 |   800/ 1106 batches | lr 20.00 | ms/batch 76.21 | loss  4.07 | ppl    58.57\n",
      "| epoch  31 |  1000/ 1106 batches | lr 20.00 | ms/batch 75.79 | loss  4.10 | ppl    60.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time: 89.04s | valid loss  4.35 | valid ppl    77.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  32 |   200/ 1106 batches | lr 20.00 | ms/batch 77.65 | loss  4.12 | ppl    61.58\n",
      "| epoch  32 |   400/ 1106 batches | lr 20.00 | ms/batch 72.33 | loss  4.00 | ppl    54.57\n",
      "| epoch  32 |   600/ 1106 batches | lr 20.00 | ms/batch 75.34 | loss  4.03 | ppl    56.39\n",
      "| epoch  32 |   800/ 1106 batches | lr 20.00 | ms/batch 75.25 | loss  4.06 | ppl    57.70\n",
      "| epoch  32 |  1000/ 1106 batches | lr 20.00 | ms/batch 75.71 | loss  4.07 | ppl    58.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time: 88.59s | valid loss  4.34 | valid ppl    76.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  33 |   200/ 1106 batches | lr 20.00 | ms/batch 75.67 | loss  4.13 | ppl    61.97\n",
      "| epoch  33 |   400/ 1106 batches | lr 20.00 | ms/batch 72.08 | loss  4.00 | ppl    54.73\n",
      "| epoch  33 |   600/ 1106 batches | lr 20.00 | ms/batch 73.55 | loss  4.02 | ppl    55.47\n",
      "| epoch  33 |   800/ 1106 batches | lr 20.00 | ms/batch 75.80 | loss  4.05 | ppl    57.19\n",
      "| epoch  33 |  1000/ 1106 batches | lr 20.00 | ms/batch 75.30 | loss  4.08 | ppl    58.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time: 88.41s | valid loss  4.34 | valid ppl    76.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  34 |   200/ 1106 batches | lr 20.00 | ms/batch 75.26 | loss  4.11 | ppl    60.74\n",
      "| epoch  34 |   400/ 1106 batches | lr 20.00 | ms/batch 73.44 | loss  3.99 | ppl    53.92\n",
      "| epoch  34 |   600/ 1106 batches | lr 20.00 | ms/batch 75.68 | loss  4.01 | ppl    55.41\n",
      "| epoch  34 |   800/ 1106 batches | lr 20.00 | ms/batch 74.85 | loss  4.05 | ppl    57.34\n",
      "| epoch  34 |  1000/ 1106 batches | lr 20.00 | ms/batch 75.60 | loss  4.06 | ppl    58.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | time: 88.02s | valid loss  4.33 | valid ppl    76.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  35 |   200/ 1106 batches | lr 20.00 | ms/batch 75.36 | loss  4.09 | ppl    59.88\n",
      "| epoch  35 |   400/ 1106 batches | lr 20.00 | ms/batch 75.51 | loss  3.97 | ppl    53.16\n",
      "| epoch  35 |   600/ 1106 batches | lr 20.00 | ms/batch 74.77 | loss  4.00 | ppl    54.53\n",
      "| epoch  35 |   800/ 1106 batches | lr 20.00 | ms/batch 74.52 | loss  4.02 | ppl    55.63\n",
      "| epoch  35 |  1000/ 1106 batches | lr 20.00 | ms/batch 73.79 | loss  4.05 | ppl    57.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | time: 87.86s | valid loss  4.33 | valid ppl    75.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  36 |   200/ 1106 batches | lr 20.00 | ms/batch 74.96 | loss  4.08 | ppl    58.87\n",
      "| epoch  36 |   400/ 1106 batches | lr 20.00 | ms/batch 75.09 | loss  3.97 | ppl    53.24\n",
      "| epoch  36 |   600/ 1106 batches | lr 20.00 | ms/batch 76.06 | loss  3.98 | ppl    53.46\n",
      "| epoch  36 |   800/ 1106 batches | lr 20.00 | ms/batch 74.49 | loss  4.01 | ppl    55.37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  36 |  1000/ 1106 batches | lr 20.00 | ms/batch 74.59 | loss  4.04 | ppl    56.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  36 | time: 88.70s | valid loss  4.33 | valid ppl    76.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  37 |   200/ 1106 batches | lr 20.00 | ms/batch 76.30 | loss  4.06 | ppl    58.05\n",
      "| epoch  37 |   400/ 1106 batches | lr 20.00 | ms/batch 76.84 | loss  3.97 | ppl    52.76\n",
      "| epoch  37 |   600/ 1106 batches | lr 20.00 | ms/batch 77.17 | loss  3.97 | ppl    52.80\n",
      "| epoch  37 |   800/ 1106 batches | lr 20.00 | ms/batch 72.93 | loss  4.01 | ppl    54.97\n",
      "| epoch  37 |  1000/ 1106 batches | lr 20.00 | ms/batch 75.50 | loss  4.04 | ppl    56.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  37 | time: 88.69s | valid loss  4.34 | valid ppl    76.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  38 |   200/ 1106 batches | lr 20.00 | ms/batch 75.13 | loss  4.07 | ppl    58.40\n",
      "| epoch  38 |   400/ 1106 batches | lr 20.00 | ms/batch 75.28 | loss  3.96 | ppl    52.29\n",
      "| epoch  38 |   600/ 1106 batches | lr 20.00 | ms/batch 74.78 | loss  3.96 | ppl    52.40\n",
      "| epoch  38 |   800/ 1106 batches | lr 20.00 | ms/batch 73.60 | loss  4.00 | ppl    54.78\n",
      "| epoch  38 |  1000/ 1106 batches | lr 20.00 | ms/batch 75.75 | loss  4.02 | ppl    55.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  38 | time: 88.95s | valid loss  4.33 | valid ppl    75.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  39 |   200/ 1106 batches | lr 20.00 | ms/batch 75.61 | loss  4.05 | ppl    57.15\n",
      "| epoch  39 |   400/ 1106 batches | lr 20.00 | ms/batch 74.55 | loss  3.93 | ppl    51.16\n",
      "| epoch  39 |   600/ 1106 batches | lr 20.00 | ms/batch 74.39 | loss  3.96 | ppl    52.44\n",
      "| epoch  39 |   800/ 1106 batches | lr 20.00 | ms/batch 74.44 | loss  3.97 | ppl    53.04\n",
      "| epoch  39 |  1000/ 1106 batches | lr 20.00 | ms/batch 76.99 | loss  4.02 | ppl    55.49\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  39 | time: 88.75s | valid loss  4.33 | valid ppl    75.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  40 |   200/ 1106 batches | lr 20.00 | ms/batch 76.50 | loss  4.04 | ppl    56.90\n",
      "| epoch  40 |   400/ 1106 batches | lr 20.00 | ms/batch 74.11 | loss  3.93 | ppl    50.99\n",
      "| epoch  40 |   600/ 1106 batches | lr 20.00 | ms/batch 76.47 | loss  3.93 | ppl    50.83\n",
      "| epoch  40 |   800/ 1106 batches | lr 20.00 | ms/batch 74.63 | loss  3.97 | ppl    53.21\n",
      "| epoch  40 |  1000/ 1106 batches | lr 20.00 | ms/batch 76.00 | loss  4.00 | ppl    54.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  40 | time: 88.77s | valid loss  4.33 | valid ppl    76.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "Switching!\n",
      "| epoch  41 |   200/ 1106 batches | lr 20.00 | ms/batch 78.98 | loss  4.03 | ppl    56.35\n",
      "| epoch  41 |   400/ 1106 batches | lr 20.00 | ms/batch 75.66 | loss  3.92 | ppl    50.60\n",
      "| epoch  41 |   600/ 1106 batches | lr 20.00 | ms/batch 77.61 | loss  3.93 | ppl    51.05\n",
      "| epoch  41 |   800/ 1106 batches | lr 20.00 | ms/batch 79.42 | loss  3.97 | ppl    52.77\n",
      "| epoch  41 |  1000/ 1106 batches | lr 20.00 | ms/batch 78.98 | loss  3.99 | ppl    54.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  41 | time: 91.65s | valid loss  4.26 | valid ppl    70.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  42 |   200/ 1106 batches | lr 20.00 | ms/batch 76.33 | loss  4.03 | ppl    56.37\n",
      "| epoch  42 |   400/ 1106 batches | lr 20.00 | ms/batch 77.72 | loss  3.91 | ppl    50.05\n",
      "| epoch  42 |   600/ 1106 batches | lr 20.00 | ms/batch 78.58 | loss  3.93 | ppl    50.93\n",
      "| epoch  42 |   800/ 1106 batches | lr 20.00 | ms/batch 78.51 | loss  3.97 | ppl    53.03\n",
      "| epoch  42 |  1000/ 1106 batches | lr 20.00 | ms/batch 78.18 | loss  3.97 | ppl    52.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  42 | time: 90.75s | valid loss  4.25 | valid ppl    70.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  43 |   200/ 1106 batches | lr 20.00 | ms/batch 77.10 | loss  4.01 | ppl    55.29\n",
      "| epoch  43 |   400/ 1106 batches | lr 20.00 | ms/batch 79.82 | loss  3.91 | ppl    49.95\n",
      "| epoch  43 |   600/ 1106 batches | lr 20.00 | ms/batch 76.90 | loss  3.92 | ppl    50.24\n",
      "| epoch  43 |   800/ 1106 batches | lr 20.00 | ms/batch 79.21 | loss  3.95 | ppl    51.88\n",
      "| epoch  43 |  1000/ 1106 batches | lr 20.00 | ms/batch 77.72 | loss  3.97 | ppl    52.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  43 | time: 92.35s | valid loss  4.25 | valid ppl    70.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  44 |   200/ 1106 batches | lr 20.00 | ms/batch 79.05 | loss  4.01 | ppl    55.10\n",
      "| epoch  44 |   400/ 1106 batches | lr 20.00 | ms/batch 77.67 | loss  3.91 | ppl    49.71\n",
      "| epoch  44 |   600/ 1106 batches | lr 20.00 | ms/batch 78.11 | loss  3.91 | ppl    49.92\n",
      "| epoch  44 |   800/ 1106 batches | lr 20.00 | ms/batch 75.53 | loss  3.94 | ppl    51.62\n",
      "| epoch  44 |  1000/ 1106 batches | lr 20.00 | ms/batch 76.37 | loss  3.97 | ppl    53.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  44 | time: 91.33s | valid loss  4.25 | valid ppl    69.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  45 |   200/ 1106 batches | lr 20.00 | ms/batch 79.73 | loss  4.00 | ppl    54.45\n",
      "| epoch  45 |   400/ 1106 batches | lr 20.00 | ms/batch 79.67 | loss  3.89 | ppl    48.90\n",
      "| epoch  45 |   600/ 1106 batches | lr 20.00 | ms/batch 76.69 | loss  3.90 | ppl    49.56\n",
      "| epoch  45 |   800/ 1106 batches | lr 20.00 | ms/batch 76.99 | loss  3.93 | ppl    50.72\n",
      "| epoch  45 |  1000/ 1106 batches | lr 20.00 | ms/batch 78.74 | loss  3.96 | ppl    52.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  45 | time: 92.25s | valid loss  4.24 | valid ppl    69.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  46 |   200/ 1106 batches | lr 20.00 | ms/batch 79.38 | loss  3.99 | ppl    54.20\n",
      "| epoch  46 |   400/ 1106 batches | lr 20.00 | ms/batch 76.03 | loss  3.88 | ppl    48.28\n",
      "| epoch  46 |   600/ 1106 batches | lr 20.00 | ms/batch 78.48 | loss  3.88 | ppl    48.56\n",
      "| epoch  46 |   800/ 1106 batches | lr 20.00 | ms/batch 79.30 | loss  3.93 | ppl    51.13\n",
      "| epoch  46 |  1000/ 1106 batches | lr 20.00 | ms/batch 79.14 | loss  3.95 | ppl    52.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  46 | time: 91.88s | valid loss  4.24 | valid ppl    69.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  47 |   200/ 1106 batches | lr 20.00 | ms/batch 78.22 | loss  4.00 | ppl    54.37\n",
      "| epoch  47 |   400/ 1106 batches | lr 20.00 | ms/batch 76.15 | loss  3.87 | ppl    47.89\n",
      "| epoch  47 |   600/ 1106 batches | lr 20.00 | ms/batch 76.16 | loss  3.89 | ppl    49.09\n",
      "| epoch  47 |   800/ 1106 batches | lr 20.00 | ms/batch 79.14 | loss  3.91 | ppl    49.98\n",
      "| epoch  47 |  1000/ 1106 batches | lr 20.00 | ms/batch 78.39 | loss  3.95 | ppl    52.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  47 | time: 91.98s | valid loss  4.24 | valid ppl    69.47\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Averaged!\n",
      "| epoch  48 |   200/ 1106 batches | lr 20.00 | ms/batch 76.51 | loss  3.97 | ppl    53.20\n",
      "| epoch  48 |   400/ 1106 batches | lr 20.00 | ms/batch 76.25 | loss  3.86 | ppl    47.57\n",
      "| epoch  48 |   600/ 1106 batches | lr 20.00 | ms/batch 77.15 | loss  3.87 | ppl    47.77\n",
      "| epoch  48 |   800/ 1106 batches | lr 20.00 | ms/batch 77.06 | loss  3.92 | ppl    50.26\n",
      "| epoch  48 |  1000/ 1106 batches | lr 20.00 | ms/batch 76.48 | loss  3.94 | ppl    51.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  48 | time: 90.10s | valid loss  4.24 | valid ppl    69.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  49 |   200/ 1106 batches | lr 20.00 | ms/batch 76.91 | loss  3.96 | ppl    52.59\n",
      "| epoch  49 |   400/ 1106 batches | lr 20.00 | ms/batch 78.69 | loss  3.86 | ppl    47.48\n",
      "| epoch  49 |   600/ 1106 batches | lr 20.00 | ms/batch 77.28 | loss  3.87 | ppl    47.88\n",
      "| epoch  49 |   800/ 1106 batches | lr 20.00 | ms/batch 79.03 | loss  3.90 | ppl    49.32\n",
      "| epoch  49 |  1000/ 1106 batches | lr 20.00 | ms/batch 76.52 | loss  3.94 | ppl    51.49\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  49 | time: 91.42s | valid loss  4.24 | valid ppl    69.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  50 |   200/ 1106 batches | lr 20.00 | ms/batch 78.93 | loss  3.96 | ppl    52.44\n",
      "| epoch  50 |   400/ 1106 batches | lr 20.00 | ms/batch 78.54 | loss  3.84 | ppl    46.70\n",
      "| epoch  50 |   600/ 1106 batches | lr 20.00 | ms/batch 77.63 | loss  3.87 | ppl    48.10\n",
      "| epoch  50 |   800/ 1106 batches | lr 20.00 | ms/batch 75.36 | loss  3.89 | ppl    49.14\n",
      "| epoch  50 |  1000/ 1106 batches | lr 20.00 | ms/batch 77.74 | loss  3.93 | ppl    51.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  50 | time: 92.57s | valid loss  4.24 | valid ppl    69.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  51 |   200/ 1106 batches | lr 20.00 | ms/batch 77.95 | loss  3.95 | ppl    52.04\n",
      "| epoch  51 |   400/ 1106 batches | lr 20.00 | ms/batch 77.07 | loss  3.86 | ppl    47.33\n",
      "| epoch  51 |   600/ 1106 batches | lr 20.00 | ms/batch 76.80 | loss  3.87 | ppl    47.96\n",
      "| epoch  51 |   800/ 1106 batches | lr 20.00 | ms/batch 76.30 | loss  3.88 | ppl    48.57\n",
      "| epoch  51 |  1000/ 1106 batches | lr 20.00 | ms/batch 78.95 | loss  3.92 | ppl    50.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  51 | time: 91.24s | valid loss  4.24 | valid ppl    69.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  52 |   200/ 1106 batches | lr 20.00 | ms/batch 78.76 | loss  3.95 | ppl    52.12\n",
      "| epoch  52 |   400/ 1106 batches | lr 20.00 | ms/batch 75.91 | loss  3.84 | ppl    46.59\n",
      "| epoch  52 |   600/ 1106 batches | lr 20.00 | ms/batch 73.78 | loss  3.86 | ppl    47.28\n",
      "| epoch  52 |   800/ 1106 batches | lr 20.00 | ms/batch 78.20 | loss  3.88 | ppl    48.40\n",
      "| epoch  52 |  1000/ 1106 batches | lr 20.00 | ms/batch 79.07 | loss  3.91 | ppl    50.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  52 | time: 91.04s | valid loss  4.24 | valid ppl    69.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  53 |   200/ 1106 batches | lr 20.00 | ms/batch 76.98 | loss  3.93 | ppl    51.09\n",
      "| epoch  53 |   400/ 1106 batches | lr 20.00 | ms/batch 77.46 | loss  3.84 | ppl    46.44\n",
      "| epoch  53 |   600/ 1106 batches | lr 20.00 | ms/batch 76.85 | loss  3.86 | ppl    47.64\n",
      "| epoch  53 |   800/ 1106 batches | lr 20.00 | ms/batch 77.78 | loss  3.88 | ppl    48.29\n",
      "| epoch  53 |  1000/ 1106 batches | lr 20.00 | ms/batch 76.89 | loss  3.91 | ppl    49.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  53 | time: 91.45s | valid loss  4.23 | valid ppl    69.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  54 |   200/ 1106 batches | lr 20.00 | ms/batch 76.33 | loss  3.92 | ppl    50.63\n",
      "| epoch  54 |   400/ 1106 batches | lr 20.00 | ms/batch 77.65 | loss  3.84 | ppl    46.41\n",
      "| epoch  54 |   600/ 1106 batches | lr 20.00 | ms/batch 79.24 | loss  3.85 | ppl    47.16\n",
      "| epoch  54 |   800/ 1106 batches | lr 20.00 | ms/batch 78.03 | loss  3.87 | ppl    47.77\n",
      "| epoch  54 |  1000/ 1106 batches | lr 20.00 | ms/batch 76.87 | loss  3.90 | ppl    49.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  54 | time: 91.68s | valid loss  4.23 | valid ppl    69.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  55 |   200/ 1106 batches | lr 20.00 | ms/batch 79.97 | loss  3.93 | ppl    50.88\n",
      "| epoch  55 |   400/ 1106 batches | lr 20.00 | ms/batch 79.90 | loss  3.82 | ppl    45.78\n",
      "| epoch  55 |   600/ 1106 batches | lr 20.00 | ms/batch 77.99 | loss  3.84 | ppl    46.31\n",
      "| epoch  55 |   800/ 1106 batches | lr 20.00 | ms/batch 78.07 | loss  3.86 | ppl    47.52\n",
      "| epoch  55 |  1000/ 1106 batches | lr 20.00 | ms/batch 77.57 | loss  3.91 | ppl    49.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  55 | time: 93.14s | valid loss  4.23 | valid ppl    68.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  56 |   200/ 1106 batches | lr 20.00 | ms/batch 78.75 | loss  3.94 | ppl    51.50\n",
      "| epoch  56 |   400/ 1106 batches | lr 20.00 | ms/batch 78.67 | loss  3.82 | ppl    45.50\n",
      "| epoch  56 |   600/ 1106 batches | lr 20.00 | ms/batch 77.56 | loss  3.83 | ppl    46.23\n",
      "| epoch  56 |   800/ 1106 batches | lr 20.00 | ms/batch 76.03 | loss  3.85 | ppl    47.04\n",
      "| epoch  56 |  1000/ 1106 batches | lr 20.00 | ms/batch 77.00 | loss  3.88 | ppl    48.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  56 | time: 91.71s | valid loss  4.23 | valid ppl    68.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  57 |   200/ 1106 batches | lr 20.00 | ms/batch 79.70 | loss  3.93 | ppl    50.87\n",
      "| epoch  57 |   400/ 1106 batches | lr 20.00 | ms/batch 77.04 | loss  3.81 | ppl    45.07\n",
      "| epoch  57 |   600/ 1106 batches | lr 20.00 | ms/batch 77.15 | loss  3.83 | ppl    46.11\n",
      "| epoch  57 |   800/ 1106 batches | lr 20.00 | ms/batch 78.07 | loss  3.84 | ppl    46.56\n",
      "| epoch  57 |  1000/ 1106 batches | lr 20.00 | ms/batch 78.12 | loss  3.89 | ppl    48.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  57 | time: 91.69s | valid loss  4.23 | valid ppl    68.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  58 |   200/ 1106 batches | lr 20.00 | ms/batch 78.43 | loss  3.92 | ppl    50.23\n",
      "| epoch  58 |   400/ 1106 batches | lr 20.00 | ms/batch 76.29 | loss  3.80 | ppl    44.87\n",
      "| epoch  58 |   600/ 1106 batches | lr 20.00 | ms/batch 79.72 | loss  3.83 | ppl    46.08\n",
      "| epoch  58 |   800/ 1106 batches | lr 20.00 | ms/batch 78.33 | loss  3.85 | ppl    46.90\n",
      "| epoch  58 |  1000/ 1106 batches | lr 20.00 | ms/batch 77.84 | loss  3.87 | ppl    47.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  58 | time: 91.47s | valid loss  4.23 | valid ppl    68.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  59 |   200/ 1106 batches | lr 20.00 | ms/batch 77.66 | loss  3.90 | ppl    49.20\n",
      "| epoch  59 |   400/ 1106 batches | lr 20.00 | ms/batch 76.72 | loss  3.80 | ppl    44.89\n",
      "| epoch  59 |   600/ 1106 batches | lr 20.00 | ms/batch 79.36 | loss  3.82 | ppl    45.51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  59 |   800/ 1106 batches | lr 20.00 | ms/batch 79.66 | loss  3.84 | ppl    46.59\n",
      "| epoch  59 |  1000/ 1106 batches | lr 20.00 | ms/batch 75.88 | loss  3.87 | ppl    47.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  59 | time: 91.49s | valid loss  4.23 | valid ppl    68.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  60 |   200/ 1106 batches | lr 20.00 | ms/batch 78.85 | loss  3.89 | ppl    48.91\n",
      "| epoch  60 |   400/ 1106 batches | lr 20.00 | ms/batch 79.50 | loss  3.81 | ppl    44.94\n",
      "| epoch  60 |   600/ 1106 batches | lr 20.00 | ms/batch 78.84 | loss  3.81 | ppl    45.35\n",
      "| epoch  60 |   800/ 1106 batches | lr 20.00 | ms/batch 76.97 | loss  3.82 | ppl    45.82\n",
      "| epoch  60 |  1000/ 1106 batches | lr 20.00 | ms/batch 74.52 | loss  3.88 | ppl    48.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  60 | time: 92.52s | valid loss  4.23 | valid ppl    68.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  61 |   200/ 1106 batches | lr 20.00 | ms/batch 79.14 | loss  3.90 | ppl    49.32\n",
      "| epoch  61 |   400/ 1106 batches | lr 20.00 | ms/batch 78.61 | loss  3.79 | ppl    44.24\n",
      "| epoch  61 |   600/ 1106 batches | lr 20.00 | ms/batch 78.25 | loss  3.82 | ppl    45.53\n",
      "| epoch  61 |   800/ 1106 batches | lr 20.00 | ms/batch 77.33 | loss  3.83 | ppl    46.01\n",
      "| epoch  61 |  1000/ 1106 batches | lr 20.00 | ms/batch 77.27 | loss  3.86 | ppl    47.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  61 | time: 92.08s | valid loss  4.23 | valid ppl    68.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  62 |   200/ 1106 batches | lr 20.00 | ms/batch 79.13 | loss  3.89 | ppl    48.91\n",
      "| epoch  62 |   400/ 1106 batches | lr 20.00 | ms/batch 78.05 | loss  3.78 | ppl    43.91\n",
      "| epoch  62 |   600/ 1106 batches | lr 20.00 | ms/batch 77.92 | loss  3.80 | ppl    44.91\n",
      "| epoch  62 |   800/ 1106 batches | lr 20.00 | ms/batch 76.19 | loss  3.81 | ppl    44.94\n",
      "| epoch  62 |  1000/ 1106 batches | lr 20.00 | ms/batch 78.08 | loss  3.87 | ppl    47.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  62 | time: 92.46s | valid loss  4.23 | valid ppl    68.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  63 |   200/ 1106 batches | lr 20.00 | ms/batch 80.19 | loss  3.90 | ppl    49.25\n",
      "| epoch  63 |   400/ 1106 batches | lr 20.00 | ms/batch 76.03 | loss  3.78 | ppl    43.91\n",
      "| epoch  63 |   600/ 1106 batches | lr 20.00 | ms/batch 77.99 | loss  3.81 | ppl    45.06\n",
      "| epoch  63 |   800/ 1106 batches | lr 20.00 | ms/batch 79.66 | loss  3.82 | ppl    45.62\n",
      "| epoch  63 |  1000/ 1106 batches | lr 20.00 | ms/batch 77.69 | loss  3.87 | ppl    47.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  63 | time: 92.09s | valid loss  4.23 | valid ppl    68.59\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  64 |   200/ 1106 batches | lr 20.00 | ms/batch 75.83 | loss  3.87 | ppl    48.03\n",
      "| epoch  64 |   400/ 1106 batches | lr 20.00 | ms/batch 78.05 | loss  3.77 | ppl    43.35\n",
      "| epoch  64 |   600/ 1106 batches | lr 20.00 | ms/batch 74.82 | loss  3.80 | ppl    44.83\n",
      "| epoch  64 |   800/ 1106 batches | lr 20.00 | ms/batch 78.62 | loss  3.81 | ppl    45.18\n",
      "| epoch  64 |  1000/ 1106 batches | lr 20.00 | ms/batch 77.65 | loss  3.85 | ppl    47.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  64 | time: 90.41s | valid loss  4.23 | valid ppl    68.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  65 |   200/ 1106 batches | lr 20.00 | ms/batch 75.67 | loss  3.88 | ppl    48.58\n",
      "| epoch  65 |   400/ 1106 batches | lr 20.00 | ms/batch 78.70 | loss  3.77 | ppl    43.44\n",
      "| epoch  65 |   600/ 1106 batches | lr 20.00 | ms/batch 77.46 | loss  3.80 | ppl    44.51\n",
      "| epoch  65 |   800/ 1106 batches | lr 20.00 | ms/batch 79.25 | loss  3.82 | ppl    45.73\n",
      "| epoch  65 |  1000/ 1106 batches | lr 20.00 | ms/batch 76.86 | loss  3.85 | ppl    46.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  65 | time: 90.85s | valid loss  4.23 | valid ppl    68.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  66 |   200/ 1106 batches | lr 20.00 | ms/batch 77.96 | loss  3.89 | ppl    49.00\n",
      "| epoch  66 |   400/ 1106 batches | lr 20.00 | ms/batch 77.17 | loss  3.76 | ppl    43.16\n",
      "| epoch  66 |   600/ 1106 batches | lr 20.00 | ms/batch 78.83 | loss  3.79 | ppl    44.36\n",
      "| epoch  66 |   800/ 1106 batches | lr 20.00 | ms/batch 77.63 | loss  3.81 | ppl    45.38\n",
      "| epoch  66 |  1000/ 1106 batches | lr 20.00 | ms/batch 75.10 | loss  3.85 | ppl    46.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  66 | time: 90.84s | valid loss  4.23 | valid ppl    68.49\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  67 |   200/ 1106 batches | lr 20.00 | ms/batch 80.62 | loss  3.87 | ppl    48.12\n",
      "| epoch  67 |   400/ 1106 batches | lr 20.00 | ms/batch 76.47 | loss  3.76 | ppl    43.06\n",
      "| epoch  67 |   600/ 1106 batches | lr 20.00 | ms/batch 78.00 | loss  3.78 | ppl    43.94\n",
      "| epoch  67 |   800/ 1106 batches | lr 20.00 | ms/batch 75.68 | loss  3.80 | ppl    44.61\n",
      "| epoch  67 |  1000/ 1106 batches | lr 20.00 | ms/batch 76.74 | loss  3.83 | ppl    46.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  67 | time: 91.43s | valid loss  4.23 | valid ppl    68.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  68 |   200/ 1106 batches | lr 20.00 | ms/batch 78.47 | loss  3.87 | ppl    47.72\n",
      "| epoch  68 |   400/ 1106 batches | lr 20.00 | ms/batch 77.07 | loss  3.77 | ppl    43.48\n",
      "| epoch  68 |   600/ 1106 batches | lr 20.00 | ms/batch 75.82 | loss  3.78 | ppl    43.98\n",
      "| epoch  68 |   800/ 1106 batches | lr 20.00 | ms/batch 76.14 | loss  3.80 | ppl    44.78\n",
      "| epoch  68 |  1000/ 1106 batches | lr 20.00 | ms/batch 77.23 | loss  3.83 | ppl    45.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  68 | time: 90.47s | valid loss  4.23 | valid ppl    68.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  69 |   200/ 1106 batches | lr 20.00 | ms/batch 77.25 | loss  3.88 | ppl    48.42\n",
      "| epoch  69 |   400/ 1106 batches | lr 20.00 | ms/batch 76.56 | loss  3.76 | ppl    43.04\n",
      "| epoch  69 |   600/ 1106 batches | lr 20.00 | ms/batch 78.44 | loss  3.77 | ppl    43.59\n",
      "| epoch  69 |   800/ 1106 batches | lr 20.00 | ms/batch 79.32 | loss  3.81 | ppl    44.99\n",
      "| epoch  69 |  1000/ 1106 batches | lr 20.00 | ms/batch 78.17 | loss  3.83 | ppl    46.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  69 | time: 91.50s | valid loss  4.23 | valid ppl    68.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  70 |   200/ 1106 batches | lr 20.00 | ms/batch 76.75 | loss  3.86 | ppl    47.33\n",
      "| epoch  70 |   400/ 1106 batches | lr 20.00 | ms/batch 76.90 | loss  3.76 | ppl    42.99\n",
      "| epoch  70 |   600/ 1106 batches | lr 20.00 | ms/batch 76.80 | loss  3.76 | ppl    43.07\n",
      "| epoch  70 |   800/ 1106 batches | lr 20.00 | ms/batch 79.95 | loss  3.79 | ppl    44.37\n",
      "| epoch  70 |  1000/ 1106 batches | lr 20.00 | ms/batch 78.16 | loss  3.82 | ppl    45.49\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  70 | time: 91.44s | valid loss  4.22 | valid ppl    68.37\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Averaged!\n",
      "| epoch  71 |   200/ 1106 batches | lr 20.00 | ms/batch 77.63 | loss  3.85 | ppl    47.13\n",
      "| epoch  71 |   400/ 1106 batches | lr 20.00 | ms/batch 77.34 | loss  3.74 | ppl    42.24\n",
      "| epoch  71 |   600/ 1106 batches | lr 20.00 | ms/batch 77.91 | loss  3.77 | ppl    43.29\n",
      "| epoch  71 |   800/ 1106 batches | lr 20.00 | ms/batch 78.07 | loss  3.79 | ppl    44.08\n",
      "| epoch  71 |  1000/ 1106 batches | lr 20.00 | ms/batch 74.99 | loss  3.82 | ppl    45.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  71 | time: 91.00s | valid loss  4.22 | valid ppl    68.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  72 |   200/ 1106 batches | lr 20.00 | ms/batch 78.92 | loss  3.85 | ppl    46.88\n",
      "| epoch  72 |   400/ 1106 batches | lr 20.00 | ms/batch 78.15 | loss  3.75 | ppl    42.45\n",
      "| epoch  72 |   600/ 1106 batches | lr 20.00 | ms/batch 77.87 | loss  3.76 | ppl    42.82\n",
      "| epoch  72 |   800/ 1106 batches | lr 20.00 | ms/batch 75.03 | loss  3.78 | ppl    43.80\n",
      "| epoch  72 |  1000/ 1106 batches | lr 20.00 | ms/batch 77.93 | loss  3.81 | ppl    45.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  72 | time: 91.11s | valid loss  4.22 | valid ppl    68.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Averaged!\n",
      "| epoch  73 |   200/ 1106 batches | lr 20.00 | ms/batch 79.16 | loss  3.86 | ppl    47.35\n",
      "| epoch  73 |   400/ 1106 batches | lr 20.00 | ms/batch 76.45 | loss  3.74 | ppl    41.97\n",
      "| epoch  73 |   600/ 1106 batches | lr 20.00 | ms/batch 75.14 | loss  3.76 | ppl    42.79\n",
      "| epoch  73 |   800/ 1106 batches | lr 20.00 | ms/batch 77.25 | loss  3.78 | ppl    43.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "Exiting from training early\n",
      "=========================================================================================\n",
      "| End of training | test loss  4.19 | test ppl    66.00\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "args = new_params(\"--data data/penn --dropouti 0.4 --dropoutl 0.29 --dropouth 0.225 --seed 20 --batch_size 12\\\n",
    "    --max_seq_len_delta 40 --lr 20.0 --epoch 150 --nhid 350 --nhidlast 100 --emsize 1200 --n_experts 15 --save PTB/3000Emb\")\n",
    "train_data = batchify(corpus.train, args.batch_size, args)\n",
    "\n",
    "model, parallel_model = new_model(args, corpus)\n",
    "SGD(model, parallel_model, args, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment dir : Experiments/WT2Softmax-20180205-002859\n",
      "torch.Size([139241, 15])\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n",
      "Model param size: 25868398\n",
      "Args: Namespace(alpha=2, batch_size=15, beta=1, bptt=70, clip=0.25, continue_train=False, cuda=True, data='data/wikitext-2', dropout=0.4, dropoute=0.1, dropouth=0.2, dropouti=0.55, dropoutl=0.29, emsize=400, epochs=8000, log_interval=200, lr=15.0, max_seq_len_delta=5, moc=False, model='LSTM', mos=False, n_experts=10, nhid=870, nhidlast=400, nlayers=3, nonmono=5, save='Experiments/WT2Softmax-20180205-002859', seed=1882, single_gpu=True, small_batch_size=3, tied=True, wdecay=1.2e-06, wdrop=0.5)\n",
      "Model total parameters: 25868398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pio/os/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py:224: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greately increasing memory usage. To compact weights again call flatten_parameters().\n",
      "  result = self.forward(*input, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 1989 batches | lr 15.00 | ms/batch 271.06 | loss  1.56 | ppl     4.74\n",
      "| epoch   1 |   400/ 1989 batches | lr 15.00 | ms/batch 263.16 | loss  1.42 | ppl     4.15\n",
      "| epoch   1 |   600/ 1989 batches | lr 15.00 | ms/batch 267.65 | loss  1.37 | ppl     3.94\n",
      "| epoch   1 |   800/ 1989 batches | lr 15.00 | ms/batch 297.09 | loss  1.33 | ppl     3.78\n",
      "| epoch   1 |  1000/ 1989 batches | lr 15.00 | ms/batch 305.71 | loss  1.30 | ppl     3.68\n",
      "| epoch   1 |  1200/ 1989 batches | lr 15.00 | ms/batch 304.38 | loss  1.29 | ppl     3.63\n",
      "| epoch   1 |  1400/ 1989 batches | lr 15.00 | ms/batch 301.25 | loss  1.27 | ppl     3.54\n",
      "| epoch   1 |  1600/ 1989 batches | lr 15.00 | ms/batch 304.00 | loss  1.26 | ppl     3.51\n",
      "| epoch   1 |  1800/ 1989 batches | lr 15.00 | ms/batch 300.61 | loss  1.25 | ppl     3.47\n",
      "| epoch   1 |  2000/ 1989 batches | lr 15.00 | ms/batch 305.30 | loss  1.23 | ppl     3.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 612.84s | valid loss  5.84 | valid ppl   342.47\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pio/os/anaconda3/lib/python3.6/site-packages/torch/serialization.py:147: UserWarning: Couldn't retrieve source code for container of type RNNModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Normal!\n",
      "| epoch   2 |   200/ 1989 batches | lr 15.00 | ms/batch 309.07 | loss  1.22 | ppl     3.40\n",
      "| epoch   2 |   400/ 1989 batches | lr 15.00 | ms/batch 309.65 | loss  1.21 | ppl     3.36\n",
      "| epoch   2 |   600/ 1989 batches | lr 15.00 | ms/batch 303.11 | loss  1.19 | ppl     3.30\n",
      "| epoch   2 |   800/ 1989 batches | lr 15.00 | ms/batch 301.44 | loss  1.19 | ppl     3.29\n",
      "| epoch   2 |  1000/ 1989 batches | lr 15.00 | ms/batch 301.30 | loss  1.18 | ppl     3.24\n",
      "| epoch   2 |  1200/ 1989 batches | lr 15.00 | ms/batch 301.12 | loss  1.17 | ppl     3.23\n",
      "| epoch   2 |  1400/ 1989 batches | lr 15.00 | ms/batch 302.27 | loss  1.16 | ppl     3.18\n",
      "| epoch   2 |  1600/ 1989 batches | lr 15.00 | ms/batch 301.67 | loss  1.16 | ppl     3.19\n",
      "| epoch   2 |  1800/ 1989 batches | lr 15.00 | ms/batch 302.27 | loss  1.16 | ppl     3.18\n",
      "| epoch   2 |  2000/ 1989 batches | lr 15.00 | ms/batch 302.51 | loss  1.15 | ppl     3.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 637.66s | valid loss  5.42 | valid ppl   225.49\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   3 |   200/ 1989 batches | lr 15.00 | ms/batch 304.58 | loss  1.15 | ppl     3.15\n",
      "| epoch   3 |   400/ 1989 batches | lr 15.00 | ms/batch 303.53 | loss  1.14 | ppl     3.12\n",
      "| epoch   3 |   600/ 1989 batches | lr 15.00 | ms/batch 302.42 | loss  1.12 | ppl     3.07\n",
      "| epoch   3 |   800/ 1989 batches | lr 15.00 | ms/batch 303.25 | loss  1.13 | ppl     3.10\n",
      "| epoch   3 |  1000/ 1989 batches | lr 15.00 | ms/batch 303.23 | loss  1.12 | ppl     3.07\n",
      "| epoch   3 |  1200/ 1989 batches | lr 15.00 | ms/batch 302.39 | loss  1.12 | ppl     3.06\n",
      "| epoch   3 |  1400/ 1989 batches | lr 15.00 | ms/batch 303.11 | loss  1.10 | ppl     3.01\n",
      "| epoch   3 |  1600/ 1989 batches | lr 15.00 | ms/batch 302.56 | loss  1.11 | ppl     3.03\n",
      "| epoch   3 |  1800/ 1989 batches | lr 15.00 | ms/batch 304.01 | loss  1.11 | ppl     3.04\n",
      "| epoch   3 |  2000/ 1989 batches | lr 15.00 | ms/batch 302.98 | loss  1.10 | ppl     3.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 635.43s | valid loss  5.19 | valid ppl   179.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   4 |   200/ 1989 batches | lr 15.00 | ms/batch 305.06 | loss  1.10 | ppl     3.02\n",
      "| epoch   4 |   400/ 1989 batches | lr 15.00 | ms/batch 303.00 | loss  1.09 | ppl     2.98\n",
      "| epoch   4 |   600/ 1989 batches | lr 15.00 | ms/batch 303.35 | loss  1.08 | ppl     2.95\n",
      "| epoch   4 |   800/ 1989 batches | lr 15.00 | ms/batch 302.33 | loss  1.09 | ppl     2.98\n",
      "| epoch   4 |  1000/ 1989 batches | lr 15.00 | ms/batch 303.71 | loss  1.09 | ppl     2.96\n",
      "| epoch   4 |  1200/ 1989 batches | lr 15.00 | ms/batch 303.17 | loss  1.08 | ppl     2.95\n",
      "| epoch   4 |  1400/ 1989 batches | lr 15.00 | ms/batch 301.85 | loss  1.07 | ppl     2.91\n",
      "| epoch   4 |  1600/ 1989 batches | lr 15.00 | ms/batch 303.39 | loss  1.07 | ppl     2.93\n",
      "| epoch   4 |  1800/ 1989 batches | lr 15.00 | ms/batch 302.75 | loss  1.08 | ppl     2.94\n",
      "| epoch   4 |  2000/ 1989 batches | lr 15.00 | ms/batch 304.21 | loss  1.07 | ppl     2.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 635.89s | valid loss  5.05 | valid ppl   155.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   5 |   200/ 1989 batches | lr 15.00 | ms/batch 305.13 | loss  1.07 | ppl     2.92\n",
      "| epoch   5 |   400/ 1989 batches | lr 15.00 | ms/batch 304.76 | loss  1.06 | ppl     2.90\n",
      "| epoch   5 |   600/ 1989 batches | lr 15.00 | ms/batch 304.19 | loss  1.05 | ppl     2.87\n",
      "| epoch   5 |   800/ 1989 batches | lr 15.00 | ms/batch 302.80 | loss  1.07 | ppl     2.90\n",
      "| epoch   5 |  1000/ 1989 batches | lr 15.00 | ms/batch 303.05 | loss  1.06 | ppl     2.88\n",
      "| epoch   5 |  1200/ 1989 batches | lr 15.00 | ms/batch 303.29 | loss  1.06 | ppl     2.87\n"
     ]
    }
   ],
   "source": [
    "# MoS\n",
    "#args = new_params(\"--data data/wikitext-2 --save Experiments/WT2MoS --dropouth 0.2 --seed 1882 --mos --n_experts 15 --nhid 900 --nhidlast 550 --emsize 300 --batch_size 15 --lr 15.0 --dropoutl 0.29 --small_batch_size 3 --max_seq_len_delta 5 --dropouti 0.55 --single_gpu\")\n",
    "#train_data = batchify(corpus.train, args.batch_size, args)\n",
    "\n",
    "#model, parallel_model = new_model(args, corpus)\n",
    "#SGD(model, parallel_model, args, corpus)\n",
    "\n",
    "# MoC\n",
    "args = new_params(\"--data data/wikitext-2 --save Experiments/WT2Softmax --dropouth 0.2 --seed 1882 --nhid 870 --nhidlast 800 --emsize 400 --batch_size 15 --lr 15.0 --dropoutl 0.29 --small_batch_size 3 --max_seq_len_delta 5 --dropouti 0.55 --single_gpu\")\n",
    "train_data = batchify(corpus.train, args.batch_size, args)\n",
    "\n",
    "model, parallel_model = new_model(args, corpus)\n",
    "SGD(model, parallel_model, args, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.41199541676867]\n"
     ]
    }
   ],
   "source": [
    "h = unserialize('Experiments/PTb-20180204-235512', 'history')\n",
    "print(h['train_errs'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
